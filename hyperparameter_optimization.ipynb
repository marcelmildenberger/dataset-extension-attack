{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import ray\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "\n",
    "from ray.air import session\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from graphMatching.gma import run_gma\n",
    "\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset_padding import TwoStepHashDatasetPadding\n",
    "from datasets.two_step_hash_dataset_frequency_string import TwoStepHashDatasetFrequencyString\n",
    "from datasets.two_step_hash_dataset_one_hot_encoding import TwoStepHashDatasetOneHotEncoding\n",
    "\n",
    "from pytorch_models_hyperparameter_optimization.base_model import BaseModel\n",
    "\n",
    "print('System Version:', sys.version)\n",
    "print('PyTorch version', torch.__version__)\n",
    "print('Torchvision version', torchvision.__version__)\n",
    "print('Numpy version', np.__version__)\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/fakename_1k.tsv\",\n",
    "    \"Overlap\": 0.68,\n",
    "    \"DropFrom\": \"Both\",\n",
    "    \"Verbose\": True,  # Print Status Messages\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": -1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": False,\n",
    "}\n",
    "\n",
    "\n",
    "DEA_CONFIG = {\n",
    "    #Padding / FrequencyString / OneHotEncoding\n",
    "    \"TSHMode\": \"OneHotEncoding\",\n",
    "    \"DevMode\": False,\n",
    "    \"BatchSize\": 32,\n",
    "    # TestSize calculated accordingly\n",
    "    \"TrainSize\": 0.8,\n",
    "    \"FilterThreshold\": 0.5,\n",
    "    \"Patience\": 5,\n",
    "    \"MinDelta\": 0.001,\n",
    "}\n",
    "\n",
    "ENC_CONFIG = {\n",
    "    # TwoStepHash / TabMinHash / BloomFilter\n",
    "    \"AliceAlgo\": \"BloomFilter\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"None\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "    # For BF encoding\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "    # For TMH encoding\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "    # For 2SH encoding\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    # For Node2Vec\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42\n",
    "}\n",
    "\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"]/2), #0005\n",
    "    \"RegInit\":1, # For BF 0.25\n",
    "    \"Batchsize\": 1, # 1 = 100%\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5 ,  # 800\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique hash identifiers for the encoding and embedding configurations\n",
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG)\n",
    "\n",
    "# Define file paths based on the configuration hashes\n",
    "path_reidentified = f\"./data/available_to_eve/reidentified_individuals_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "path_not_reidentified = f\"./data/available_to_eve/not_reidentified_individuals_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "path_all = f\"./data/dev/alice_data_complete_with_encoding_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "\n",
    "# Check if the output files already exist\n",
    "if os.path.isfile(path_reidentified) and os.path.isfile(path_not_reidentified) and os.path.isfile(path_all):\n",
    "    # Load previously saved attack results\n",
    "    reidentified_data = hkl.load(path_reidentified)\n",
    "    not_reidentified_data = hkl.load(path_not_reidentified)\n",
    "    all_data = hkl.load(path_all)\n",
    "\n",
    "else:\n",
    "    # Run Graph Matching Attack if files are not found\n",
    "    reidentified_data, not_reidentified_data, all_data = run_gma(\n",
    "        GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG,\n",
    "        eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash\n",
    "    )\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "df_reidentified = pd.DataFrame(reidentified_data[1:], columns=reidentified_data[0])\n",
    "df_not_reidentified = pd.DataFrame(not_reidentified_data[1:], columns=not_reidentified_data[0])\n",
    "df_all = pd.DataFrame(all_data[1:], columns=all_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate a dictionary of all possible 2-grams from letters and digits ---\n",
    "\n",
    "# Lowercase alphabet: 'a' to 'z'\n",
    "alphabet = string.ascii_lowercase\n",
    "\n",
    "# Digits: '0' to '9'\n",
    "digits = string.digits\n",
    "\n",
    "# Generate all letter-letter 2-grams (e.g., 'aa', 'ab', ..., 'zz')\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]\n",
    "\n",
    "# Generate all digit-digit 2-grams (e.g., '00', '01', ..., '99')\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]\n",
    "\n",
    "# Generate all letter-digit 2-grams (e.g., 'a0', 'a1', ..., 'z9')\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]\n",
    "\n",
    "# Combine all generated 2-grams into one list\n",
    "all_two_grams = letter_letter_grams + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Create a dictionary mapping index to each 2-gram\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Bloom Filter Encoding\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "    data_labeled = BloomFilterDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = BloomFilterDataset(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    bloomfilter_length = len(df_reidentified[\"bloomfilter\"][0])\n",
    "\n",
    "# 2️⃣ Tabulation MinHash Encoding\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "    data_labeled = TabMinHashDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TabMinHashDataset(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    tabminhash_length = len(df_reidentified[\"tabminhash\"][0])\n",
    "\n",
    "# 3️⃣ Two-Step Hash Encoding (Padding Mode)\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\" and DEA_CONFIG[\"TSHMode\"] == \"Padding\":\n",
    "    max_len_reid = df_reidentified[\"twostephash\"].apply(lambda x: len(list(x))).max()\n",
    "    max_len_not_reid = df_not_reidentified[\"twostephash\"].apply(lambda x: len(list(x))).max()\n",
    "    max_twostephash_length = max(max_len_reid, max_len_not_reid)\n",
    "\n",
    "    data_labeled = TwoStepHashDatasetPadding(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        max_set_size=max_twostephash_length,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TwoStepHashDatasetPadding(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        max_set_size=max_twostephash_length,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "\n",
    "# 4️⃣ Two-Step Hash Encoding (Frequency String Mode)\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\" and DEA_CONFIG[\"TSHMode\"] == \"FrequencyString\":\n",
    "    max_len_reid = df_reidentified[\"twostephash\"].apply(lambda x: max(x)).max()\n",
    "    max_len_not_reid = df_not_reidentified[\"twostephash\"].apply(lambda x: max(x)).max()\n",
    "    max_twostephash_length = max(max_len_reid, max_len_not_reid)\n",
    "\n",
    "    data_labeled = TwoStepHashDatasetFrequencyString(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        frequency_string_length=max_twostephash_length,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TwoStepHashDatasetFrequencyString(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        frequency_string_length=max_twostephash_length,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "\n",
    "# 5️⃣ Two-Step Hash Encoding (One-Hot Encoding Mode)\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\" and DEA_CONFIG[\"TSHMode\"] == \"OneHotEncoding\":\n",
    "    # Collect all unique integers across both reidentified and non-reidentified data\n",
    "    unique_ints_reid = set().union(*df_reidentified[\"twostephash\"])\n",
    "    unique_ints_not_reid = set().union(*df_not_reidentified[\"twostephash\"])\n",
    "    unique_ints_sorted = sorted(unique_ints_reid.union(unique_ints_not_reid))\n",
    "    unique_integers_dict = {i: val for i, val in enumerate(unique_ints_sorted)}\n",
    "\n",
    "    data_labeled = TwoStepHashDatasetOneHotEncoding(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_integers=unique_ints_sorted,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TwoStepHashDatasetOneHotEncoding(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_integers=unique_ints_sorted,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset split proportions\n",
    "train_size = int(DEA_CONFIG[\"TrainSize\"] * len(data_labeled))\n",
    "val_size = len(data_labeled) - train_size\n",
    "\n",
    "# Split the reidentified dataset into training and validation sets\n",
    "data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "dataloader_train = DataLoader(\n",
    "    data_train,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True  # Important for training\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    data_val,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True  # Allows variation in validation batches\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    data_not_labeled,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    model = BaseModel(\n",
    "    input_dim=bloomfilter_length,\n",
    "    num_two_grams=len(all_two_grams),\n",
    "    num_layers=config[\"num_layers\"],\n",
    "    hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "    dropout_rate=config[\"dropout_rate\"],\n",
    "    activation_fn=config[\"activation_fn\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Define possible loss functions\n",
    "    loss_functions = {\n",
    "    \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss(),\n",
    "    \"MultiLabelSoftMarginLoss\": nn.MultiLabelSoftMarginLoss()\n",
    "    }\n",
    "\n",
    "    criterion = loss_functions[config[\"loss_fn\"]]\n",
    "\n",
    "    optimizers = {\n",
    "        \"Adam\": optim.Adam(model.parameters(), lr=config[\"lr\"]),\n",
    "        \"SGD\": optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9),\n",
    "        \"RMSprop\": optim.RMSprop(model.parameters(), lr=config[\"lr\"])\n",
    "    }\n",
    "\n",
    "    optimizer = optimizers[config[\"optimizer\"]]\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, labels, _ in dataloader_train:\n",
    "            # Move data to device\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "        train_loss = running_loss / len(dataloader_train.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        #Calculate true training loss?\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, labels, _ in dataloader_val:\n",
    "                # Move data to device\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "            val_loss = running_loss / len(dataloader_val.dataset)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Define Threshhold\n",
    "    threshold = DEA_CONFIG[\"FilterThreshold\"]\n",
    "\n",
    "    # Loop through the test dataloader\n",
    "    with torch.no_grad():  # No need to compute gradients during inference\n",
    "        for data_batch, uids in dataloader_test:\n",
    "            # Filter relevant individuals from df_all_individuals\n",
    "            filtered_df = df_all_individuals[df_all_individuals[\"uid\"].isin(uids)].drop(df_all_individuals.columns[-2], axis=1) # Drop encoding column\n",
    "\n",
    "            actual_two_grams_batch = []\n",
    "            for _, entry in filtered_df.iterrows():\n",
    "                row = entry[:-1] # Exclude UID\n",
    "                extracted_two_grams = extract_two_grams(\"\".join(map(str, row)))\n",
    "                actual_two_grams_batch.append({\"uid\": entry[\"uid\"], \"two_grams\": extracted_two_grams})\n",
    "\n",
    "            # Move data to device\n",
    "            data_batch = data_batch.to(device)\n",
    "\n",
    "            # Apply model\n",
    "            logits = model(data_batch)\n",
    "\n",
    "            # Convert logits to probabilities using sigmoid (for binary classification)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities into 2-gram scores (use two_gram_dict as before)\n",
    "            batch_two_gram_scores = [\n",
    "                {two_gram_dict[j]: score.item() for j, score in enumerate(probabilities[i])} #2: For each sample, go through all predicted probabilities (scores)\n",
    "                for i in range(probabilities.size(0))  # 1: Iterate over each sample in the batch\n",
    "            ]\n",
    "\n",
    "            # Apply threshold to filter 2-gram scores (values above threshold are kept)\n",
    "            batch_filtered_two_gram_scores = [\n",
    "                {two_gram: score for two_gram, score in two_gram_scores.items() if score > threshold}\n",
    "                for two_gram_scores in batch_two_gram_scores\n",
    "            ]\n",
    "\n",
    "            filtered_two_grams = [\n",
    "            {\"uid\": uid, \"two_grams\": {key for key in two_grams.keys()}}\n",
    "            for uid, two_grams in zip(uids, batch_filtered_two_gram_scores)\n",
    "            ]\n",
    "\n",
    "            sum_dice = 0\n",
    "            for entry_two_grams_batch in actual_two_grams_batch:  # Loop through each uid in the batch\n",
    "                for entry_filtered_two_grams in filtered_two_grams:\n",
    "                    if entry_two_grams_batch[\"uid\"] == entry_filtered_two_grams[\"uid\"]:\n",
    "                        dice_sim = dice_coefficient(entry_two_grams_batch[\"two_grams\"], entry_filtered_two_grams[\"two_grams\"])\n",
    "                        sum_dice += dice_sim\n",
    "    tune.report({\"dice\": sum(val_losses)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define extended search space\n",
    "search_space = {\n",
    "    \"num_layers\": tune.randint(1, 8),  # Vary number of layers\n",
    "    \"hidden_layer_size\": tune.choice([128, 256, 512, 1024, 2048]),  # Size of hidden layers\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.4),  # Dropout\n",
    "    \"activation_fn\": tune.choice([\"relu\", \"leaky_relu\", \"gelu\"]),  # Activation function\n",
    "    \"optimizer\": tune.choice([\"Adam\", \"SGD\", \"RMSprop\"]),  # Optimizer selection\n",
    "    \"loss_fn\": tune.choice([\"BCEWithLogitsLoss\"]),  # Loss function selection\n",
    "    # \"loss_fn\": tune.choice([\"BCEWithLogitsLoss\", \"MultiLabelSoftMarginLoss\"]),  # Loss function selection\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),  # Learning rate\n",
    "    \"epochs\": tune.randint(10, 20),  # Fixed number of epochs\n",
    "}\n",
    "\n",
    "# Run Ray Tune with Optuna\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "optuna_search = OptunaSearch(metric=\"dice\", mode=\"max\")\n",
    "scheduler = ASHAScheduler(metric=\"dice\", mode=\"max\")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_model,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,\n",
    "        scheduler=scheduler,\n",
    "        num_samples=5  # Number of trials\n",
    "    ),\n",
    "    param_space=search_space\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "print(\"Best hyperparameters:\", results.get_best_result(metric=\"dice\", mode=\"max\").config)\n",
    "\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
