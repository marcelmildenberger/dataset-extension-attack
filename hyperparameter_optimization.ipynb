{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:20:01) [Clang 18.1.8 ]\n",
      "PyTorch version 2.1.2\n",
      "Torchvision version 0.16.2\n",
      "Numpy version 1.24.4\n",
      "Pandas version 2.0.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import ray\n",
    "from ray.tune import Result\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "\n",
    "from ray.air import session\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from graphMatching.gma import run_gma\n",
    "\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset_padding import TwoStepHashDatasetPadding\n",
    "from datasets.two_step_hash_dataset_frequency_string import TwoStepHashDatasetFrequencyString\n",
    "from datasets.two_step_hash_dataset_one_hot_encoding import TwoStepHashDatasetOneHotEncoding\n",
    "\n",
    "from pytorch_models_hyperparameter_optimization.base_model import BaseModel\n",
    "\n",
    "print('System Version:', sys.version)\n",
    "print('PyTorch version', torch.__version__)\n",
    "print('Torchvision version', torchvision.__version__)\n",
    "print('Numpy version', np.__version__)\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/fakename_1k.tsv\",\n",
    "    \"Overlap\": 0.8,\n",
    "    \"DropFrom\": \"Eve\",\n",
    "    \"Verbose\": True,  # Print Status Messages\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": -1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": False,\n",
    "}\n",
    "\n",
    "\n",
    "DEA_CONFIG = {\n",
    "    #Padding / FrequencyString / OneHotEncoding\n",
    "    \"TSHMode\": \"OneHotEncoding\",\n",
    "    \"DevMode\": False,\n",
    "    \"BatchSize\": 32,\n",
    "    # TestSize calculated accordingly\n",
    "    \"TrainSize\": 0.8,\n",
    "    \"FilterThreshold\": 0.5,\n",
    "    \"Patience\": 5,\n",
    "    \"MinDelta\": 0.001,\n",
    "}\n",
    "\n",
    "ENC_CONFIG = {\n",
    "    # TwoStepHash / TabMinHash / BloomFilter\n",
    "    \"AliceAlgo\": \"BloomFilter\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"None\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "    # For BF encoding\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "    # For TMH encoding\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "    # For 2SH encoding\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    # For Node2Vec\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42\n",
    "}\n",
    "\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"]/2), #0005\n",
    "    \"RegInit\":1, # For BF 0.25\n",
    "    \"Batchsize\": 1, # 1 = 100%\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5 ,  # 800\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique hash identifiers for the encoding and embedding configurations\n",
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG)\n",
    "\n",
    "# Define file paths based on the configuration hashes\n",
    "path_reidentified = f\"./data/available_to_eve/reidentified_individuals_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "path_not_reidentified = f\"./data/available_to_eve/not_reidentified_individuals_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "path_all = f\"./data/dev/alice_data_complete_with_encoding_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "\n",
    "# Check if the output files already exist\n",
    "if os.path.isfile(path_reidentified) and os.path.isfile(path_not_reidentified) and os.path.isfile(path_all):\n",
    "    # Load previously saved attack results\n",
    "    reidentified_data = hkl.load(path_reidentified)\n",
    "    not_reidentified_data = hkl.load(path_not_reidentified)\n",
    "    all_data = hkl.load(path_all)\n",
    "\n",
    "else:\n",
    "    # Run Graph Matching Attack if files are not found\n",
    "    reidentified_data, not_reidentified_data, all_data = run_gma(\n",
    "        GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG,\n",
    "        eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash\n",
    "    )\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "df_reidentified = pd.DataFrame(reidentified_data[1:], columns=reidentified_data[0])\n",
    "df_not_reidentified = pd.DataFrame(not_reidentified_data[1:], columns=not_reidentified_data[0])\n",
    "df_all = pd.DataFrame(all_data[1:], columns=all_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate a dictionary of all possible 2-grams from letters and digits ---\n",
    "\n",
    "# Lowercase alphabet: 'a' to 'z'\n",
    "alphabet = string.ascii_lowercase\n",
    "\n",
    "# Digits: '0' to '9'\n",
    "digits = string.digits\n",
    "\n",
    "# Generate all letter-letter 2-grams (e.g., 'aa', 'ab', ..., 'zz')\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]\n",
    "\n",
    "# Generate all digit-digit 2-grams (e.g., '00', '01', ..., '99')\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]\n",
    "\n",
    "# Generate all letter-digit 2-grams (e.g., 'a0', 'a1', ..., 'z9')\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]\n",
    "\n",
    "# Combine all generated 2-grams into one list\n",
    "all_two_grams = letter_letter_grams + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Create a dictionary mapping index to each 2-gram\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Bloom Filter Encoding\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "    data_labeled = BloomFilterDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = BloomFilterDataset(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    input_layer_size = len(df_reidentified[\"bloomfilter\"][0])\n",
    "\n",
    "# 2️⃣ Tabulation MinHash Encoding\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "    data_labeled = TabMinHashDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TabMinHashDataset(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    input_layer_size = len(df_reidentified[\"tabminhash\"][0])\n",
    "\n",
    "# 3️⃣ Two-Step Hash Encoding (Padding Mode)\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\" and DEA_CONFIG[\"TSHMode\"] == \"Padding\":\n",
    "    max_len_reid = df_reidentified[\"twostephash\"].apply(lambda x: len(list(x))).max()\n",
    "    max_len_not_reid = df_not_reidentified[\"twostephash\"].apply(lambda x: len(list(x))).max()\n",
    "    input_layer_size = max(max_len_reid, max_len_not_reid)\n",
    "\n",
    "    data_labeled = TwoStepHashDatasetPadding(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        max_set_size=input_layer_size,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TwoStepHashDatasetPadding(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        max_set_size=input_layer_size,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "\n",
    "# 4️⃣ Two-Step Hash Encoding (Frequency String Mode)\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\" and DEA_CONFIG[\"TSHMode\"] == \"FrequencyString\":\n",
    "    max_len_reid = df_reidentified[\"twostephash\"].apply(lambda x: max(x)).max()\n",
    "    max_len_not_reid = df_not_reidentified[\"twostephash\"].apply(lambda x: max(x)).max()\n",
    "    input_layer_size = max(max_len_reid, max_len_not_reid)\n",
    "\n",
    "    data_labeled = TwoStepHashDatasetFrequencyString(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        frequency_string_length=input_layer_size,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TwoStepHashDatasetFrequencyString(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        frequency_string_length=input_layer_size,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "\n",
    "# 5️⃣ Two-Step Hash Encoding (One-Hot Encoding Mode)\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\" and DEA_CONFIG[\"TSHMode\"] == \"OneHotEncoding\":\n",
    "    # Collect all unique integers across both reidentified and non-reidentified data\n",
    "    unique_ints_reid = set().union(*df_reidentified[\"twostephash\"])\n",
    "    unique_ints_not_reid = set().union(*df_not_reidentified[\"twostephash\"])\n",
    "    unique_ints_sorted = sorted(unique_ints_reid.union(unique_ints_not_reid))\n",
    "    unique_integers_dict = {i: val for i, val in enumerate(unique_ints_sorted)}\n",
    "    input_layer_size = len(unique_ints_sorted)\n",
    "\n",
    "    data_labeled = TwoStepHashDatasetOneHotEncoding(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_integers=unique_ints_sorted,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TwoStepHashDatasetOneHotEncoding(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_integers=unique_ints_sorted,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset split proportions\n",
    "train_size = int(DEA_CONFIG[\"TrainSize\"] * len(data_labeled))\n",
    "val_size = len(data_labeled) - train_size\n",
    "\n",
    "# Split the reidentified dataset into training and validation sets\n",
    "data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "dataloader_train = DataLoader(\n",
    "    data_train,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True  # Important for training\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    data_val,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True  # Allows variation in validation batches\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    data_not_labeled,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Setup for Training\n",
    "\n",
    "This setup for hyperparameter tuning in a neural network model improves modularity, ensuring easy customization for experimentation.\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   - The model is initialized using hyperparameters from the `config` dictionary, including the number of layers, hidden layer size, dropout rate, and activation function.\n",
    "\n",
    "2. **Loss Function and Optimizer Selection**:\n",
    "   - The loss function (`criterion`) and optimizer are selected dynamically from the `config` dictionary.\n",
    "\n",
    "3. **Training & Validation Loop**:\n",
    "   - The training and validation phases are handled in separate loops. The loss is computed at each step, and metrics are logged.\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - After training, the model is evaluated on a test set, where 2-gram predictions are compared against the actual 2-grams.\n",
    "   - **Dice similarity coefficient** is used as a metric to evaluate model performance.\n",
    "\n",
    "5. **Custom Helper Functions**:\n",
    "   - `extract_two_grams_batch()`: Extracts 2-grams for all samples in the batch.\n",
    "   - `convert_to_two_gram_scores()`: Converts model output logits into 2-gram scores.\n",
    "   - `filter_two_grams()`: Applies a threshold to filter 2-gram scores.\n",
    "   - `filter_two_grams_per_uid()`: Filters and formats 2-gram predictions for each UID.\n",
    "\n",
    "6. **Hyperparameter Tuning**:\n",
    "   - The setup is integrated with **Ray Tune** (`tune.report`) to enable hyperparameter tuning by reporting the Dice similarity metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Initialize lists to store training and validation losses\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # Define and initialize model with hyperparameters from config\n",
    "    model = BaseModel(\n",
    "        input_dim=input_layer_size,\n",
    "        num_two_grams=len(all_two_grams),\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        activation_fn=config[\"activation_fn\"]\n",
    "    )\n",
    "\n",
    "    # Set device for model (GPU or CPU)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Select loss function based on config\n",
    "    loss_functions = {\n",
    "        \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss(),\n",
    "        \"MultiLabelSoftMarginLoss\": nn.MultiLabelSoftMarginLoss()\n",
    "    }\n",
    "    criterion = loss_functions[config[\"loss_fn\"]]\n",
    "\n",
    "    # Select optimizer based on config\n",
    "    optimizers = {\n",
    "        \"Adam\": optim.Adam(model.parameters(), lr=config[\"lr\"]),\n",
    "        \"SGD\": optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9),\n",
    "        \"RMSprop\": optim.RMSprop(model.parameters(), lr=config[\"lr\"])\n",
    "    }\n",
    "    optimizer = optimizers[config[\"optimizer\"]]\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = run_epoch(model, dataloader_train, criterion, optimizer, device, is_training=True)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(model, dataloader_val, criterion, optimizer, device, is_training=False)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Log metrics\n",
    "        log_metrics(train_loss, val_loss, epoch, config[\"epochs\"])\n",
    "\n",
    "    # Test phase with reconstruction and evaluation\n",
    "    model.eval()\n",
    "    threshold = DEA_CONFIG[\"FilterThreshold\"]\n",
    "    sum_dice = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data_batch, uids in dataloader_test:\n",
    "            # Process the test data\n",
    "            filtered_df = df_all[df_all[\"uid\"].isin(uids)].drop(df_all.columns[-2], axis=1)\n",
    "            actual_two_grams_batch = extract_two_grams_batch(filtered_df)\n",
    "\n",
    "            # Move data to device and make predictions\n",
    "            data_batch = data_batch.to(device)\n",
    "            logits = model(data_batch)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities into 2-gram scores\n",
    "            batch_two_gram_scores = convert_to_two_gram_scores(probabilities)\n",
    "\n",
    "            # Filter out low-scoring 2-grams\n",
    "            batch_filtered_two_gram_scores = filter_two_grams(batch_two_gram_scores, threshold)\n",
    "            filtered_two_grams = combine_two_grams_with_uid(uids, batch_filtered_two_gram_scores)\n",
    "\n",
    "            # Calculate Dice similarity for evaluation\n",
    "            sum_dice += calculate_dice_similarity(actual_two_grams_batch, filtered_two_grams)\n",
    "\n",
    "    # Report evaluation metric\n",
    "    tune.report({\"dice\": sum_dice})\n",
    "\n",
    "\n",
    "def run_epoch(model, dataloader, criterion, optimizer, device, is_training):\n",
    "    running_loss = 0.0\n",
    "    with torch.set_grad_enabled(is_training):\n",
    "        for data, labels, _ in dataloader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if is_training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def log_metrics(train_loss, val_loss, epoch, total_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{total_epochs} - \"\n",
    "          f\"Train loss: {train_loss:.4f}, \"\n",
    "          f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "def extract_two_grams_batch(df):\n",
    "    return [\n",
    "        {\"uid\": entry[\"uid\"], \"two_grams\": extract_two_grams(\"\".join(map(str, entry[:-1])))}\n",
    "        for _, entry in df.iterrows()\n",
    "    ]\n",
    "\n",
    "\n",
    "def convert_to_two_gram_scores(probabilities):\n",
    "    return [\n",
    "        {two_gram_dict[j]: score.item() for j, score in enumerate(probabilities[i])}\n",
    "        for i in range(probabilities.size(0))\n",
    "    ]\n",
    "\n",
    "\n",
    "def filter_two_grams(two_gram_scores, threshold):\n",
    "    return [\n",
    "        {two_gram: score for two_gram, score in two_gram_scores.items() if score > threshold}\n",
    "        for two_gram_scores in two_gram_scores\n",
    "    ]\n",
    "\n",
    "\n",
    "def combine_two_grams_with_uid(uids, filtered_two_gram_scores):\n",
    "    return [\n",
    "        {\"uid\": uid, \"two_grams\": {key for key in two_grams.keys()}}\n",
    "        for uid, two_grams in zip(uids, filtered_two_gram_scores)\n",
    "    ]\n",
    "\n",
    "def calculate_dice_similarity(actual_two_grams_batch, filtered_two_grams):\n",
    "    sum_dice = 0\n",
    "    for entry_two_grams_batch in actual_two_grams_batch:\n",
    "        for entry_filtered_two_grams in filtered_two_grams:\n",
    "            if entry_two_grams_batch[\"uid\"] == entry_filtered_two_grams[\"uid\"]:\n",
    "                sum_dice += dice_coefficient(entry_two_grams_batch[\"two_grams\"], entry_filtered_two_grams[\"two_grams\"])\n",
    "    return sum_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-04-11 12:58:36</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:21.58        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.6/32.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: 21.2843<br>Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th>activation_fn  </th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">  hidden_layer_size</th><th>loss_fn          </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>optimizer  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    dice</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_08c42797</td><td>TERMINATED</td><td>127.0.0.1:55127</td><td>relu           </td><td style=\"text-align: right;\">      0.297408</td><td style=\"text-align: right;\">      14</td><td style=\"text-align: right;\">                512</td><td>BCEWithLogitsLoss</td><td style=\"text-align: right;\">1.73616e-05</td><td style=\"text-align: right;\">           7</td><td>SGD        </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.12917 </td><td style=\"text-align: right;\">  7.5368</td></tr>\n",
       "<tr><td>train_model_03ba5a41</td><td>TERMINATED</td><td>127.0.0.1:55148</td><td>relu           </td><td style=\"text-align: right;\">      0.195333</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss</td><td style=\"text-align: right;\">0.00398153 </td><td style=\"text-align: right;\">           3</td><td>RMSprop    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.46378 </td><td style=\"text-align: right;\"> 20.9032</td></tr>\n",
       "<tr><td>train_model_b43cbe67</td><td>TERMINATED</td><td>127.0.0.1:55158</td><td>relu           </td><td style=\"text-align: right;\">      0.218483</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss</td><td style=\"text-align: right;\">0.000426687</td><td style=\"text-align: right;\">           1</td><td>Adam       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.25947 </td><td style=\"text-align: right;\">125.654 </td></tr>\n",
       "<tr><td>train_model_0d71f827</td><td>TERMINATED</td><td>127.0.0.1:55163</td><td>gelu           </td><td style=\"text-align: right;\">      0.143011</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">                256</td><td>BCEWithLogitsLoss</td><td style=\"text-align: right;\">0.0027015  </td><td style=\"text-align: right;\">           4</td><td>Adam       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.721283</td><td style=\"text-align: right;\"> 20.9032</td></tr>\n",
       "<tr><td>train_model_c288d6a2</td><td>TERMINATED</td><td>127.0.0.1:55169</td><td>gelu           </td><td style=\"text-align: right;\">      0.240724</td><td style=\"text-align: right;\">      14</td><td style=\"text-align: right;\">                512</td><td>BCEWithLogitsLoss</td><td style=\"text-align: right;\">0.00231417 </td><td style=\"text-align: right;\">           7</td><td>Adam       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.71662 </td><td style=\"text-align: right;\"> 21.2843</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-04-11 12:58:22,735 E 55102 485938] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-04-11_12-58-10_337968_55028 is over 95% full, available space: 11.9065 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 1/14 - Train loss: 0.6933, Validation loss: 0.6933\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 2/14 - Train loss: 0.6933, Validation loss: 0.6933\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 3/14 - Train loss: 0.6933, Validation loss: 0.6933\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 4/14 - Train loss: 0.6933, Validation loss: 0.6933\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 5/14 - Train loss: 0.6933, Validation loss: 0.6933\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 6/14 - Train loss: 0.6933, Validation loss: 0.6932\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 7/14 - Train loss: 0.6933, Validation loss: 0.6932\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 8/14 - Train loss: 0.6933, Validation loss: 0.6932\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 9/14 - Train loss: 0.6933, Validation loss: 0.6932\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 10/14 - Train loss: 0.6933, Validation loss: 0.6932\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 11/14 - Train loss: 0.6933, Validation loss: 0.6932\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 12/14 - Train loss: 0.6933, Validation loss: 0.6932\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 13/14 - Train loss: 0.6933, Validation loss: 0.6932\n",
      "\u001b[36m(train_model pid=55127)\u001b[0m Epoch 14/14 - Train loss: 0.6933, Validation loss: 0.6932\n",
      "\u001b[36m(train_model pid=55158)\u001b[0m Epoch 2/15 - Train loss: 0.0785, Validation loss: 0.0694\u001b[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-04-11 12:58:32,821 E 55102 485938] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-04-11_12-58-10_337968_55028 is over 95% full, available space: 11.9045 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=55169)\u001b[0m Epoch 4/14 - Train loss: 0.0646, Validation loss: 0.0642\u001b[32m [repeated 23x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 12:58:36,355\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/I538952/ray_results/train_model_2025-04-11_12-58-14' in 0.0270s.\n",
      "2025-04-11 12:58:36,358\tINFO tune.py:1041 -- Total run time: 21.71 seconds (21.55 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'num_layers': 1, 'hidden_layer_size': 2048, 'dropout_rate': 0.2184829310348446, 'activation_fn': 'relu', 'optimizer': 'Adam', 'loss_fn': 'BCEWithLogitsLoss', 'lr': 0.0004266874847799782, 'epochs': 15}\n",
      "\u001b[36m(train_model pid=55169)\u001b[0m Epoch 14/14 - Train loss: 0.0633, Validation loss: 0.0637\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define search space for hyperparameter optimization\n",
    "search_space = {\n",
    "    \"num_layers\": tune.randint(1, 8),  # Vary the number of layers in the model\n",
    "    \"hidden_layer_size\": tune.choice([128, 256, 512, 1024, 2048]),  # Different sizes for hidden layers\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.4),  # Dropout rate between 0.1 and 0.4\n",
    "    \"activation_fn\": tune.choice([\"relu\", \"leaky_relu\", \"gelu\"]),  # Activation functions to choose from\n",
    "    \"optimizer\": tune.choice([\"Adam\", \"SGD\", \"RMSprop\"]),  # Optimizer options\n",
    "    \"loss_fn\": tune.choice([\"BCEWithLogitsLoss\"]),  # Loss function (currently using BCEWithLogitsLoss)\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),  # Learning rate in a log-uniform range\n",
    "    \"epochs\": tune.randint(5, 30),  # Number of epochs between 10 and 20\n",
    "}\n",
    "\n",
    "# Initialize Ray for hyperparameter optimization\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Optuna Search Algorithm for optimizing the hyperparameters\n",
    "optuna_search = OptunaSearch(metric=\"dice\", mode=\"max\")\n",
    "\n",
    "# Use ASHAScheduler to manage trials and early stopping\n",
    "scheduler = ASHAScheduler(metric=\"dice\", mode=\"max\")\n",
    "\n",
    "# Define and configure the Tuner for Ray Tune\n",
    "tuner = tune.Tuner(\n",
    "    train_model,  # The function to optimize (training function)\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,  # Search strategy using Optuna\n",
    "        scheduler=scheduler,  # Use ASHA to manage the trials\n",
    "        num_samples=250  # Number of trials to run\n",
    "    ),\n",
    "    param_space=search_space  # Pass in the defined hyperparameter search space\n",
    ")\n",
    "\n",
    "# Run the tuner\n",
    "results = tuner.fit()\n",
    "\n",
    "# Output the best configuration based on the 'dice' metric\n",
    "best_config = results.get_best_result(metric=\"dice\", mode=\"max\").config\n",
    "print(\"Best hyperparameters:\", best_config)\n",
    "\n",
    "# Shut down Ray after finishing the optimization\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mAnalysis\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/I538952/ray_results/train_model_2025-04-11_12-58-14\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m best_config \u001b[38;5;241m=\u001b[39m analysis\u001b[38;5;241m.\u001b[39mget_best_config(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdice\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# or mode=\"min\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest config: \u001b[39m\u001b[38;5;124m\"\u001b[39m, best_config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Analysis' is not defined"
     ]
    }
   ],
   "source": [
    "analysis = Analysis(\"/Users/I538952/ray_results/train_model_2025-04-11_12-58-14\")\n",
    "best_config = analysis.get_best_config(metric=\"dice\", mode=\"max\")  # or mode=\"min\"\n",
    "print(\"Best config: \", best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from /Users/I538952/ray_results/train_model_2025-04-11_12-58-14...\n",
      "Restored 5 trials from the experiment.\n",
      "Best result: Result(\n",
      "  metrics={'dice': 125.6536},\n",
      "  path='/Users/I538952/ray_results/train_model_2025-04-11_12-58-14/train_model_b43cbe67_3_activation_fn=relu,dropout_rate=0.2185,epochs=15,hidden_layer_size=2048,loss_fn=BCEWithLogitsLoss,lr=0.0004_2025-04-11_12-58-26',\n",
      "  filesystem='local',\n",
      "  checkpoint=None\n",
      ")\n",
      "Worst performing result: Result(\n",
      "  metrics={'dice': 7.5367999999999995},\n",
      "  path='/Users/I538952/ray_results/train_model_2025-04-11_12-58-14/train_model_08c42797_1_activation_fn=relu,dropout_rate=0.2974,epochs=14,hidden_layer_size=512,loss_fn=BCEWithLogitsLoss,lr=0.0000,_2025-04-11_12-58-14',\n",
      "  filesystem='local',\n",
      "  checkpoint=None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "experiment_path = \"/Users/I538952/ray_results/train_model_2025-04-11_12-58-14\"\n",
    "print(f\"Loading results from {experiment_path}...\")\n",
    "\n",
    "restored_tuner = tune.Tuner.restore(experiment_path, trainable=train_model)\n",
    "result_grid = restored_tuner.get_results()\n",
    "print(f\"Restored {len(result_grid)} trials from the experiment.\")\n",
    "\n",
    "# Get the result with the maximum `dice` metric\n",
    "best_result: Result = result_grid.get_best_result(metric=\"dice\", mode=\"max\")\n",
    "\n",
    "# Get the result with the minimum `mean_accuracy`\n",
    "worst_performing_result: Result = result_grid.get_best_result(\n",
    "    metric=\"dice\", mode=\"min\"\n",
    ")\n",
    "\n",
    "print(f\"Best result: {best_result}\")\n",
    "print(f\"Worst performing result: {worst_performing_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
