{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "\n",
    "import ray\n",
    "from ray.air import session\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "\n",
    "from graphMatching.gma import run_gma\n",
    "\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset import TwoStepHashDataset\n",
    "\n",
    "from pytorch_models_hyperparameter_optimization.base_model import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/fakename_10k.tsv\",\n",
    "    \"Overlap\": 0.8,\n",
    "    \"DropFrom\": \"Eve\",\n",
    "    \"Verbose\": True,  # Print Status Messages\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": -1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": False,\n",
    "}\n",
    "\n",
    "\n",
    "DEA_CONFIG = {\n",
    "    \"DevMode\": False,\n",
    "    \"BatchSize\": 32,\n",
    "    # TestSize calculated accordingly\n",
    "    \"TrainSize\": 0.8,\n",
    "    \"FilterThreshold\": 0.5,\n",
    "    \"Patience\": 5,\n",
    "    \"MinDelta\": 0.001,\n",
    "}\n",
    "\n",
    "ENC_CONFIG = {\n",
    "    # TwoStepHash / TabMinHash / BloomFilter\n",
    "    \"AliceAlgo\": \"BloomFilter\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"None\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "    # For BF encoding\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "    # For TMH encoding\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "    # For 2SH encoding\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    # For Node2Vec\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42\n",
    "}\n",
    "\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"]/2), #0005\n",
    "    \"RegInit\":1, # For BF 0.25\n",
    "    \"Batchsize\": 1, # 1 = 100%\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5 ,  # 800\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique hash identifiers for the encoding and embedding configurations\n",
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG)\n",
    "\n",
    "# Define file paths based on the configuration hashes\n",
    "path_reidentified = f\"./data/available_to_eve/reidentified_individuals_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "path_not_reidentified = f\"./data/available_to_eve/not_reidentified_individuals_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "path_all = f\"./data/dev/alice_data_complete_with_encoding_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "\n",
    "# Check if the output files already exist\n",
    "if os.path.isfile(path_reidentified) and os.path.isfile(path_not_reidentified) and os.path.isfile(path_all):\n",
    "    # Load previously saved attack results\n",
    "    reidentified_data = hkl.load(path_reidentified)\n",
    "    not_reidentified_data = hkl.load(path_not_reidentified)\n",
    "    all_data = hkl.load(path_all)\n",
    "\n",
    "else:\n",
    "    # Run Graph Matching Attack if files are not found\n",
    "    reidentified_data, not_reidentified_data, all_data = run_gma(\n",
    "        GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG,\n",
    "        eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash\n",
    "    )\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "df_reidentified = pd.DataFrame(reidentified_data[1:], columns=reidentified_data[0])\n",
    "df_not_reidentified = pd.DataFrame(not_reidentified_data[1:], columns=not_reidentified_data[0])\n",
    "df_all = pd.DataFrame(all_data[1:], columns=all_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate a dictionary of all possible 2-grams from letters and digits ---\n",
    "\n",
    "# Lowercase alphabet: 'a' to 'z'\n",
    "alphabet = string.ascii_lowercase\n",
    "\n",
    "# Digits: '0' to '9'\n",
    "digits = string.digits\n",
    "\n",
    "# Generate all letter-letter 2-grams (e.g., 'aa', 'ab', ..., 'zz')\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]\n",
    "\n",
    "# Generate all digit-digit 2-grams (e.g., '00', '01', ..., '99')\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]\n",
    "\n",
    "# Generate all letter-digit 2-grams (e.g., 'a0', 'a1', ..., 'z9')\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]\n",
    "\n",
    "# Combine all generated 2-grams into one list\n",
    "all_two_grams = letter_letter_grams + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Create a dictionary mapping index to each 2-gram\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Bloom Filter Encoding\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "    data_labeled = BloomFilterDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = BloomFilterDataset(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    input_layer_size = len(df_reidentified[\"bloomfilter\"][0])\n",
    "\n",
    "# 2️⃣ Tabulation MinHash Encoding\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "    data_labeled = TabMinHashDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TabMinHashDataset(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    input_layer_size = len(df_reidentified[\"tabminhash\"][0])\n",
    "\n",
    "# 3 Two-Step Hash Encoding (One-Hot Encoding Mode)\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\":\n",
    "    # Collect all unique integers across both reidentified and non-reidentified data\n",
    "    unique_ints_reid = set().union(*df_reidentified[\"twostephash\"])\n",
    "    unique_ints_not_reid = set().union(*df_not_reidentified[\"twostephash\"])\n",
    "    unique_ints_sorted = sorted(unique_ints_reid.union(unique_ints_not_reid))\n",
    "    unique_integers_dict = {i: val for i, val in enumerate(unique_ints_sorted)}\n",
    "    input_layer_size = len(unique_ints_sorted)\n",
    "\n",
    "    data_labeled = TwoStepHashDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_integers=unique_ints_sorted,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TwoStepHashDataset(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_integers=unique_ints_sorted,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset split proportions\n",
    "train_size = int(DEA_CONFIG[\"TrainSize\"] * len(data_labeled))\n",
    "val_size = len(data_labeled) - train_size\n",
    "\n",
    "# Split the reidentified dataset into training and validation sets\n",
    "data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "dataloader_train = DataLoader(\n",
    "    data_train,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True  # Important for training\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    data_val,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True  # Allows variation in validation batches\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    data_not_labeled,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Setup for Training\n",
    "\n",
    "This setup for hyperparameter tuning in a neural network model improves modularity, ensuring easy customization for experimentation.\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   - The model is initialized using hyperparameters from the `config` dictionary, including the number of layers, hidden layer size, dropout rate, and activation function.\n",
    "\n",
    "2. **Loss Function and Optimizer Selection**:\n",
    "   - The loss function (`criterion`) and optimizer are selected dynamically from the `config` dictionary.\n",
    "\n",
    "3. **Training & Validation Loop**:\n",
    "   - The training and validation phases are handled in separate loops. The loss is computed at each step, and metrics are logged.\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - After training, the model is evaluated on a test set, where 2-gram predictions are compared against the actual 2-grams.\n",
    "   - **Dice similarity coefficient** is used as a metric to evaluate model performance.\n",
    "\n",
    "5. **Custom Helper Functions**:\n",
    "   - `extract_two_grams_batch()`: Extracts 2-grams for all samples in the batch.\n",
    "   - `convert_to_two_gram_scores()`: Converts model output logits into 2-gram scores.\n",
    "   - `filter_two_grams()`: Applies a threshold to filter 2-gram scores.\n",
    "   - `filter_two_grams_per_uid()`: Filters and formats 2-gram predictions for each UID.\n",
    "\n",
    "6. **Hyperparameter Tuning**:\n",
    "   - The setup is integrated with **Ray Tune** (`tune.report`) to enable hyperparameter tuning by reporting the Dice similarity metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Initialize lists to store training and validation losses\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # Define and initialize model with hyperparameters from config\n",
    "    model = BaseModel(\n",
    "        input_dim=input_layer_size,\n",
    "        output_dim=len(all_two_grams),\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        activation_fn=config[\"activation_fn\"]\n",
    "    )\n",
    "\n",
    "    # Set device for model (GPU or CPU)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Select loss function based on config\n",
    "    loss_functions = {\n",
    "        \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss(),\n",
    "        \"MultiLabelSoftMarginLoss\": nn.MultiLabelSoftMarginLoss()\n",
    "    }\n",
    "    criterion = loss_functions[config[\"loss_fn\"]]\n",
    "\n",
    "    # Select optimizer based on config\n",
    "    optimizers = {\n",
    "        \"Adam\": optim.Adam(model.parameters(), lr=config[\"lr\"]),\n",
    "        \"SGD\": optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9),\n",
    "        \"RMSprop\": optim.RMSprop(model.parameters(), lr=config[\"lr\"])\n",
    "    }\n",
    "    optimizer = optimizers[config[\"optimizer\"]]\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = run_epoch(model, dataloader_train, criterion, optimizer, device, is_training=True)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(model, dataloader_val, criterion, optimizer, device, is_training=False)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Log metrics\n",
    "        log_metrics(train_loss, val_loss, epoch, config[\"epochs\"])\n",
    "\n",
    "    # Test phase with reconstruction and evaluation\n",
    "    model.eval()\n",
    "    threshold = DEA_CONFIG[\"FilterThreshold\"]\n",
    "    sum_dice = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data_batch, uids in dataloader_test:\n",
    "            # Process the test data\n",
    "            filtered_df = df_all[df_all[\"uid\"].isin(uids)].drop(df_all.columns[-2], axis=1)\n",
    "            actual_two_grams_batch = extract_two_grams_batch(filtered_df)\n",
    "\n",
    "            # Move data to device and make predictions\n",
    "            data_batch = data_batch.to(device)\n",
    "            logits = model(data_batch)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities into 2-gram scores\n",
    "            batch_two_gram_scores = convert_to_two_gram_scores(probabilities)\n",
    "\n",
    "            # Filter out low-scoring 2-grams\n",
    "            batch_filtered_two_gram_scores = filter_two_grams(batch_two_gram_scores, threshold)\n",
    "            filtered_two_grams = combine_two_grams_with_uid(uids, batch_filtered_two_gram_scores)\n",
    "\n",
    "            # Calculate Dice similarity for evaluation\n",
    "            sum_dice += calculate_dice_similarity(actual_two_grams_batch, filtered_two_grams)\n",
    "\n",
    "    # Report evaluation metric\n",
    "    train.report({\"dice\": sum_dice})\n",
    "\n",
    "\n",
    "def run_epoch(model, dataloader, criterion, optimizer, device, is_training):\n",
    "    running_loss = 0.0\n",
    "    with torch.set_grad_enabled(is_training):\n",
    "        for data, labels, _ in dataloader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if is_training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def extract_two_grams_batch(df):\n",
    "    return [\n",
    "        {\"uid\": entry[\"uid\"], \"two_grams\": extract_two_grams(\"\".join(map(str, entry[:-1])))}\n",
    "        for _, entry in df.iterrows()\n",
    "    ]\n",
    "\n",
    "\n",
    "def convert_to_two_gram_scores(probabilities):\n",
    "    return [\n",
    "        {two_gram_dict[j]: score.item() for j, score in enumerate(probabilities[i])}\n",
    "        for i in range(probabilities.size(0))\n",
    "    ]\n",
    "\n",
    "\n",
    "def filter_two_grams(two_gram_scores, threshold):\n",
    "    return [\n",
    "        {two_gram: score for two_gram, score in two_gram_scores.items() if score > threshold}\n",
    "        for two_gram_scores in two_gram_scores\n",
    "    ]\n",
    "\n",
    "\n",
    "def combine_two_grams_with_uid(uids, filtered_two_gram_scores):\n",
    "    return [\n",
    "        {\"uid\": uid, \"two_grams\": {key for key in two_grams.keys()}}\n",
    "        for uid, two_grams in zip(uids, filtered_two_gram_scores)\n",
    "    ]\n",
    "\n",
    "def calculate_dice_similarity(actual_two_grams_batch, filtered_two_grams):\n",
    "    sum_dice = 0\n",
    "    for entry_two_grams_batch in actual_two_grams_batch:\n",
    "        for entry_filtered_two_grams in filtered_two_grams:\n",
    "            if entry_two_grams_batch[\"uid\"] == entry_filtered_two_grams[\"uid\"]:\n",
    "                sum_dice += dice_coefficient(entry_two_grams_batch[\"two_grams\"], entry_filtered_two_grams[\"two_grams\"])\n",
    "    return sum_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space for hyperparameter optimization\n",
    "search_space = {\n",
    "    \"num_layers\": tune.randint(1, 8),  # Vary the number of layers in the model\n",
    "    \"hidden_layer_size\": tune.choice([128, 256, 512, 1024, 2048]),  # Different sizes for hidden layers\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.4),  # Dropout rate between 0.1 and 0.4\n",
    "    \"activation_fn\": tune.choice([\"relu\", \"leaky_relu\", \"gelu\"]),  # Activation functions to choose from\n",
    "    \"optimizer\": tune.choice([\"Adam\", \"SGD\", \"RMSprop\"]),  # Optimizer options\n",
    "    \"loss_fn\": tune.choice([\"BCEWithLogitsLoss\"]),  # Loss function (currently using BCEWithLogitsLoss)\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),  # Learning rate in a log-uniform range\n",
    "    \"epochs\": tune.randint(5, 30),  # Number of epochs between 10 and 20\n",
    "}\n",
    "\n",
    "# Initialize Ray for hyperparameter optimization\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Optuna Search Algorithm for optimizing the hyperparameters\n",
    "optuna_search = OptunaSearch(metric=\"dice\", mode=\"max\")\n",
    "\n",
    "# Use ASHAScheduler to manage trials and early stopping\n",
    "scheduler = ASHAScheduler(metric=\"dice\", mode=\"max\")\n",
    "\n",
    "# Define and configure the Tuner for Ray Tune\n",
    "tuner = tune.Tuner(\n",
    "    train_model,  # The function to optimize (training function)\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,  # Search strategy using Optuna\n",
    "        scheduler=scheduler,  # Use ASHA to manage the trials\n",
    "        num_samples=5  # Number of trials to run\n",
    "    ),\n",
    "    param_space=search_space  # Pass in the defined hyperparameter search space\n",
    ")\n",
    "\n",
    "# Run the tuner\n",
    "results = tuner.fit()\n",
    "\n",
    "# Output the best configuration based on the 'dice' metric\n",
    "best_config = results.get_best_result(metric=\"dice\", mode=\"max\").config\n",
    "print(\"Best hyperparameters:\", best_config)\n",
    "\n",
    "# Shut down Ray after finishing the optimization\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = \"\"\n",
    "\n",
    "# Restore the tuner from a previous experiment\n",
    "restored_tuner = tune.Tuner.restore(experiment_path, trainable=train_model)\n",
    "result_grid = restored_tuner.get_results()\n",
    "\n",
    "# Get the best and worst result based on the \"dice\" metric\n",
    "best_result = result_grid.get_best_result(metric=\"dice\", mode=\"max\")\n",
    "worst_result = result_grid.get_best_result(metric=\"dice\", mode=\"min\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
