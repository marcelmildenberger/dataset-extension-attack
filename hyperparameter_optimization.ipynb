{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "import ray\n",
    "from ray import tune, air\n",
    "from ray import train\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from utils import get_hashes, convert_to_two_gram_scores, filter_two_grams, calculate_performance_metrics, run_epoch, label_tensors_to_two_grams\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "import string\n",
    "from early_stopping.early_stopping import EarlyStopping\n",
    "\n",
    "from graphMatching.gma import run_gma\n",
    "\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset import TwoStepHashDataset\n",
    "\n",
    "from pytorch_models_hyperparameter_optimization.base_model import BaseModel\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/fakename_10k.tsv\",\n",
    "    \"Overlap\": 0.8,\n",
    "    \"DropFrom\": \"Eve\",\n",
    "    \"Verbose\": False,  # Print Status Messages\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": -1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": False,\n",
    "}\n",
    "\n",
    "\n",
    "DEA_CONFIG = {\n",
    "    \"DevMode\": False,\n",
    "    \"BatchSize\": 64,\n",
    "    # TestSize calculated accordingly\n",
    "    \"TrainSize\": 0.8,\n",
    "    \"FilterThreshold\": 0.5,\n",
    "    \"Patience\": 10,\n",
    "    \"MinDelta\": 1e-4,\n",
    "    \"NumSamples\": 100,\n",
    "    \"Epochs\": 20,\n",
    "}\n",
    "\n",
    "ENC_CONFIG = {\n",
    "    # TwoStepHash / TabMinHash / BloomFilter\n",
    "    \"AliceAlgo\": \"BloomFilter\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"None\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "    # For BF encoding\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "    # For TMH encoding\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "    # For 2SH encoding\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    # For Node2Vec\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42\n",
    "}\n",
    "\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"]/2), #0005\n",
    "    \"RegInit\":1, # For BF 0.25\n",
    "    \"Batchsize\": 1, # 1 = 100%\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5 ,  # 800\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique hash identifiers for the encoding and embedding configurations\n",
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG)\n",
    "\n",
    "# Define file paths based on the configuration hashes\n",
    "path_reidentified = f\"./data/available_to_eve/reidentified_individuals_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "path_all = f\"./data/dev/alice_data_complete_with_encoding_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "\n",
    "# Check if the output files already exist\n",
    "if os.path.isfile(path_reidentified) and os.path.isfile(path_all):\n",
    "    # Load previously saved attack results\n",
    "    reidentified_data = hkl.load(path_reidentified)\n",
    "    all_data = hkl.load(path_all)\n",
    "\n",
    "else:\n",
    "    # Run Graph Matching Attack if files are not found\n",
    "    reidentified_data, _, all_data = run_gma(\n",
    "        GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG,\n",
    "        eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash\n",
    "    )\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "df_reidentified = pd.DataFrame(reidentified_data[1:], columns=reidentified_data[0])\n",
    "df_all = pd.DataFrame(all_data[1:], columns=all_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate a dictionary of all possible 2-grams from letters and digits ---\n",
    "\n",
    "# Lowercase alphabet: 'a' to 'z'\n",
    "alphabet = string.ascii_lowercase\n",
    "\n",
    "# Digits: '0' to '9'\n",
    "digits = string.digits\n",
    "\n",
    "# Generate all letter-letter 2-grams (e.g., 'aa', 'ab', ..., 'zz')\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]\n",
    "\n",
    "# Generate all digit-digit 2-grams (e.g., '00', '01', ..., '99')\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]\n",
    "\n",
    "# Generate all letter-digit 2-grams (e.g., 'a0', 'a1', ..., 'z9')\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]\n",
    "\n",
    "# Combine all generated 2-grams into one list\n",
    "all_two_grams = letter_letter_grams + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Create a dictionary mapping index to each 2-gram\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Bloom Filter Encoding\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "    data_labeled = BloomFilterDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    input_layer_size = len(df_reidentified[\"bloomfilter\"][0])\n",
    "\n",
    "# 2️⃣ Tabulation MinHash Encoding\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "    data_labeled = TabMinHashDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    input_layer_size = len(df_reidentified[\"tabminhash\"][0])\n",
    "\n",
    "# 3 Two-Step Hash Encoding (One-Hot Encoding Mode)\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\":\n",
    "    # Collect all unique integers across both reidentified and non-reidentified data\n",
    "    unique_ints = sorted(set().union(*df_all[\"twostephash\"]))\n",
    "    unique_integers_dict = {i: val for i, val in enumerate(unique_ints)}\n",
    "    input_layer_size = len(unique_ints)\n",
    "\n",
    "    data_labeled = TwoStepHashDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_integers=unique_ints,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset split proportions\n",
    "train_size = int(DEA_CONFIG[\"TrainSize\"] * len(data_labeled))\n",
    "val_size = len(data_labeled) - train_size\n",
    "\n",
    "# Split the reidentified dataset into training and validation sets\n",
    "data_train, data_val = random_split(data_labeled, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Setup for Training\n",
    "\n",
    "This setup for hyperparameter tuning in a neural network model improves modularity, ensuring easy customization for experimentation.\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   - The model is initialized using hyperparameters from the `config` dictionary, including the number of layers, hidden layer size, dropout rate, and activation function.\n",
    "\n",
    "2. **Loss Function and Optimizer Selection**:\n",
    "   - The loss function (`criterion`) and optimizer are selected dynamically from the `config` dictionary.\n",
    "\n",
    "3. **Training & Validation Loop**:\n",
    "   - The training and validation phases are handled in separate loops. The loss is computed at each step, and metrics are logged.\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - After training, the model is evaluated on a test set, where 2-gram predictions are compared against the actual 2-grams.\n",
    "   - **Dice similarity coefficient** is used as a metric to evaluate model performance.\n",
    "\n",
    "5. **Custom Helper Functions**:\n",
    "   - `extract_two_grams_batch()`: Extracts 2-grams for all samples in the batch.\n",
    "   - `convert_to_two_gram_scores()`: Converts model output logits into 2-gram scores.\n",
    "   - `filter_two_grams()`: Applies a threshold to filter 2-gram scores.\n",
    "   - `filter_two_grams_per_uid()`: Filters and formats 2-gram predictions for each UID.\n",
    "\n",
    "6. **Hyperparameter Tuning**:\n",
    "   - The setup is integrated with **Ray Tune** (`tune.report`) to enable hyperparameter tuning by reporting the Dice similarity metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Create DataLoaders for training, validation, and testing\n",
    "    dataloader_train = DataLoader(\n",
    "        config[\"data_train\"],\n",
    "        batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "        shuffle=True  # Important for training\n",
    "    )\n",
    "\n",
    "    dataloader_val = DataLoader(\n",
    "        config[\"data_val\"],\n",
    "        batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "        shuffle=False  # Allows variation in validation batches\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    total_precision = total_recall = total_f1 = total_dice = 0.0\n",
    "    n = len(dataloader_val.dataset)\n",
    "    epochs = 0\n",
    "    early_stopper = EarlyStopping(patience=DEA_CONFIG[\"Patience\"], min_delta=DEA_CONFIG[\"MinDelta\"])\n",
    "\n",
    "    # Define and initialize model with hyperparameters from config\n",
    "    model = BaseModel(\n",
    "        input_dim=config[\"input_dim\"],\n",
    "        output_dim=config[\"output_dim\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        activation_fn=config[\"activation_fn\"]\n",
    "    )\n",
    "\n",
    "    # Set device for model (GPU or CPU)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Select loss function based on config\n",
    "    loss_functions = {\n",
    "        \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss(),\n",
    "        \"MultiLabelSoftMarginLoss\": nn.MultiLabelSoftMarginLoss(),\n",
    "        \"FocalLoss\": nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1.0)),\n",
    "        \"WeightedBCE\": nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1.0)),\n",
    "        \"CrossEntropyLoss\": nn.CrossEntropyLoss(),\n",
    "        \"MSELoss\": nn.MSELoss(),\n",
    "        \"L1Loss\": nn.L1Loss(),\n",
    "    }\n",
    "    criterion = loss_functions[config[\"loss_fn\"]]\n",
    "\n",
    "    learning_rate = config[\"optimizer\"][\"lr\"].sample()\n",
    "    # Select optimizer based on config\n",
    "    optimizers = {\n",
    "        \"Adam\": lambda: optim.Adam(model.parameters(), lr=learning_rate),\n",
    "        \"AdamW\": lambda: optim.AdamW(model.parameters(), lr=learning_rate),\n",
    "        \"SGD\": lambda: optim.SGD(model.parameters(), lr=learning_rate, momentum=config[\"optimizer\"][\"momentum\"].sample()),\n",
    "        \"RMSprop\": lambda: optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    }\n",
    "    optimizer = optimizers[config[\"optimizer\"][\"name\"]]()\n",
    "\n",
    "    schedulers = {\n",
    "        \"StepLR\": lambda: torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config[\"lr_scheduler\"][\"step_size\"].sample(),\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ExponentialLR\": lambda: torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ReduceLROnPlateau\": lambda: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=config[\"lr_scheduler\"][\"mode\"],\n",
    "            factor=config[\"lr_scheduler\"][\"factor\"].sample(),\n",
    "            patience=config[\"lr_scheduler\"][\"patience\"].sample()\n",
    "        ),\n",
    "        \"CosineAnnealingLR\": lambda: torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config[\"lr_scheduler\"][\"T_max\"].sample(),\n",
    "            eta_min=config[\"lr_scheduler\"][\"eta_min\"].sample()\n",
    "        ),\n",
    "        \"CyclicLR\": lambda: torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=config[\"lr_scheduler\"][\"base_lr\"].sample(),\n",
    "            max_lr=config[\"lr_scheduler\"][\"max_lr\"].sample(),\n",
    "            step_size_up=config[\"lr_scheduler\"][\"step_size_up\"].sample(),\n",
    "            mode=config[\"lr_scheduler\"][\"mode_cyclic\"].sample(),\n",
    "            cycle_momentum=False\n",
    "        ),\n",
    "        \"None\": lambda: None,\n",
    "    }\n",
    "    scheduler = schedulers[config[\"lr_scheduler\"][\"name\"]]()\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(DEA_CONFIG[\"Epochs\"]):\n",
    "        epochs += 1\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = run_epoch(model, dataloader_train, criterion, optimizer, device, is_training=True, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(model, dataloader_val, criterion, optimizer, device, is_training=False, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "         # Early stopping check\n",
    "        if early_stopper(val_loss):\n",
    "            break\n",
    "\n",
    "    # Test phase with reconstruction and evaluation\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in dataloader_val:\n",
    "\n",
    "            actual_two_grams = label_tensors_to_two_grams(two_gram_dict, labels)\n",
    "\n",
    "            # Move data to device and make predictions\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities into 2-gram scores\n",
    "            batch_two_gram_scores = convert_to_two_gram_scores(two_gram_dict, probabilities)\n",
    "\n",
    "            # Filter out low-scoring 2-grams\n",
    "            batch_filtered_two_gram_scores = filter_two_grams(batch_two_gram_scores, config[\"threshold\"])\n",
    "\n",
    "            # Calculate performance metrics for evaluation\n",
    "            dice, precision, recall, f1 = calculate_performance_metrics(\n",
    "                actual_two_grams, batch_filtered_two_gram_scores)\n",
    "\n",
    "            total_dice += dice\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "\n",
    "    train.report({\n",
    "            \"average_dice\": total_dice / n,\n",
    "            \"average_precision\": total_precision / n,\n",
    "            \"average_recall\": total_recall / n,\n",
    "            \"average_f1\": total_f1 / n,\n",
    "            \"len_train\": len(dataloader_train.dataset),\n",
    "            \"len_val\": len(dataloader_val.dataset),\n",
    "            \"epochs\": epochs\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space for hyperparameter optimization\n",
    "search_space = {\n",
    "    \"num_layers\": tune.randint(1, 8),  # Vary the number of layers in the model\n",
    "    \"hidden_layer_size\": tune.choice([64, 128, 256, 512, 1024, 2048]),  # Different sizes for hidden layers\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.4),  # Dropout rate between 0.1 and 0.4\n",
    "    \"activation_fn\": tune.choice([\"relu\", \"leaky_relu\", \"gelu\", \"elu\", \"selu\", \"tanh\"]),  # Activation functions to choose from\n",
    "    \"optimizer\": tune.choice([\n",
    "        {\"name\": \"Adam\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"AdamW\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"SGD\", \"lr\": tune.loguniform(1e-4, 1e-2), \"momentum\": tune.uniform(0.0, 0.99)},\n",
    "        {\"name\": \"RMSprop\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "    ]),\n",
    "    \"loss_fn\": tune.choice([\"BCEWithLogitsLoss\", \"MultiLabelSoftMarginLoss\", \"FocalLoss\", \"WeightedBCE\"]),\n",
    "    \"threshold\": tune.uniform(0.3, 0.8),  # oder: DEA_CONFIG[\"FilterThreshold\"]\n",
    "    \"input_dim\": input_layer_size,\n",
    "    \"output_dim\": len(all_two_grams),\n",
    "    \"data_train\": data_train,\n",
    "    \"data_val\": data_val,\n",
    "    \"lr_scheduler\": tune.choice([\n",
    "        {\"name\": \"StepLR\", \"step_size\": tune.choice([5, 10, 20]), \"gamma\": tune.uniform(0.1, 0.9)},\n",
    "        {\"name\": \"ExponentialLR\", \"gamma\": tune.uniform(0.85, 0.99)},\n",
    "        {\"name\": \"ReduceLROnPlateau\", \"mode\": \"min\", \"factor\": tune.uniform(0.1, 0.5), \"patience\": tune.choice([5, 10, 15])},\n",
    "        {\"name\": \"CosineAnnealingLR\", \"T_max\": tune.loguniform(10, 50) , \"eta_min\": tune.choice([1e-5, 1e-6, 0])},\n",
    "        {\"name\": \"CyclicLR\", \"base_lr\": tune.loguniform(1e-5, 1e-3), \"max_lr\": tune.loguniform(1e-3, 1e-1), \"step_size_up\": tune.choice([2000, 4000]), \"mode_cyclic\": tune.choice([\"triangular\", \"triangular2\", \"exp_range\"]) },\n",
    "        {\"name\": \"None\"}  # No scheduler\n",
    "    ])\n",
    "}\n",
    "\n",
    "selected_dataset = GLOBAL_CONFIG[\"Data\"].split(\"/\")[-1].replace(\".tsv\", \"\")\n",
    "\n",
    "experiment_tag = \"experiment_\" + ENC_CONFIG[\"AliceAlgo\"] + \"_\" + selected_dataset + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "local_dir = os.path.abspath(\"./ray_results\")\n",
    "\n",
    "# Initialize Ray for hyperparameter optimization\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Optuna Search Algorithm for optimizing the hyperparameters\n",
    "optuna_search = OptunaSearch(metric=\"average_dice\", mode=\"max\")\n",
    "\n",
    "# Use ASHAScheduler to manage trials and early stopping\n",
    "scheduler = ASHAScheduler(metric=\"average_dice\", mode=\"max\")\n",
    "\n",
    "# Define and configure the Tuner for Ray Tune\n",
    "tuner = tune.Tuner(\n",
    "    train_model,  # The function to optimize (training function)\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,  # Search strategy using Optuna\n",
    "        scheduler=scheduler,  # Use ASHA to manage the trials\n",
    "        num_samples=DEA_CONFIG[\"NumSamples\"],  # Number of trials to run\n",
    "        max_concurrent_trials=5  # or 1 if your models are heavy\n",
    "    ),\n",
    "    run_config=air.RunConfig(\n",
    "        name=experiment_tag,\n",
    "        local_dir=local_dir\n",
    "    ),\n",
    "    param_space=search_space  # Pass in the defined hyperparameter search space\n",
    ")\n",
    "\n",
    "# Run the tuner\n",
    "results = tuner.fit()\n",
    "\n",
    "# Shut down Ray after finishing the optimization\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keys to remove\n",
    "keys_to_remove = [\n",
    "    \"config\", \"checkpoint_dir_name\", \"experiment_tag\", \"done\", \"training_iteration\",\n",
    "    \"trial_id\", \"date\", \"time_this_iter_s\", \"pid\", \"time_total_s\", \"hostname\",\n",
    "    \"node_ip\", \"time_since_restore\", \"iterations_since_restore\", \"timestamp\",\n",
    "    \"input_dim\", \"output_dim\", \"data_train\", \"data_val\"\n",
    "]\n",
    "\n",
    "def clean_result_dict(result_dict):\n",
    "    for key in keys_to_remove:\n",
    "        result_dict.pop(key, None)\n",
    "    return result_dict\n",
    "\n",
    "def resolve_config(config):\n",
    "    resolved = {}\n",
    "    for k, v in config.items():\n",
    "        # If the value is a dictionary, recurse and apply resolve_config\n",
    "        if isinstance(v, dict):\n",
    "            resolved[k] = resolve_config(v)\n",
    "        # If the value is a Ray search sample object (e.g., Float, Categorical)\n",
    "        elif not isinstance(v, (int, float, str, Subset)):\n",
    "            resolved[k] = v.sample()  # Get the concrete value from the sample\n",
    "        else:\n",
    "            resolved[k] = v  # Leave it as-is if it's not a sample object or Subset\n",
    "    return resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to = f\"experiment_results/{experiment_tag}\"\n",
    "os.makedirs(save_to, exist_ok=True)\n",
    "result_grid = results\n",
    "\n",
    "# Best and worst result based on dice\n",
    "best_result = result_grid.get_best_result(metric=\"average_dice\", mode=\"max\")\n",
    "worst_result = result_grid.get_best_result(metric=\"average_dice\", mode=\"min\")\n",
    "\n",
    "# Combine configs and metrics into a DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        **clean_result_dict(resolve_config(result.config)),\n",
    "        **{k: result.metrics.get(k) for k in [\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]},\n",
    "    }\n",
    "    for result in result_grid\n",
    "])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(f\"{save_to}/all_trial_results.csv\", index=False)\n",
    "print(\"✅ Results saved to all_trial_results.csv\")\n",
    "\n",
    "\n",
    "def print_result(label, result):\n",
    "    print(f\"\\n🔍 {label}\")\n",
    "    print(\"-\" * 40)\n",
    "    cleaned_config = resolve_config(result.config)\n",
    "    print(f\"Config: {cleaned_config}\")\n",
    "    print(f\"Average Dice: {result.metrics.get('average_dice'):.4f}\")\n",
    "    print(f\"Average Precision: {result.metrics.get('average_precision'):.4f}\")\n",
    "    print(f\"Average Recall: {result.metrics.get('average_recall'):.4f}\")\n",
    "    print(f\"Average F1: {result.metrics.get('average_f1'):.4f}\")\n",
    "    result_dict = {**cleaned_config, **result.metrics}\n",
    "    clean_result_dict(result_dict)\n",
    "    # Convert to a DataFrame and save\n",
    "    df = pd.DataFrame([result_dict])\n",
    "    df.to_csv(f\"{save_to}/{label}.csv\", index=False)\n",
    "\n",
    "print_result(\"Best_Result\", best_result)\n",
    "print_result(\"Worst_Result\", worst_result)\n",
    "\n",
    "# Compute and print average metrics\n",
    "print(\"\\n📊 Average Metrics Across All Trials\")\n",
    "avg_metrics = df[[\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]].mean()\n",
    "print(\"-\" * 40)\n",
    "for key, value in avg_metrics.items():\n",
    "    print(f\"{key.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# --- 📈 Plotting performance metrics ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[[\"average_dice\", \"average_recall\", \"average_f1\", \"average_precision\"]])\n",
    "plt.title(\"Distribution of Performance Metrics Across Trials\")\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{save_to}/metric_distributions.png\")\n",
    "print(\"📊 Saved plot: metric_distributions.png\")\n",
    "\n",
    "# --- 📌 Correlation between config params and performance ---\n",
    "# Only include numeric config columns\n",
    "exclude_cols = {\"input_dim\", \"output_dim\"}\n",
    "numeric_config_cols = [\n",
    "    col for col in df.columns\n",
    "    if pd.api.types.is_numeric_dtype(df[col]) and col not in exclude_cols\n",
    "]\n",
    "correlation_df = df[numeric_config_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Between Parameters and Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{save_to}/correlation_heatmap.png\")\n",
    "print(\"📌 Saved heatmap: correlation_heatmap.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
