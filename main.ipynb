{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL): Investigating Dataset Extension Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from graphMatching.gma import run_gma\n",
    "\n",
    "from datasets.BloomFilterDataset import BloomFilterDataset\n",
    "from datasets.TabMinHashDataset import TabMinHashDataset\n",
    "from datasets.TwoStepHashDatasetPadding import TwoStepHashDatasetPadding\n",
    "from datasets.TwoStepHashDatasetFrequencyString import TwoStepHashDatasetFrequencyString\n",
    "\n",
    "from pytorch_models.BloomFilterToTwoGramClassifier import BloomFilterToTwoGramClassifier\n",
    "from pytorch_models.TabMinHashToTwoGramClassifier import TabMinHashToTwoGramClassifier\n",
    "from pytorch_models.TwoStepHashToTwoGramClassifier import TwoStepHashToTwoGramClassifier\n",
    "\n",
    "print('System Version:', sys.version)\n",
    "print('PyTorch version', torch.__version__)\n",
    "print('Torchvision version', torchvision.__version__)\n",
    "print('Numpy version', np.__version__)\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/titanic_full.tsv\",\n",
    "    \"Overlap\": 0.9,\n",
    "    \"DropFrom\": \"Both\",\n",
    "    \"Verbose\": True,  # Print Status Messages\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": -1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "}\n",
    "\n",
    "ENC_CONFIG = {\n",
    "    \"AliceAlgo\": \"TabMinHash\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"TabMinHash\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "    # For BF encoding\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "    # For TMH encoding\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "    # For 2SH encoding\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    # For Node2Vec\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42\n",
    "}\n",
    "\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"]/2), #0005\n",
    "    \"RegInit\":1, # For BF 0.25\n",
    "    \"Batchsize\": 1, # 1 = 100%\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5 ,  # 800\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True\n",
    "}\n",
    "\n",
    "DEA_CONFIG = {\n",
    "    \"TSHPadding\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reidentified_individuals, not_reidentified_individuals = run_gma(GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG)\n",
    "\n",
    "df_reidentified_individuals = pd.DataFrame(reidentified_individuals[1:], columns=reidentified_individuals[0])\n",
    "df_not_reidentified_individuals = pd.DataFrame(not_reidentified_individuals[1:], columns=not_reidentified_individuals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If GMA should be skipped and data be loaded from disk\n",
    "reidentified_individuals = pd.read_csv('./data/available_to_eve/reidentified_individuals.tsv', delimiter='\\t')\n",
    "df_reidentified_individuals = reidentified_individuals\n",
    "\n",
    "not_reidentified_individuals = hkl.load('./data/available_to_eve/not_reidentified_individuals.h5')\n",
    "df_not_reidentified_individuals = pd.DataFrame(not_reidentified_individuals[1:], columns=not_reidentified_individuals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reidentified Individuals:')\n",
    "print(df_reidentified_individuals.head())\n",
    "print('Not Reidentified Individuals:')\n",
    "print(df_not_reidentified_individuals.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate all 2-grams\n",
    "alphabet = string.ascii_lowercase\n",
    "# Generate all letter-letter 2-grams (aa-zz)\n",
    "alphabet = string.ascii_lowercase\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]\n",
    "\n",
    "# Generate all digit-digit 2-grams (00-99)\n",
    "digits = string.digits\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]\n",
    "\n",
    "# Generate all letter-digit 2-grams (a0-z9)\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]\n",
    "\n",
    "# Combine all sets\n",
    "all_two_grams = letter_letter_grams  + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Get a dictionary associating each 2-gram with an index\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}\n",
    "\n",
    "# Create Datasets\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "    data_labeled = BloomFilterDataset(df_reidentified_individuals, isLabeled=True, all_two_grams=all_two_grams)\n",
    "    data_not_labeled = BloomFilterDataset(df_not_reidentified_individuals, isLabeled=False, all_two_grams=all_two_grams)\n",
    "    bloomfilter_length = len(data_labeled[0][0])\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "    data_labeled = TabMinHashDataset(df_reidentified_individuals, isLabeled=True, all_two_grams=all_two_grams)\n",
    "    data_not_labeled = TabMinHashDataset(df_not_reidentified_individuals, isLabeled=False, all_two_grams=all_two_grams)\n",
    "    tabminhash_length = len(data_labeled[0][0])\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\" & DEA_CONFIG[\"TSHPadding\"]:\n",
    "    max_length_reidentified = df_reidentified_individuals[\"twostephash\"].apply(lambda x: len(list(x))).max()\n",
    "    max_length_not_reidentified = df_not_reidentified_individuals[\"twostephash\"].apply(lambda x: len(list(x))).max()\n",
    "    max_twostephash_length = max(max_length_reidentified, max_length_not_reidentified)\n",
    "    data_labeled = TwoStepHashDatasetPadding(df_reidentified_individuals, isLabeled=True, all_two_grams=all_two_grams, max_length=max_twostephash_length)\n",
    "    data_not_labeled = TwoStepHashDatasetPadding(df_not_reidentified_individuals, isLabeled=False, all_two_grams=all_two_grams, max_length=max_twostephash_length)\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\" &  (not DEA_CONFIG[\"TSHPadding\"]):\n",
    "    max_length_reidentified = df_reidentified_individuals[\"twostephash\"].apply(lambda x: max(x)).max()\n",
    "    max_length_not_reidentified = df_not_reidentified_individuals[\"twostephash\"].apply(lambda x: max(x)).max()\n",
    "    max_twostephash_length = max(max_length_reidentified, max_length_not_reidentified)\n",
    "    data_labeled = TwoStepHashDatasetFrequencyString(df_reidentified_individuals, isLabeled=True, all_two_grams=all_two_grams, max_length=max_twostephash_length)\n",
    "    data_not_labeled = TwoStepHashDatasetFrequencyString(df_not_reidentified_individuals, isLabeled=False, all_two_grams=all_two_grams, max_length=max_twostephash_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split proportions\n",
    "train_size = int(0.8 * len(data_labeled))  # 80% training\n",
    "val_size = len(data_labeled) - train_size  # 20% validation\n",
    "\n",
    "# Split dataset\n",
    "data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "dataloader_train = DataLoader(data_train, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(data_val, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(data_not_labeled, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "    model = BloomFilterToTwoGramClassifier(input_dim=bloomfilter_length, num_two_grams=len(all_two_grams))\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "    model = TabMinHashToTwoGramClassifier(input_dim=tabminhash_length, num_two_grams=len(all_two_grams))\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\" & DEA_CONFIG[\"TSHPadding\"]:\n",
    "    model = TwoStepHashToTwoGramClassifier(input_dim=max_twostephash_length, num_two_grams=len(all_two_grams))\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\" & (not DEA_CONFIG[\"TSHPadding\"]):\n",
    "    model = TabMinHashToTwoGramClassifier(input_dim=tabminhash_length, num_two_grams=len(all_two_grams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function for multi-label classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 15\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for bloom_filters, labels in tqdm(dataloader_train, desc=\"Training loop\"):\n",
    "        # Move data to device\n",
    "        bloom_filters, labels = bloom_filters.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(bloom_filters)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    train_loss = running_loss / len(dataloader_train.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for bloom_filters, labels in tqdm(dataloader_val, desc=\"Validation loop\"):\n",
    "            # Move data to device\n",
    "            bloom_filters, labels = bloom_filters.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(bloom_filters)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "        val_loss = running_loss / len(dataloader_val.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Losses and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Identify Not-Reidentified Individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance for Re-Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First entry for reference (labeled data):\n",
    "# surname                                                  Hegarty\n",
    "# firstname                                    Miss. Hanora \"Nora\"\n",
    "# bloomfilter    0000000010100011000100000101000000000000100000001000000000100111011000001001100100000100000000001000000000000000001000100000000000010010000000000000100010001110110111000000000000100000000100000001010000000000100101000011000010001010000001000000000000000000001000011010011001000100000011100100000000000011000100100000110011000000000010000000000010000000000000110000000110000000000010000000011100000001000000100000001100101011001000000000010000001000000000001000010110110000000001001000100001010111010000000010000000111000000000010010110000000000001000000101010001000000001000001000010000100110000111001110000000001010011110000100000000000100000001100001100000000000010000000000000000000000100000000010000001000000000011100000000000001000101000010100001001000011000000000010001100000000100000001000001000000000100000101000000000000000000010000000100000000100001000000100000000000000011100000001001000000001100010000001000001000000000000010100100000000110101110010000010000010100000000011000001000000001110000101001000010101111\n",
    "# uid                                                          654\n",
    "# Name: 0, dtype: object\n",
    "#print('Length Labeled data:', len(data_labeled))\n",
    "#print('Length Unlabeled data:', len(data_not_labeled))\n",
    "\n",
    "#bloomfilter_tensor, label_tensor = data_labeled[0]\n",
    "\n",
    "#print('Bloom Filter Tensor:', bloomfilter_tensor)\n",
    "#print('Bloom Filter Tensor Shape:', bloomfilter_tensor.shape)\n",
    "#print('Label Tensor:', label_tensor)\n",
    "#print('Label Tensor Shape:', label_tensor.shape)\n",
    "\n",
    "#for bloomfilter_tensors, label_tensors in dataloader_train:\n",
    "#    print('Bloom Filter Tensor Shape:', bloomfilter_tensors.shape)\n",
    "#    print('Label Tensor Shape:', label_tensors.shape)\n",
    "#    print(label_tensors)\n",
    "#    break\n",
    "\n",
    "#print(str(model)[:500])\n",
    "#example_bloom, example_label = data_train[0]\n",
    "#example_out = model(example_bloom)\n",
    "#print(example_out.shape)\n",
    "#loss_function_applied = criterion(example_out, example_label)\n",
    "#print(loss_function_applied)\n",
    "#print(example_out)\n",
    "\n",
    "print(data_labeled[0])\n",
    "\n",
    "# Apply model\n",
    "result = model(bloomfilter_tensor)\n",
    "# Result = Tensor of shape 676 with prob. for each 2gram\n",
    "two_gram_scores = {two_gram_dict[i]: score.item() for i, score in enumerate(result)}\n",
    "\n",
    "threshold = 0.000000001\n",
    "filtered_two_gram_scores = {two_gram: score for two_gram, score in two_gram_scores.items() if score > threshold}\n",
    "filtered_two_gram_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To Decode: \",df_not_reidentified_individuals.iloc[1])\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#torch.set_printoptions(profile=\"default\")\n",
    "print(\"BF Tensor: \", data_not_labeled[1])\n",
    "# Apply model\n",
    "model.eval()\n",
    "logits = model(data_not_labeled[1])\n",
    "probabilities = torch.sigmoid(logits)\n",
    "print(\"Prob: \", probabilities)\n",
    "two_gram_scores = {two_gram_dict[i]: score.item() for i, score in enumerate(probabilities)}\n",
    "threshold = 0.00000001\n",
    "filtered_two_gram_scores = {two_gram: score for two_gram, score in two_gram_scores.items() if score > threshold}\n",
    "print(\"Decoded 2grams: \", filtered_two_gram_scores)\n",
    "\n",
    "# person is: Roy\tJeon\t9/19/1975\n",
    "# to or re\n",
    "# at tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bloom_filters, labels in dataloader_train:\n",
    "    print(\"Bloom Filters Batch Shape:\", bloom_filters.shape)\n",
    "    print(\"Labels Batch Shape:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(20, 5) # predict logits for 5 classes\n",
    "x = torch.randn(1, 20)\n",
    "print(x.shape)\n",
    "y = torch.tensor([[1., 0., 1., 0., 0.]]) # get classA and classC as active\n",
    "print(y.shape)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Loss: {:.3f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bloom_filters, labels in dataloader_train:\n",
    "    print(\"Bloom Filters Batch Shape:\", bloom_filters[0])\n",
    "    print(\"Labels Batch Shape:\", labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
