{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL): Investigating Dataset Extension Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Import all relevant libraries and classes used throughout the project. Key components include:\n",
    "\n",
    "- **Torch** ‚Äì for tensor operations and neural network functionality  \n",
    "- **Datasets** ‚Äì for handling training and evaluation data  \n",
    "- **PyTorch Models** ‚Äì custom and pre-defined models for the DEA  \n",
    "- **Graph Matching Attack (GMA)** ‚Äì core logic for the initial re-identification phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "from functools import partial  # Import partial from functools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "import ray\n",
    "from ray import tune, air\n",
    "from ray import train\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from utils import get_hashes, convert_to_two_gram_scores, filter_two_grams, calculate_performance_metrics, run_epoch, label_tensors_to_two_grams, reconstruct_using_ai\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "import string\n",
    "from early_stopping.early_stopping import EarlyStopping\n",
    "\n",
    "from graphMatching.gma import run_gma\n",
    "\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset import TwoStepHashDataset\n",
    "\n",
    "from pytorch_models_hyperparameter_optimization.base_model_hyperparameter_optimization import BaseModelHyperparameterOptimization\n",
    "from pytorch_models.base_model import BaseModel\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Configuration and parameters for the Graph Matching Attack (GMA) and Dataset Extension Attack (DEA). For details and possible values, refer to the documentation at ```./docs/parameters.md```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/fakename_5k.tsv\",\n",
    "    \"Overlap\": 0.8,\n",
    "    \"DropFrom\": \"Eve\",\n",
    "    \"Verbose\": False,  # Print Status Messages\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": -1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": False,\n",
    "}\n",
    "\n",
    "\n",
    "DEA_CONFIG = {\n",
    "    \"DevMode\": False,\n",
    "    # TestSize calculated accordingly\n",
    "    \"TrainSize\": 0.8,\n",
    "    \"Patience\": 5,\n",
    "    \"MinDelta\": 1e-4,\n",
    "    \"NumSamples\": 50,\n",
    "    \"Epochs\": 15,\n",
    "    \"EarlyStoppingPatience\": 5,\n",
    "    \"NumCPU\": 11,  # 11 on my local 19 on cluster (general: n-1)\n",
    "    \"MetricToOptimize\": \"average_recall\", # \"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"\n",
    "}\n",
    "\n",
    "ENC_CONFIG = {\n",
    "    # TwoStepHash / TabMinHash / BloomFilter\n",
    "    \"AliceAlgo\": \"BloomFilter\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"None\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "    # For BF encoding\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "    # For TMH encoding\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "    # For 2SH encoding\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    # For Node2Vec\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42\n",
    "}\n",
    "\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"]/2), #0005\n",
    "    \"RegInit\":1, # For BF 0.25\n",
    "    \"Batchsize\": 1, # 1 = 100%\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5 ,  # 800\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate a dictionary of all possible 2-grams from letters and digits ---\n",
    "\n",
    "# Lowercase alphabet: 'a' to 'z'\n",
    "alphabet = string.ascii_lowercase\n",
    "\n",
    "# Digits: '0' to '9'\n",
    "digits = string.digits\n",
    "\n",
    "# Generate all letter-letter 2-grams (e.g., 'aa', 'ab', ..., 'zz')\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]\n",
    "\n",
    "# Generate all digit-digit 2-grams (e.g., '00', '01', ..., '99')\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]\n",
    "\n",
    "# Generate all letter-digit 2-grams (e.g., 'a0', 'a1', ..., 'z9')\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]\n",
    "\n",
    "# Combine all generated 2-grams into one list\n",
    "all_two_grams = letter_letter_grams + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Create a dictionary mapping index to each 2-gram\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load or Compute Graph Matching Attack (GMA) Results\n",
    "\n",
    "This code snippet either loads previously computed Graph Matching Attack (GMA) results from disk or runs the attack if no saved data is found.\n",
    "\n",
    "1. **Generate Configuration Hashes:**  \n",
    "   The function `get_hashes` creates unique hash values based on the encoding and embedding configurations. These are used to create distinct filenames for the data.\n",
    "\n",
    "2. **Create File Paths:**  \n",
    "   Based on the configuration hashes, paths are generated for:\n",
    "   - Reidentified individuals\n",
    "   - Not reidentified individuals\n",
    "   - All individuals in Alice‚Äôs dataset (with encoding)\n",
    "\n",
    "3. **Load Results from Disk (if available):**  \n",
    "   If the `.h5` files already exist, they are loaded using `hickle` and converted into `pandas.DataFrames`.  \n",
    "   The data format assumes that the first row contains the column headers, and the rest is the data ‚Äî hence the slicing `[1:]` and `columns=...`.\n",
    "\n",
    "4. **Run GMA If Data Is Not Available:**  \n",
    "   If the files are missing, the GMA is executed via `run_gma()`. The results are again converted to `DataFrames`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath(\"./data\")\n",
    "\n",
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG)\n",
    "\n",
    "identifier = f\"{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}\"\n",
    "\n",
    "# Define file paths based on the configuration hashes\n",
    "path_reidentified = f\"{data_dir}/available_to_eve/reidentified_individuals_{identifier}.h5\"\n",
    "path_not_reidentified = f\"{data_dir}/available_to_eve/not_reidentified_individuals_{identifier}.h5\"\n",
    "path_all = f\"{data_dir}/dev/alice_data_complete_with_encoding_{identifier}.h5\"\n",
    "\n",
    "# Check if the output files already exist\n",
    "if not (os.path.isfile(path_reidentified) and os.path.isfile(path_all) and os.path.isfile(path_not_reidentified)):\n",
    "    run_gma(\n",
    "        GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG,\n",
    "        eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Step 2: Data Preparation\n",
    "\n",
    "This section initializes the dataset objects depending on which encoding method Alice used. Each encoding requires a different preprocessing strategy for compatibility with downstream neural models.\n",
    "\n",
    "### 1. Bloom Filter (`\"BloomFilter\"`)\n",
    "- Uses binary Bloom filters to represent identifiers.\n",
    "- Loads `BloomFilterDataset` objects.\n",
    "- Stores the bit-length of the bloom filter.\n",
    "\n",
    "### 2. Tabulation MinHash (`\"TabMinHash\"`)\n",
    "- Applies a MinHash-based encoding.\n",
    "- Loads `TabMinHashDataset`.\n",
    "- Captures the length of each encoded vector.\n",
    "\n",
    "### 3. Two-Step Hash with One-Hot Encoding (`\"TwoStepHash\"`)\n",
    "- Extracts all **unique hash values** to build a consistent one-hot vector space.\n",
    "- Constructs datasets using `TwoStepHashDatasetOneHotEncoding`.\n",
    "\n",
    "> ‚öôÔ∏è All dataset constructors are passed:\n",
    "> - Whether the data is labeled\n",
    "> - The full 2-gram list (used as feature tokens)\n",
    "> - Additional encoding-specific configurations\n",
    "> - Dev mode toggle (for debugging or smaller runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_directory, identifier, load_test=False):\n",
    "    # Get unique hash identifiers for the encoding and embedding configurations\n",
    "\n",
    "    data_train = data_val = data_test = None\n",
    "\n",
    "    if load_test:\n",
    "        path_not_reidentified = f\"{data_directory}/available_to_eve/not_reidentified_individuals_{identifier}.h5\"\n",
    "        path_all = f\"{data_directory}/dev/alice_data_complete_with_encoding_{identifier}.h5\"\n",
    "        not_reidentified_data = hkl.load(path_not_reidentified)\n",
    "        all_data = hkl.load(path_all)\n",
    "        # Convert lists to DataFrames\n",
    "        df_not_reidentified = pd.DataFrame(not_reidentified_data[1:], columns=not_reidentified_data[0])\n",
    "        df_all = pd.DataFrame(all_data[1:], columns=all_data[0])\n",
    "        df_not_reidentified_labeled = df_all[df_all[\"uid\"].isin(df_not_reidentified[\"uid\"])].reset_index(drop=True)\n",
    "\n",
    "    # Define file paths based on the configuration hashes\n",
    "    path_reidentified = f\"{data_directory}/available_to_eve/reidentified_individuals_{identifier}.h5\"\n",
    "\n",
    "    reidentified_data = hkl.load(path_reidentified)\n",
    "\n",
    "    # Convert lists to DataFrames\n",
    "    df_reidentified = pd.DataFrame(reidentified_data[1:], columns=reidentified_data[0])\n",
    "\n",
    "    # 1Ô∏è‚É£ Bloom Filter Encoding\n",
    "    if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "        data_labeled = BloomFilterDataset(\n",
    "            df_reidentified,\n",
    "            is_labeled=True,\n",
    "            all_two_grams=all_two_grams,\n",
    "            dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "        )\n",
    "        if load_test:\n",
    "            # If loading validation data, also create a dataset for not reidentified individuals\n",
    "            data_test = BloomFilterDataset(\n",
    "                df_not_reidentified_labeled,\n",
    "                is_labeled=True,\n",
    "                all_two_grams=all_two_grams,\n",
    "                dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "            )\n",
    "\n",
    "    # 2Ô∏è‚É£ Tabulation MinHash Encoding\n",
    "    elif ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "        data_labeled = TabMinHashDataset(\n",
    "            df_reidentified,\n",
    "            is_labeled=True,\n",
    "            all_two_grams=all_two_grams,\n",
    "            dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "        )\n",
    "        if load_test:\n",
    "            # If loading validation data, also create a dataset for not reidentified individuals\n",
    "            data_test = TabMinHashDataset(\n",
    "                df_not_reidentified_labeled,\n",
    "                is_labeled=True,\n",
    "                all_two_grams=all_two_grams,\n",
    "                dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "            )\n",
    "\n",
    "    # 3 Two-Step Hash Encoding (One-Hot Encoding Mode)\n",
    "    elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\":\n",
    "        # Collect all unique integers across both reidentified and non-reidentified data\n",
    "        unique_ints = sorted(set().union(*df_reidentified[\"twostephash\"]))\n",
    "        data_labeled = TwoStepHashDataset(\n",
    "            df_reidentified,\n",
    "            is_labeled=True,\n",
    "            all_integers=unique_ints,\n",
    "            all_two_grams=all_two_grams,\n",
    "            dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "        )\n",
    "        if load_test:\n",
    "            # If loading validation data, also create a dataset for not reidentified individuals\n",
    "            data_test = TwoStepHashDataset(\n",
    "                df_not_reidentified_labeled,\n",
    "                is_labeled=True,\n",
    "                all_integers=unique_ints,\n",
    "                all_two_grams=all_two_grams,\n",
    "                dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "            )\n",
    "\n",
    "    # Define dataset split proportions\n",
    "    train_size = int(DEA_CONFIG[\"TrainSize\"] * len(data_labeled))\n",
    "    val_size = len(data_labeled) - train_size\n",
    "\n",
    "    # Split the reidentified dataset into training and validation sets\n",
    "    data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "\n",
    "    return data_train, data_val, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, data_dir, output_dim, identifier, patience, min_delta):\n",
    "    # Create DataLoaders for training, validation, and testing\n",
    "\n",
    "    data_train, data_val, _ = load_data(data_dir, identifier, load_test=False)\n",
    "\n",
    "    input_dim = data_train[0][0].shape[0]  # Get the input dimension from the first sample\n",
    "\n",
    "    dataloader_train = DataLoader(\n",
    "        data_train,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True  # Important for training\n",
    "    )\n",
    "\n",
    "    dataloader_val = DataLoader(\n",
    "        data_val,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True  # Allows variation in validation batches\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    total_precision = total_recall = total_f1 = total_dice = total_val_loss = 0.0\n",
    "    n = len(dataloader_val.dataset)\n",
    "    epochs = 0\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "\n",
    "    # Define and initialize model with hyperparameters from config\n",
    "    model = BaseModelHyperparameterOptimization(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        activation_fn=config[\"activation_fn\"]\n",
    "    )\n",
    "\n",
    "    # Set device for model (GPU or CPU)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Select loss function based on config\n",
    "    loss_functions = {\n",
    "        \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss(),\n",
    "        \"MultiLabelSoftMarginLoss\": nn.MultiLabelSoftMarginLoss(),\n",
    "        \"SoftMarginLoss\": nn.SoftMarginLoss(),\n",
    "    }\n",
    "    criterion = loss_functions[config[\"loss_fn\"]]\n",
    "\n",
    "    learning_rate = config[\"optimizer\"][\"lr\"].sample()\n",
    "    # Select optimizer based on config\n",
    "    optimizers = {\n",
    "        \"Adam\": lambda: optim.Adam(model.parameters(), lr=learning_rate),\n",
    "        \"AdamW\": lambda: optim.AdamW(model.parameters(), lr=learning_rate),\n",
    "        \"SGD\": lambda: optim.SGD(model.parameters(), lr=learning_rate, momentum=config[\"optimizer\"][\"momentum\"].sample()),\n",
    "        \"RMSprop\": lambda: optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    }\n",
    "    optimizer = optimizers[config[\"optimizer\"][\"name\"]]()\n",
    "\n",
    "    schedulers = {\n",
    "        \"StepLR\": lambda: torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config[\"lr_scheduler\"][\"step_size\"].sample(),\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ExponentialLR\": lambda: torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ReduceLROnPlateau\": lambda: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=config[\"lr_scheduler\"][\"mode\"],\n",
    "            factor=config[\"lr_scheduler\"][\"factor\"].sample(),\n",
    "            patience=config[\"lr_scheduler\"][\"patience\"].sample()\n",
    "        ),\n",
    "        \"CosineAnnealingLR\": lambda: torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config[\"lr_scheduler\"][\"T_max\"].sample(),\n",
    "            eta_min=config[\"lr_scheduler\"][\"eta_min\"].sample()\n",
    "        ),\n",
    "        \"CyclicLR\": lambda: torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=config[\"lr_scheduler\"][\"base_lr\"].sample(),\n",
    "            max_lr=config[\"lr_scheduler\"][\"max_lr\"].sample(),\n",
    "            step_size_up=config[\"lr_scheduler\"][\"step_size_up\"].sample(),\n",
    "            mode=config[\"lr_scheduler\"][\"mode_cyclic\"].sample(),\n",
    "            cycle_momentum=False\n",
    "        ),\n",
    "        \"None\": lambda: None,\n",
    "    }\n",
    "    scheduler = schedulers[config[\"lr_scheduler\"][\"name\"]]()\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(DEA_CONFIG[\"Epochs\"]):\n",
    "        epochs += 1\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = run_epoch(model, dataloader_train, criterion, optimizer, device, is_training=True, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(model, dataloader_val, criterion, optimizer, device, is_training=False, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        total_val_loss += val_loss\n",
    "\n",
    "         # Early stopping check\n",
    "        if early_stopper(val_loss):\n",
    "            break\n",
    "\n",
    "    # Test phase with reconstruction and evaluation\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in dataloader_val:\n",
    "\n",
    "            actual_two_grams = label_tensors_to_two_grams(two_gram_dict, labels)\n",
    "\n",
    "            # Move data to device and make predictions\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities into 2-gram scores\n",
    "            batch_two_gram_scores = convert_to_two_gram_scores(two_gram_dict, probabilities)\n",
    "\n",
    "            # Filter out low-scoring 2-grams\n",
    "            batch_filtered_two_gram_scores = filter_two_grams(batch_two_gram_scores, config[\"threshold\"])\n",
    "\n",
    "            # Calculate performance metrics for evaluation\n",
    "            dice, precision, recall, f1 = calculate_performance_metrics(\n",
    "                actual_two_grams, batch_filtered_two_gram_scores)\n",
    "\n",
    "            total_dice += dice\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "\n",
    "    train.report({\n",
    "            \"average_dice\": total_dice / n,\n",
    "            \"average_precision\": total_precision / n,\n",
    "            \"average_recall\": total_recall / n,\n",
    "            \"average_f1\": total_f1 / n,\n",
    "            \"total_val_loss\": total_val_loss,\n",
    "            \"len_train\": len(dataloader_train.dataset),\n",
    "            \"len_val\": len(dataloader_val.dataset),\n",
    "            \"epochs\": epochs\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space for hyperparameter optimization\n",
    "search_space = {\n",
    "    \"output_dim\": len(all_two_grams),  # Output dimension is also the number of unique 2-grams\n",
    "    \"num_layers\": tune.randint(1, 8),  # Vary the number of layers in the model\n",
    "    \"hidden_layer_size\": tune.choice([64, 128, 256, 512, 1024, 2048]),  # Different sizes for hidden layers\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.4),  # Dropout rate between 0.1 and 0.4\n",
    "    \"activation_fn\": tune.choice([\"relu\", \"leaky_relu\", \"gelu\", \"elu\", \"selu\", \"tanh\"]),  # Activation functions to choose from\n",
    "    \"optimizer\": tune.choice([\n",
    "        {\"name\": \"Adam\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"AdamW\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"SGD\", \"lr\": tune.loguniform(1e-4, 1e-2), \"momentum\": tune.uniform(0.0, 0.99)},\n",
    "        {\"name\": \"RMSprop\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "    ]),\n",
    "    \"loss_fn\": tune.choice([\"BCEWithLogitsLoss\", \"MultiLabelSoftMarginLoss\", \"SoftMarginLoss\"]),\n",
    "    \"threshold\": tune.uniform(0.3, 0.8),\n",
    "    \"lr_scheduler\": tune.choice([\n",
    "        {\"name\": \"StepLR\", \"step_size\": tune.choice([5, 10, 20]), \"gamma\": tune.uniform(0.1, 0.9)},\n",
    "        {\"name\": \"ExponentialLR\", \"gamma\": tune.uniform(0.85, 0.99)},\n",
    "        {\"name\": \"ReduceLROnPlateau\", \"mode\": \"min\", \"factor\": tune.uniform(0.1, 0.5), \"patience\": tune.choice([5, 10, 15])},\n",
    "        {\"name\": \"CosineAnnealingLR\", \"T_max\": tune.loguniform(10, 50) , \"eta_min\": tune.choice([1e-5, 1e-6, 0])},\n",
    "        {\"name\": \"CyclicLR\", \"base_lr\": tune.loguniform(1e-5, 1e-3), \"max_lr\": tune.loguniform(1e-3, 1e-1), \"step_size_up\": tune.choice([2000, 4000]), \"mode_cyclic\": tune.choice([\"triangular\", \"triangular2\", \"exp_range\"]) },\n",
    "        {\"name\": \"None\"}  # No scheduler\n",
    "    ]),\n",
    "    \"batch_size\": tune.choice([8, 16, 32, 64]),  # Batch sizes to test\n",
    "}\n",
    "\n",
    "selected_dataset = GLOBAL_CONFIG[\"Data\"].split(\"/\")[-1].replace(\".tsv\", \"\")\n",
    "\n",
    "experiment_tag = \"experiment_\" + ENC_CONFIG[\"AliceAlgo\"] + \"_\" + selected_dataset + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Initialize Ray for hyperparameter optimization\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Optuna Search Algorithm for optimizing the hyperparameters\n",
    "optuna_search = OptunaSearch(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"max\")\n",
    "\n",
    "# Use ASHAScheduler to manage trials and early stopping\n",
    "scheduler = ASHAScheduler(metric=\"total_val_loss\", mode=\"min\")\n",
    "\n",
    "# Define and configure the Tuner for Ray Tune\n",
    "tuner = tune.Tuner(\n",
    "    partial(train_model, data_dir=data_dir, output_dim=len(all_two_grams), identifier=identifier , patience=DEA_CONFIG[\"Patience\"], min_delta=DEA_CONFIG[\"MinDelta\"]),  # The function to optimize (training function)\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,  # Search strategy using Optuna\n",
    "        scheduler=scheduler,  # Use ASHA to manage the trials\n",
    "        num_samples=DEA_CONFIG[\"NumSamples\"],  # Number of trials to run\n",
    "        max_concurrent_trials=DEA_CONFIG[\"NumCPU\"],\n",
    "    ),\n",
    "    param_space=search_space  # Pass in the defined hyperparameter search space\n",
    ")\n",
    "\n",
    "# Run the tuner\n",
    "results = tuner.fit()\n",
    "\n",
    "# Shut down Ray after finishing the optimization\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keys to remove\n",
    "keys_to_remove = [\n",
    "    \"config\", \"checkpoint_dir_name\", \"experiment_tag\", \"done\", \"training_iteration\",\n",
    "    \"trial_id\", \"date\", \"time_this_iter_s\", \"pid\", \"time_total_s\", \"hostname\",\n",
    "    \"node_ip\", \"time_since_restore\", \"iterations_since_restore\", \"timestamp\"\n",
    "]\n",
    "\n",
    "def clean_result_dict(result_dict):\n",
    "    for key in keys_to_remove:\n",
    "        result_dict.pop(key, None)\n",
    "    return result_dict\n",
    "\n",
    "def resolve_config(config):\n",
    "    resolved = {}\n",
    "    for k, v in config.items():\n",
    "        # If the value is a dictionary, recurse and apply resolve_config\n",
    "        if isinstance(v, dict):\n",
    "            resolved[k] = resolve_config(v)\n",
    "        # If the value is a Ray search sample object (e.g., Float, Categorical)\n",
    "        elif not isinstance(v, (int, float, str, Subset)):\n",
    "            resolved[k] = v.sample()  # Get the concrete value from the sample\n",
    "        else:\n",
    "            resolved[k] = v  # Leave it as-is if it's not a sample object or Subset\n",
    "    return resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to = f\"experiment_results/{experiment_tag}\"\n",
    "os.makedirs(save_to, exist_ok=True)\n",
    "result_grid = results\n",
    "\n",
    "# Best and worst result based on dice\n",
    "best_result = result_grid.get_best_result(metric=\"average_dice\", mode=\"max\")\n",
    "worst_result = result_grid.get_best_result(metric=\"average_dice\", mode=\"min\")\n",
    "\n",
    "# Combine configs and metrics into a DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        **clean_result_dict(resolve_config(result.config)),\n",
    "        **{k: result.metrics.get(k) for k in [\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]},\n",
    "    }\n",
    "    for result in result_grid\n",
    "])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(f\"{save_to}/all_trial_results.csv\", index=False)\n",
    "print(\"‚úÖ Results saved to all_trial_results.csv\")\n",
    "\n",
    "\n",
    "def print_result(label, result):\n",
    "    print(f\"\\nüîç {label}\")\n",
    "    print(\"-\" * 40)\n",
    "    cleaned_config = resolve_config(result.config)\n",
    "    print(f\"Config: {cleaned_config}\")\n",
    "    print(f\"Average Dice: {result.metrics.get('average_dice'):.4f}\")\n",
    "    print(f\"Average Precision: {result.metrics.get('average_precision'):.4f}\")\n",
    "    print(f\"Average Recall: {result.metrics.get('average_recall'):.4f}\")\n",
    "    print(f\"Average F1: {result.metrics.get('average_f1'):.4f}\")\n",
    "    result_dict = {**cleaned_config, **result.metrics}\n",
    "    clean_result_dict(result_dict)\n",
    "    # Convert to a DataFrame and save\n",
    "    df = pd.DataFrame([result_dict])\n",
    "    df.to_csv(f\"{save_to}/{label}.csv\", index=False)\n",
    "\n",
    "print_result(\"Best_Result\", best_result)\n",
    "print_result(\"Worst_Result\", worst_result)\n",
    "\n",
    "# Compute and print average metrics\n",
    "print(\"\\nüìä Average Metrics Across All Trials\")\n",
    "avg_metrics = df[[\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]].mean()\n",
    "print(\"-\" * 40)\n",
    "for key, value in avg_metrics.items():\n",
    "    print(f\"{key.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# --- üìà Plotting performance metrics ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[[\"average_dice\", \"average_recall\", \"average_f1\", \"average_precision\"]])\n",
    "plt.title(\"Distribution of Performance Metrics Across Trials\")\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{save_to}/metric_distributions.png\")\n",
    "print(\"üìä Saved plot: metric_distributions.png\")\n",
    "\n",
    "# --- üìå Correlation between config params and performance ---\n",
    "# Only include numeric config columns\n",
    "exclude_cols = {\"input_dim\", \"output_dim\"}\n",
    "numeric_config_cols = [\n",
    "    col for col in df.columns\n",
    "    if pd.api.types.is_numeric_dtype(df[col]) and col not in exclude_cols\n",
    "]\n",
    "correlation_df = df[numeric_config_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Between Parameters and Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{save_to}/correlation_heatmap.png\")\n",
    "print(\"üìå Saved heatmap: correlation_heatmap.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "\n",
    "The neural network model is selected dynamically based on the encoding technique used for Alice‚Äôs data.\n",
    "\n",
    "### Supported Models:\n",
    "\n",
    "- **BloomFilter** ‚Üí `BloomFilterToTwoGramClassifier`  \n",
    "  - Input: Binary vector (Bloom filter)  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TabMinHash** ‚Üí `TabMinHashToTwoGramClassifier`  \n",
    "  - Input: Tabulated MinHash signature  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TwoStepHash** ‚Üí `TwoStepHashToTwoGramClassifier`  \n",
    "  - Input: Length of the unique integers present\n",
    "  - Output: 2-gram predicition\n",
    "    \n",
    "Each model outputs predictions over the set of all possible 2-grams (`all_two_grams`), and the input dimension is dynamically configured based on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = resolve_config(best_result.config)\n",
    "data_train, data_val, data_test = load_data(data_dir, identifier, load_test=True)\n",
    "input_dim=data_train[0][0].shape[0]\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "        data_train,\n",
    "        batch_size=int(best_config.get(\"batch_size\", 32)),  # Default to 32 if not specified\n",
    "        shuffle=True  # Important for training\n",
    "    )\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    data_val,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),\n",
    "    shuffle=True  # Allows variation in validation batches\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    data_test,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),\n",
    "    shuffle=True  # Allows variation in validation batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=len(all_two_grams),\n",
    "            hidden_layer=best_config.get(\"hidden_layer_size\", 128),  # Default to 128 if not specified\n",
    "            num_layers=best_config.get(\"num_layers\", 2),  # Default to 2 if not specified\n",
    "            dropout_rate=best_config.get(\"dropout_rate\", 0.2),  # Default to 0.2 if not specified\n",
    "            activation_fn=best_config.get(\"activation_fn\", \"relu\")  # Default to 'relu' if not specified\n",
    "        )\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Environment Setup\n",
    "This code initializes the core components needed for training a neural network model.\n",
    "\n",
    "1. TensorBoard Setup\n",
    "    - Creates unique run name by combining:\n",
    "    - Loss function type\n",
    "    - Optimizer choice\n",
    "    - Alice's algorithm\n",
    "    - Initializes TensorBoard writer in runs directory\n",
    "2. Device Configuration\n",
    "    - Automatically selects GPU if available, falls back to CPU\n",
    "    - Moves model to selected device\n",
    "3. Loss Functions\n",
    "    - `BCEWithLogitsLoss`: Binary Cross Entropy with Logits\n",
    "    - `MultiLabelSoftMarginLoss`: Multi-Label Soft Margin Loss\n",
    "4. Optimizers:\n",
    "    - `Adam`: Adaptive Moment Estimation\n",
    "    - `AdamW`: Adam with Weight Decay\n",
    "    - `SGD`: Stochastic Gradient Descent (with momentum)\n",
    "    - `RMSprop`: Root Mean Square Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tensorboard logging\n",
    "run_name = \"\".join([\n",
    "    best_config.get(\"loss_fn\", \"MultiLabelSoftMarginLoss\"),\n",
    "    best_config.get(\"optimizer\").get(\"name\", \"Adam\"),\n",
    "    ENC_CONFIG[\"AliceAlgo\"],\n",
    "    best_config.get(\"activation_fn\", \"relu\"),\n",
    "])\n",
    "tb_writer = SummaryWriter(f\"{save_to}/{run_name}\")\n",
    "\n",
    "# Setup compute device (GPU/CPU)\n",
    "compute_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(compute_device)\n",
    "\n",
    "# Initialize loss function\n",
    "match best_config.get(\"loss_fn\", \"MultiLabelSoftMarginLoss\"):\n",
    "    case \"BCEWithLogitsLoss\":\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    case \"MultiLabelSoftMarginLoss\":\n",
    "        criterion = nn.MultiLabelSoftMarginLoss(reduction='mean')\n",
    "    case \"SoftMarginLoss\":\n",
    "        criterion = nn.SoftMarginLoss()\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported loss function: {best_config.get('loss_fn', 'MultiLabelSoftMarginLoss')}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "match best_config.get(\"optimizer\").get(\"name\", \"Adam\"):\n",
    "    case \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                            lr=best_config.get(\"optimizer\").get(\"lr\"),\n",
    "                            momentum=best_config.get(\"optimizer\").get(\"momentum\"))\n",
    "    case \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported optimizer: {best_config.get('optimizer').get('name', 'Adam')}\")\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "match best_config.get(\"lr_scheduler\").get(\"name\", \"None\"):\n",
    "    case \"StepLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=best_config.get(\"lr_scheduler\").get(\"step_size\"),\n",
    "            gamma=best_config.get(\"lr_scheduler\").get(\"gamma\")\n",
    "        )\n",
    "    case \"ExponentialLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=best_config.get(\"lr_scheduler\").get(\"gamma\")\n",
    "        )\n",
    "    case \"ReduceLROnPlateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=best_config.get(\"lr_scheduler\").get(\"mode\"),\n",
    "            factor=best_config.get(\"lr_scheduler\").get(\"factor\"),\n",
    "            patience=best_config.get(\"lr_scheduler\").get(\"patience\")\n",
    "        )\n",
    "    case \"CosineAnnealingLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=best_config.get(\"lr_scheduler\").get(\"T_max\")\n",
    "        )\n",
    "    case \"CyclicLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=best_config.get(\"lr_scheduler\").get(\"base_lr\"),\n",
    "            max_lr=best_config.get(\"lr_scheduler\").get(\"max_lr\"),\n",
    "            step_size_up=best_config.get(\"lr_scheduler\").get(\"step_size_up\"),\n",
    "            mode=best_config.get(\"lr_scheduler\").get(\"mode_cyclic\"),\n",
    "            cycle_momentum=False  # usually False for Adam/AdamW\n",
    "        )\n",
    "    case None | \"None\":\n",
    "        scheduler = None\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported LR scheduler: {best_config.get('lr_scheduler').get('name', 'None')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Early Stopping\n",
    "\n",
    "The function `train_model` orchestrates the training process for the neural network, including both training and validation phases for each epoch. It also utilizes **early stopping** to halt training when the validation loss fails to improve over multiple epochs, avoiding overfitting.\n",
    "\n",
    "### Key Phases:\n",
    "1. **Training Phase**: \n",
    "   - The model is trained on the `dataloader_train`, computing the training loss using the specified loss function (`criterion`) and optimizer. Gradients are calculated, and the model parameters are updated.\n",
    "  \n",
    "2. **Validation Phase**:\n",
    "   - The model is evaluated on the `dataloader_val` without updating weights. The validation loss is computed to track model performance on unseen data.\n",
    "\n",
    "3. **Logging**: \n",
    "   - Training and validation losses are logged to both the console and **TensorBoard** for tracking model performance during training.\n",
    "\n",
    "4. **Early Stopping**: \n",
    "   - If the validation loss does not improve after a certain number of epochs (defined by `DEA_CONFIG[\"Patience\"]`), the training process is halted to prevent overfitting.\n",
    "\n",
    "### Helper Functions:\n",
    "- `run_epoch`: Handles a single epoch, either for training or validation, depending on the flag `is_training`.\n",
    "- `log_metrics`: Logs the training and validation losses to the console and TensorBoard for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, device):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_loss = train_loss = 0.0\n",
    "    early_stopper = EarlyStopping(patience=DEA_CONFIG[\"EarlyStoppingPatience\"], min_delta=DEA_CONFIG[\"MinDelta\"], verbose=GLOBAL_CONFIG[\"Verbose\"])\n",
    "\n",
    "    for epoch in range(best_config.get(\"epochs\", DEA_CONFIG[\"Epochs\"])):  # Use best_config epochs or default to DEA_CONFIG\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = run_epoch(\n",
    "            model, dataloader_train, criterion, optimizer,\n",
    "            device, is_training=True, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(\n",
    "            model, dataloader_val, criterion, optimizer,\n",
    "            device, is_training=False, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler\n",
    "        )\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Logging\n",
    "        log_metrics(train_loss, val_loss, epoch, best_config.get(\"epochs\", DEA_CONFIG[\"Epochs\"]))\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopper(val_loss):\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def log_metrics(train_loss, val_loss, epoch, total_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{total_epochs} - \"\n",
    "          f\"Train loss: {train_loss:.4f}, \"\n",
    "          f\"Validation loss: {val_loss:.4f}\")\n",
    "    tb_writer.add_scalar(\"Loss/train\", train_loss, epoch + 1)\n",
    "    tb_writer.add_scalar(\"Loss/validation\", val_loss, epoch + 1)\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model, dataloader_train, dataloader_val,\n",
    "    criterion, optimizer, compute_device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Visualization over Epochs\n",
    "\n",
    "This code snippet generates a plot to visualize the **training loss** and **validation loss** across epochs. It's useful for tracking model performance during training and evaluating if overfitting is occurring (i.e., when validation loss starts increasing while training loss continues to decrease).\n",
    "\n",
    "### Key Elements:\n",
    "1. **Plotting the Losses**: \n",
    "   - The `train_losses` and `val_losses` are plotted over the epochs. \n",
    "   - The **blue line** represents the training loss, and the **red line** represents the validation loss.\n",
    "\n",
    "2. **Legend**: \n",
    "   - A legend is added to distinguish between training and validation losses.\n",
    "\n",
    "3. **Title and Labels**: \n",
    "   - The plot is titled \"Training and Validation Loss over Epochs\" for context.\n",
    "   - **X-axis** represents the epoch number, and **Y-axis** represents the loss value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses over epochs\n",
    "plt.plot(train_losses, label='Training loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation loss', color='red')\n",
    "\n",
    "# Adding a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Setting the title and labels for clarity\n",
    "plt.title(\"Training and Validation Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Application to Encoded Data\n",
    "\n",
    "This code performs inference on the test data and compares the predicted 2-grams with the actual 2-grams, providing a performance evaluation based on the **Dice similarity coefficient**.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Prepare for Evaluation**:\n",
    "   - The model is switched to **evaluation mode** (`model.eval()`), ensuring no gradient computation.\n",
    "   \n",
    "2. **Thresholding**:\n",
    "   - A threshold (`DEA_CONFIG[\"FilterThreshold\"]`) is applied to filter out low-probability predictions, retaining only the most confident predictions.\n",
    "\n",
    "3. **Inference and 2-Gram Scoring**:\n",
    "   - The model is applied to the batch, and the **logits** are converted into probabilities using the **sigmoid function**.\n",
    "   - The probabilities are then mapped to **2-gram scores**, and scores below the threshold are discarded.\n",
    "\n",
    "4. **Reconstructing Words**:\n",
    "   - For each sample in the batch, **2-grams** are reconstructed into words based on the filtered scores.\n",
    "\n",
    "5. **Performance Metrics**:\n",
    "   - The actual 2-grams (from the test dataset) are compared with the predicted 2-grams, and the **Dice similarity coefficient** is calculated for each sample.\n",
    "\n",
    "### Result:\n",
    "- The code generates a list `combined_results_performance`, which contains a detailed comparison for each UID, including:\n",
    "  - **Actual 2-grams** (from the test data)\n",
    "  - **Predicted 2-grams** (from the model)\n",
    "  - **Dice similarity** score indicating how similar the actual and predicted 2-grams are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_to = \"experiment_results/experiment_BloomFilter_fakename_5k_2025-06-06_14-19-00\" # overwrite if necessary\n",
    "save_path = os.path.join(save_to, \"trained_model\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model_file = os.path.join(save_path, \"model.pt\")\n",
    "config_file = os.path.join(save_path, \"config.json\")\n",
    "result_file = os.path.join(save_path, \"result.json\")\n",
    "metrics_file = os.path.join(save_path, \"metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "torch.save(model.state_dict(), model_file)\n",
    "with open(config_file, \"w\") as f:\n",
    "    json.dump(best_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "with open(config_file) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "model = BaseModel(\n",
    "    input_dim=1024,\n",
    "    output_dim=1036,\n",
    "    hidden_layer=config.get(\"hidden_layer_size\", 128),  # Default to 128 if not specified\n",
    "    num_layers=config.get(\"num_layers\", 2),  # Default to 2 if not specified\n",
    "    dropout_rate=config.get(\"dropout_rate\", 0.2),  # Default to 0.2 if not specified\n",
    "    activation_fn=config.get(\"activation_fn\", \"relu\")  # Default to 'relu' if not specified\n",
    ")\n",
    "# Load model\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store decoded 2-gram scores for all test samples\n",
    "\n",
    "decoded_test_results_words = []\n",
    "result = []\n",
    "total_precision = total_recall = total_f1 = total_dice = 0.0\n",
    "n = len(dataloader_test.dataset)\n",
    "\n",
    "# Switch to evaluation mode (no gradient computation during inference)\n",
    "model.eval()\n",
    "\n",
    "# Define Threshold for filtering predictions\n",
    "threshold = best_config.get(\"threshold\", 0.5)  # Default threshold if not specified\n",
    "\n",
    "# Loop through the test dataloader for inference\n",
    "with torch.no_grad():  # No need to compute gradients during inference\n",
    "    for data, labels, uid in tqdm(dataloader_test, desc=\"Test loop\") if GLOBAL_CONFIG[\"Verbose\"] else dataloader_test:\n",
    "\n",
    "        actual_two_grams = label_tensors_to_two_grams(two_gram_dict, labels)\n",
    "\n",
    "        # Move data to device and make predictions\n",
    "        data = data.to(compute_device)\n",
    "        logits = model(data)\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "\n",
    "        # Convert probabilities into 2-gram scores\n",
    "        batch_two_gram_scores = convert_to_two_gram_scores(two_gram_dict, probabilities)\n",
    "\n",
    "        # Filter out low-scoring 2-grams\n",
    "        batch_filtered_two_gram_scores = filter_two_grams(batch_two_gram_scores, threshold)\n",
    "\n",
    "        # Calculate performance metrics for evaluation\n",
    "        dice, precision, recall, f1 = calculate_performance_metrics(\n",
    "            actual_two_grams, batch_filtered_two_gram_scores)\n",
    "        total_dice += dice\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "\n",
    "        for two_grams, two_grams_predicted, uid in zip(actual_two_grams, batch_filtered_two_gram_scores, uid):\n",
    "            # Create a dictionary to store the results for each test sample\n",
    "            result_dict = {\n",
    "                \"uid\": uid,\n",
    "                \"actual_two_grams\": two_grams,\n",
    "                \"filtered_two_grams\": two_grams_predicted,\n",
    "            }\n",
    "            # Append the result dictionary to the combined results list\n",
    "            result.append(result_dict)\n",
    "\n",
    "        average_precision = total_precision / n\n",
    "        average_recall = total_recall / n\n",
    "        average_f1 = total_f1 / n\n",
    "        average_dice = total_dice / n\n",
    "\n",
    "\n",
    "# Now `combined_results_performance` contains detailed comparison for all test samples\n",
    "print (f\"Average Precision: {average_precision}\")\n",
    "print (f\"Average Recall: {average_recall}\")\n",
    "print (f\"Average F1 Score: {average_f1}\")\n",
    "print (f\"Average Dice Similarity: {average_dice}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE - Metrics and Result\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    f.write(f\"Average Precision: {average_precision:.4f}\\n\")\n",
    "    f.write(f\"Average Recall: {average_recall:.4f}\\n\")\n",
    "    f.write(f\"Average F1 Score: {average_f1:.4f}\\n\")\n",
    "    f.write(f\"Average Dice Similarity: {average_dice:.4f}\\n\")\n",
    "\n",
    "with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD - Result\n",
    "with open(result_file, 'r', encoding='utf-8') as f:\n",
    "    result = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance for Re-Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(result)\n",
    "\n",
    "# Calculate per-sample 2-gram overlap metrics\n",
    "def two_gram_overlap(row):\n",
    "    actual = set(row['actual_two_grams'])\n",
    "    predicted = set(row['filtered_two_grams'])\n",
    "    intersection = actual & predicted\n",
    "    return {\n",
    "        \"uid\": row[\"uid\"],\n",
    "        \"precision\": len(intersection) / len(predicted) if predicted else 0,\n",
    "        \"recall\": len(intersection) / len(actual) if actual else 0,\n",
    "        \"f1\": 2 * len(intersection) / (len(actual) + len(predicted)) if actual and predicted else 0,\n",
    "        \"actual_len\": len(actual),\n",
    "        \"predicted_len\": len(predicted)\n",
    "    }\n",
    "\n",
    "overlap_df = pd.DataFrame([two_gram_overlap(row) for _, row in results_df.iterrows()])\n",
    "\n",
    "# Plot 1: Distribution of precision, recall, F1\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(overlap_df[['precision', 'recall', 'f1']], bins=20, kde=True, palette='Set2', element=\"step\", fill=False)\n",
    "plt.title('Distribution of Precision, Recall, F1 across Samples')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['Precision', 'Recall', 'F1'])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Sample examples of reconstruction\n",
    "print(\"\\nüîç Sample Reconstructions (first 5):\")\n",
    "for idx, row in results_df.head(5).iterrows():\n",
    "    print(f\"UID: {row['uid']}\")\n",
    "    print(f\"  Actual 2-grams:       {row['actual_two_grams']}\")\n",
    "    print(f\"  Predicted 2-grams:    {row['filtered_two_grams']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Refinement and Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction LLM\n",
    "reconstructed_results = reconstruct_using_ai(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoconstructed_results_df = pd.DataFrame(reconstructed_results)\n",
    "recoconstructed_results_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
