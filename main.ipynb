{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL): Investigating Dataset Extension Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import all relevant libraries and classes used throughout the project. Key components include:\n",
    "\n",
    "- **Torch** â€“ for tensor operations and neural network functionality  \n",
    "- **Datasets** â€“ for handling training and evaluation data  \n",
    "- **PyTorch Models** â€“ custom and pre-defined models for the DEA  \n",
    "- **Graph Matching Attack (GMA)** â€“ core logic for the initial re-identification phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from graphMatching.gma import run_gma\n",
    "\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset import TwoStepHashDataset\n",
    "\n",
    "from pytorch_models.bloom_filter_to_two_gram_classifier import BloomFilterToTwoGramClassifier\n",
    "from pytorch_models.tab_min_hash_to_two_gram_classifier import TabMinHashToTwoGramClassifier\n",
    "from pytorch_models.two_step_hash_to_two_gram_classifier import TwoStepHashToTwoGramClassifier\n",
    "from pytorch_models.test_model import TestModel\n",
    "\n",
    "from early_stopping.early_stopping import EarlyStopping\n",
    "\n",
    "print('System Version:', sys.version)\n",
    "print('PyTorch version', torch.__version__)\n",
    "print('Torchvision version', torchvision.__version__)\n",
    "print('Numpy version', np.__version__)\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Configuration and parameters for the Graph Matching Attack (GMA) and Dataset Extension Attack (DEA). For details and possible values, refer to the documentation at ```./docs/parameters.md```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/fakename_1k.tsv\",\n",
    "    \"Overlap\": 0.8,\n",
    "    \"DropFrom\": \"Eve\",\n",
    "    \"Verbose\": True,  # Print Status Messages\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": -1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": False,\n",
    "}\n",
    "\n",
    "\n",
    "DEA_CONFIG = {\n",
    "    \"DevMode\": False,\n",
    "    # BCEWithLogitsLoss / MultiLabelSoftMarginLoss\n",
    "    \"LossFunction:\": \"BCEWithLogitsLoss\",\n",
    "    # Adam / AdamW / SGD / RMSprop\n",
    "    \"Optimizer\": \"Adam\",\n",
    "    \"LearningRate\": 0.001,\n",
    "    # SGD only\n",
    "    \"Momentum\": 0.9,\n",
    "    \"BatchSize\": 16,\n",
    "    \"Epochs\": 20,\n",
    "    # TestSize calculated accordingly\n",
    "    \"TrainSize\": 0.8,\n",
    "    \"FilterThreshold\": 0.5,\n",
    "    \"Patience\": 5,\n",
    "    \"MinDelta\": 0.001,\n",
    "}\n",
    "\n",
    "ENC_CONFIG = {\n",
    "    # TwoStepHash / TabMinHash / BloomFilter\n",
    "    \"AliceAlgo\": \"BloomFilter\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"None\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "    # For BF encoding\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "    # For TMH encoding\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "    # For 2SH encoding\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    # For Node2Vec\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42\n",
    "}\n",
    "\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"]/2), #0005\n",
    "    \"RegInit\":1, # For BF 0.25\n",
    "    \"Batchsize\": 1, # 1 = 100%\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5 ,  # 800\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Data Preparation: Load or Compute Graph Matching Attack (GMA) Results\n",
    "\n",
    "This code snippet either loads previously computed Graph Matching Attack (GMA) results from disk or runs the attack if no saved data is found.\n",
    "\n",
    "1. **Generate Configuration Hashes:**  \n",
    "   The function `get_hashes` creates unique hash values based on the encoding and embedding configurations. These are used to create distinct filenames for the data.\n",
    "\n",
    "2. **Create File Paths:**  \n",
    "   Based on the configuration hashes, paths are generated for:\n",
    "   - Reidentified individuals\n",
    "   - Not reidentified individuals\n",
    "   - All individuals in Aliceâ€™s dataset (with encoding)\n",
    "\n",
    "3. **Load Results from Disk (if available):**  \n",
    "   If the `.h5` files already exist, they are loaded using `hickle` and converted into `pandas.DataFrames`.  \n",
    "   The data format assumes that the first row contains the column headers, and the rest is the data â€” hence the slicing `[1:]` and `columns=...`.\n",
    "\n",
    "4. **Run GMA If Data Is Not Available:**  \n",
    "   If the files are missing, the GMA is executed via `run_gma()`. The results are again converted to `DataFrames`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique hash identifiers for the encoding and embedding configurations\n",
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG)\n",
    "\n",
    "# Define file paths based on the configuration hashes\n",
    "path_reidentified = f\"./data/available_to_eve/reidentified_individuals_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "path_not_reidentified = f\"./data/available_to_eve/not_reidentified_individuals_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "path_all = f\"./data/dev/alice_data_complete_with_encoding_{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}.h5\"\n",
    "\n",
    "# Check if the output files already exist\n",
    "if os.path.isfile(path_reidentified) and os.path.isfile(path_not_reidentified) and os.path.isfile(path_all):\n",
    "    # Load previously saved attack results\n",
    "    print(\"Loading previously saved attack results...\")\n",
    "    reidentified_data = hkl.load(path_reidentified)\n",
    "    not_reidentified_data = hkl.load(path_not_reidentified)\n",
    "    all_data = hkl.load(path_all)\n",
    "\n",
    "else:\n",
    "    # Run Graph Matching Attack if files are not found\n",
    "    reidentified_data, not_reidentified_data, all_data = run_gma(\n",
    "        GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG,\n",
    "        eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash\n",
    "    )\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "df_reidentified = pd.DataFrame(reidentified_data[1:], columns=reidentified_data[0])\n",
    "df_not_reidentified = pd.DataFrame(not_reidentified_data[1:], columns=not_reidentified_data[0])\n",
    "df_all = pd.DataFrame(all_data[1:], columns=all_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¤ Create 2-Gram Dictionary (Letters & Digits)\n",
    "\n",
    "This code creates a comprehensive dictionary of all possible **2-grams** (two-character combinations) that consist of lowercase letters and digits.\n",
    "\n",
    "1. **Character Sets:**\n",
    "   - `string.ascii_lowercase`: the lowercase English alphabet ('a' to 'z')\n",
    "   - `string.digits`: the digits '0' to '9'\n",
    "\n",
    "2. **2-Gram Types Generated:**\n",
    "   - **Letter-Letter (LL):** All combinations like `'aa'`, `'ab'`, ..., `'zz'` (26Ã—26 = 676)\n",
    "   - **Digit-Digit (DD):** All combinations like `'00'`, `'01'`, ..., `'99'` (10Ã—10 = 100)\n",
    "   - **Letter-Digit (LD):** All combinations like `'a0'`, `'a1'`, ..., `'z9'` (26Ã—10 = 260)\n",
    "\n",
    "3. **Combining All 2-Grams:**\n",
    "   - All three types are concatenated into a single list.\n",
    "\n",
    "4. **Indexed Dictionary:**\n",
    "   - The `enumerate()` function is used to assign each 2-gram a unique index in `two_gram_dict`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate a dictionary of all possible 2-grams from letters and digits ---\n",
    "\n",
    "# Lowercase alphabet: 'a' to 'z'\n",
    "alphabet = string.ascii_lowercase\n",
    "\n",
    "# Digits: '0' to '9'\n",
    "digits = string.digits\n",
    "\n",
    "# Generate all letter-letter 2-grams (e.g., 'aa', 'ab', ..., 'zz')\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]\n",
    "\n",
    "# Generate all digit-digit 2-grams (e.g., '00', '01', ..., '99')\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]\n",
    "\n",
    "# Generate all letter-digit 2-grams (e.g., 'a0', 'a1', ..., 'z9')\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]\n",
    "\n",
    "# Combine all generated 2-grams into one list\n",
    "all_two_grams = letter_letter_grams + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Create a dictionary mapping index to each 2-gram\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§© Dataset Creation Based on Aliceâ€™s Encoding Scheme\n",
    "\n",
    "This section initializes the dataset objects depending on which encoding method Alice used. Each encoding requires a different preprocessing strategy for compatibility with downstream neural models.\n",
    "\n",
    "### 1. Bloom Filter (`\"BloomFilter\"`)\n",
    "- Uses binary Bloom filters to represent identifiers.\n",
    "- Loads `BloomFilterDataset` objects.\n",
    "- Stores the bit-length of the bloom filter.\n",
    "\n",
    "### 2. Tabulation MinHash (`\"TabMinHash\"`)\n",
    "- Applies a MinHash-based encoding.\n",
    "- Loads `TabMinHashDataset`.\n",
    "- Captures the length of each encoded vector.\n",
    "\n",
    "### 3. Two-Step Hash with One-Hot Encoding (`\"TwoStepHash\"`)\n",
    "- Extracts all **unique hash values** to build a consistent one-hot vector space.\n",
    "- Constructs datasets using `TwoStepHashDatasetOneHotEncoding`.\n",
    "\n",
    "> âš™ï¸ All dataset constructors are passed:\n",
    "> - Whether the data is labeled\n",
    "> - The full 2-gram list (used as feature tokens)\n",
    "> - Additional encoding-specific configurations\n",
    "> - Dev mode toggle (for debugging or smaller runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ï¸ Bloom Filter Encoding\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "    data_labeled = BloomFilterDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = BloomFilterDataset(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    bloomfilter_length = len(df_reidentified[\"bloomfilter\"][0])\n",
    "\n",
    "# 2ï¸ Tabulation MinHash Encoding\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "    data_labeled = TabMinHashDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TabMinHashDataset(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    tabminhash_length = len(df_reidentified[\"tabminhash\"][0])\n",
    "\n",
    "# 3 Two-Step Hash Encoding (One-Hot Encoding Mode)\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\":\n",
    "    # Collect all unique integers across both reidentified and non-reidentified data\n",
    "    unique_ints_reid = set().union(*df_reidentified[\"twostephash\"])\n",
    "    unique_ints_not_reid = set().union(*df_not_reidentified[\"twostephash\"])\n",
    "    unique_ints_sorted = sorted(unique_ints_reid.union(unique_ints_not_reid))\n",
    "    unique_integers_dict = {i: val for i, val in enumerate(unique_ints_sorted)}\n",
    "\n",
    "    data_labeled = TwoStepHashDataset(\n",
    "        df_reidentified,\n",
    "        is_labeled=True,\n",
    "        all_integers=unique_ints_sorted,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )\n",
    "    data_not_labeled = TwoStepHashDataset(\n",
    "        df_not_reidentified,\n",
    "        is_labeled=False,\n",
    "        all_integers=unique_ints_sorted,\n",
    "        all_two_grams=all_two_grams,\n",
    "        dev_mode=GLOBAL_CONFIG[\"DevMode\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting & Loader Setup\n",
    "\n",
    "After preprocessing the encoded data, we divide it into training, validation, and test sets using PyTorch's `DataLoader` and `random_split`.\n",
    "\n",
    "### Dataset Proportions\n",
    "- The proportion for the training set is defined in `DEA_CONFIG[\"TrainSize\"]`.\n",
    "- The remainder is used for validation.\n",
    "\n",
    "### Splitting\n",
    "- `data_labeled` (the reidentified individuals) is split into:\n",
    "  - `data_train` for training\n",
    "  - `data_val` for validation\n",
    "- `data_not_labeled` (unidentified individuals) is used exclusively for testing.\n",
    "\n",
    "### Dataloader Configuration\n",
    "- **Training Loader**: shuffled for learning generalization.\n",
    "- **Validation Loader**: also shuffled to vary batches during evaluation.\n",
    "- **Test Loader**: also shuffled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset split proportions\n",
    "train_size = int(DEA_CONFIG[\"TrainSize\"] * len(data_labeled))\n",
    "val_size = len(data_labeled) - train_size\n",
    "\n",
    "# Split the reidentified dataset into training and validation sets\n",
    "data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "dataloader_train = DataLoader(\n",
    "    data_train,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True  # Important for training\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    data_val,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True  # Allows variation in validation batches\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    data_not_labeled,\n",
    "    batch_size=DEA_CONFIG[\"BatchSize\"],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation Based on Encoding Scheme\n",
    "\n",
    "The neural network model is selected dynamically based on the encoding technique used for Aliceâ€™s data.\n",
    "\n",
    "### Supported Models:\n",
    "\n",
    "- **BloomFilter** â†’ `BloomFilterToTwoGramClassifier`  \n",
    "  - Input: Binary vector (Bloom filter)  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TabMinHash** â†’ `TabMinHashToTwoGramClassifier`  \n",
    "  - Input: Tabulated MinHash signature  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TwoStepHash** â†’ `TwoStepHashToTwoGramClassifier`  \n",
    "  - Input: Length of the unique integers present\n",
    "  - Output: 2-gram predicition\n",
    "    \n",
    "Each model outputs predictions over the set of all possible 2-grams (`all_two_grams`), and the input dimension is dynamically configured based on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TestModel(\n",
    "    input_dim=bloomfilter_length,\n",
    "    output_dim=len(all_two_grams),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model based on selected encoding scheme\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "    model = BloomFilterToTwoGramClassifier(\n",
    "        input_dim=bloomfilter_length,\n",
    "        output_dim=len(all_two_grams)\n",
    "    )\n",
    "\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "    model = TabMinHashToTwoGramClassifier(\n",
    "        input_dim=tabminhash_length,\n",
    "        output_dim=len(all_two_grams)\n",
    "    )\n",
    "\n",
    "elif ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\":\n",
    "    model = TwoStepHashToTwoGramClassifier(\n",
    "        input_dim=len(unique_ints_sorted),\n",
    "        output_dim=len(all_two_grams)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Environment Setup\n",
    "This code initializes the core components needed for training a neural network model.\n",
    "\n",
    "1. TensorBoard Setup\n",
    "    - Creates unique run name by combining:\n",
    "    - Loss function type\n",
    "    - Optimizer choice\n",
    "    - Alice's algorithm\n",
    "    - Initializes TensorBoard writer in runs directory\n",
    "2. Device Configuration\n",
    "    - Automatically selects GPU if available, falls back to CPU\n",
    "    - Moves model to selected device\n",
    "3. Loss Functions\n",
    "    - `BCEWithLogitsLoss`: Binary Cross Entropy with Logits\n",
    "    - `MultiLabelSoftMarginLoss`: Multi-Label Soft Margin Loss\n",
    "4. Optimizers:\n",
    "    - `Adam`: Adaptive Moment Estimation\n",
    "    - `AdamW`: Adam with Weight Decay\n",
    "    - `SGD`: Stochastic Gradient Descent (with momentum)\n",
    "    - `RMSprop`: Root Mean Square Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tensorboard logging\n",
    "run_name = \"\".join([\n",
    "    DEA_CONFIG[\"LossFunction:\"],\n",
    "    DEA_CONFIG[\"Optimizer\"],\n",
    "    ENC_CONFIG[\"AliceAlgo\"],\n",
    "])\n",
    "tb_writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "\n",
    "# Setup compute device (GPU/CPU)\n",
    "compute_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(compute_device)\n",
    "\n",
    "# Initialize loss function\n",
    "match DEA_CONFIG[\"LossFunction:\"]:\n",
    "    case \"BCEWithLogitsLoss\":\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    case \"MultiLabelSoftMarginLoss\":\n",
    "        criterion = nn.MultiLabelSoftMarginLoss(reduction='mean')\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported loss function: {DEA_CONFIG['LossFunction:']}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "match DEA_CONFIG[\"Optimizer\"]:\n",
    "    case \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=DEA_CONFIG[\"LearningRate\"])\n",
    "    case \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=DEA_CONFIG[\"LearningRate\"])\n",
    "    case \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                            lr=DEA_CONFIG[\"LearningRate\"],\n",
    "                            momentum=DEA_CONFIG[\"Momentum\"])\n",
    "    case \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=DEA_CONFIG[\"LearningRate\"])\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported optimizer: {DEA_CONFIG['Optimizer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Early Stopping\n",
    "\n",
    "The function `train_model` orchestrates the training process for the neural network, including both training and validation phases for each epoch. It also utilizes **early stopping** to halt training when the validation loss fails to improve over multiple epochs, avoiding overfitting.\n",
    "\n",
    "### Key Phases:\n",
    "1. **Training Phase**: \n",
    "   - The model is trained on the `dataloader_train`, computing the training loss using the specified loss function (`criterion`) and optimizer. Gradients are calculated, and the model parameters are updated.\n",
    "  \n",
    "2. **Validation Phase**:\n",
    "   - The model is evaluated on the `dataloader_val` without updating weights. The validation loss is computed to track model performance on unseen data.\n",
    "\n",
    "3. **Logging**: \n",
    "   - Training and validation losses are logged to both the console and **TensorBoard** for tracking model performance during training.\n",
    "\n",
    "4. **Early Stopping**: \n",
    "   - If the validation loss does not improve after a certain number of epochs (defined by `DEA_CONFIG[\"Patience\"]`), the training process is halted to prevent overfitting.\n",
    "\n",
    "### Helper Functions:\n",
    "- `run_epoch`: Handles a single epoch, either for training or validation, depending on the flag `is_training`.\n",
    "- `log_metrics`: Logs the training and validation losses to the console and TensorBoard for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, device):\n",
    "    train_losses, val_losses = [], []\n",
    "    early_stopper = EarlyStopping(patience=DEA_CONFIG[\"Patience\"], min_delta=DEA_CONFIG[\"MinDelta\"])\n",
    "\n",
    "    for epoch in range(DEA_CONFIG[\"Epochs\"]):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = run_epoch(\n",
    "            model, dataloader_train, criterion, optimizer,\n",
    "            device, is_training=True\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(\n",
    "            model, dataloader_val, criterion, optimizer,\n",
    "            device, is_training=False\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Logging\n",
    "        log_metrics(train_loss, val_loss, epoch, DEA_CONFIG[\"Epochs\"])\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopper(val_loss):\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def run_epoch(model, dataloader, criterion, optimizer, device, is_training):\n",
    "    running_loss = 0.0\n",
    "    with torch.set_grad_enabled(is_training):\n",
    "        for data, labels, _ in tqdm(dataloader,\n",
    "                                  desc=\"Training\" if is_training else \"Validation\"):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if is_training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def log_metrics(train_loss, val_loss, epoch, total_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{total_epochs} - \"\n",
    "          f\"Train loss: {train_loss:.4f}, \"\n",
    "          f\"Validation loss: {val_loss:.4f}\")\n",
    "    tb_writer.add_scalar(\"Loss/train\", train_loss, epoch + 1)\n",
    "    tb_writer.add_scalar(\"Loss/validation\", val_loss, epoch + 1)\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model, dataloader_train, dataloader_val,\n",
    "    criterion, optimizer, compute_device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Visualization over Epochs\n",
    "\n",
    "This code snippet generates a plot to visualize the **training loss** and **validation loss** across epochs. It's useful for tracking model performance during training and evaluating if overfitting is occurring (i.e., when validation loss starts increasing while training loss continues to decrease).\n",
    "\n",
    "### Key Elements:\n",
    "1. **Plotting the Losses**: \n",
    "   - The `train_losses` and `val_losses` are plotted over the epochs. \n",
    "   - The **blue line** represents the training loss, and the **red line** represents the validation loss.\n",
    "\n",
    "2. **Legend**: \n",
    "   - A legend is added to distinguish between training and validation losses.\n",
    "\n",
    "3. **Title and Labels**: \n",
    "   - The plot is titled \"Training and Validation Loss over Epochs\" for context.\n",
    "   - **X-axis** represents the epoch number, and **Y-axis** represents the loss value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses over epochs\n",
    "plt.plot(train_losses, label='Training loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation loss', color='red')\n",
    "\n",
    "# Adding a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Setting the title and labels for clarity\n",
    "plt.title(\"Training and Validation Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference and 2-Gram Comparison\n",
    "\n",
    "This code performs inference on the test data and compares the predicted 2-grams with the actual 2-grams, providing a performance evaluation based on the **Dice similarity coefficient**.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Prepare for Evaluation**:\n",
    "   - The model is switched to **evaluation mode** (`model.eval()`), ensuring no gradient computation.\n",
    "   \n",
    "2. **Thresholding**:\n",
    "   - A threshold (`DEA_CONFIG[\"FilterThreshold\"]`) is applied to filter out low-probability predictions, retaining only the most confident predictions.\n",
    "\n",
    "3. **Inference and 2-Gram Scoring**:\n",
    "   - The model is applied to the batch, and the **logits** are converted into probabilities using the **sigmoid function**.\n",
    "   - The probabilities are then mapped to **2-gram scores**, and scores below the threshold are discarded.\n",
    "\n",
    "4. **Reconstructing Words**:\n",
    "   - For each sample in the batch, **2-grams** are reconstructed into words based on the filtered scores.\n",
    "\n",
    "5. **Performance Metrics**:\n",
    "   - The actual 2-grams (from the test dataset) are compared with the predicted 2-grams, and the **Dice similarity coefficient** is calculated for each sample.\n",
    "\n",
    "### Result:\n",
    "- The code generates a list `combined_results_performance`, which contains a detailed comparison for each UID, including:\n",
    "  - **Actual 2-grams** (from the test data)\n",
    "  - **Predicted 2-grams** (from the model)\n",
    "  - **Dice similarity** score indicating how similar the actual and predicted 2-grams are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store decoded 2-gram scores for all test samples\n",
    "decoded_test_results_words = []\n",
    "combined_results_performance = []\n",
    "\n",
    "# Switch to evaluation mode (no gradient computation during inference)\n",
    "model.eval()\n",
    "\n",
    "# Define Threshold for filtering predictions\n",
    "threshold = DEA_CONFIG[\"FilterThreshold\"]\n",
    "\n",
    "# Loop through the test dataloader for inference\n",
    "with torch.no_grad():  # No need to compute gradients during inference\n",
    "    for data_batch, uids in tqdm(dataloader_test, desc=\"Test loop\"):\n",
    "        # Filter relevant individuals from the dataset based on UIDs\n",
    "        filtered_df = df_all[df_all[\"uid\"].isin(uids)].drop(df_all.columns[-2], axis=1) # Drop encoding column\n",
    "\n",
    "        # Extract 2-grams from actual data for comparison\n",
    "        actual_two_grams_batch = []\n",
    "        for _, entry in filtered_df.iterrows():\n",
    "            row = entry[:-1]  # Exclude UID from row\n",
    "            extracted_two_grams = extract_two_grams(\"\".join(map(str, row)))  # Extract 2-grams from the row\n",
    "            actual_two_grams_batch.append({\"uid\": entry[\"uid\"], \"two_grams\": extracted_two_grams})\n",
    "\n",
    "        # Move the batch of data to the device (e.g., GPU)\n",
    "        data_batch = data_batch.to(compute_device)\n",
    "\n",
    "        # Apply the model to get logits (raw predictions)\n",
    "        logits = model(data_batch)\n",
    "\n",
    "        # Convert logits to probabilities using sigmoid (binary classification)\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "\n",
    "        # Convert probabilities into 2-gram scores (using the two_gram_dict to map to 2-gram labels)\n",
    "        batch_two_gram_scores = [\n",
    "            {two_gram_dict[j]: score.item() for j, score in enumerate(probabilities[i])}  # Map each probability to its 2-gram\n",
    "            for i in range(probabilities.size(0))  # Iterate over each sample in the batch\n",
    "        ]\n",
    "\n",
    "        # Apply threshold to filter out low-scoring 2-grams\n",
    "        batch_filtered_two_gram_scores = [\n",
    "            {two_gram: score for two_gram, score in two_gram_scores.items() if score > threshold}  # Only keep scores above threshold\n",
    "            for two_gram_scores in batch_two_gram_scores\n",
    "        ]\n",
    "\n",
    "        # Filtered 2-grams per UID in the batch\n",
    "        filtered_two_grams = [\n",
    "            {\"uid\": uid, \"two_grams\": {key for key in two_grams.keys()}}  # Only keep the 2-gram keys (no scores)\n",
    "            for uid, two_grams in zip(uids, batch_filtered_two_gram_scores)\n",
    "        ]\n",
    "\n",
    "        # Reconstruct words from the filtered 2-grams for each sample\n",
    "        batch_reconstructed_words = [\n",
    "            reconstruct_words(filtered_scores) for filtered_scores in batch_filtered_two_gram_scores\n",
    "        ]\n",
    "\n",
    "        # Append the reconstructed words to the results list\n",
    "        decoded_test_results_words.extend(batch_reconstructed_words)\n",
    "\n",
    "        # Compare predicted 2-grams with actual 2-grams and calculate performance metrics\n",
    "        for entry_two_grams_batch in actual_two_grams_batch:  # Loop through each UID in the batch\n",
    "            for entry_filtered_two_grams in filtered_two_grams:\n",
    "                if entry_two_grams_batch[\"uid\"] == entry_filtered_two_grams[\"uid\"]:\n",
    "                    # Calculate Dice similarity between actual and predicted 2-grams\n",
    "                    combined_results_performance.append({\n",
    "                        \"uid\": entry_two_grams_batch[\"uid\"],\n",
    "                        \"actual_two_grams\": entry_two_grams_batch[\"two_grams\"],  # Get actual 2-grams for this UID\n",
    "                        \"predicted_two_grams\": entry_filtered_two_grams[\"two_grams\"],  # Get predicted 2-grams for this UID\n",
    "                        \"dice_similarity\": dice_coefficient(entry_two_grams_batch[\"two_grams\"], entry_filtered_two_grams[\"two_grams\"]),\n",
    "                    })\n",
    "\n",
    "# Now `combined_results_performance` contains detailed comparison for all test samples\n",
    "print(combined_results_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(\"Stopping execution at this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance for Re-Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reidentified Individuals:')\n",
    "print(df_reidentified.head())\n",
    "print('Not Reidentified Individuals:')\n",
    "print(df_not_reidentified.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeled.labelTensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, labels in dataloader_train:\n",
    "    data, labels = data.to(compute_device), labels.to(compute_device)\n",
    "    print(data.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To Decode: \",df_not_reidentified.iloc[1])\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "#torch.set_printoptions(profile=\"default\")\n",
    "print(\"Tensor: \", data_not_labeled[1])\n",
    "# Apply model\n",
    "model.eval()\n",
    "logits = model(data_not_labeled[1])\n",
    "probabilities = torch.sigmoid(logits)\n",
    "print(\"Prob: \", probabilities)\n",
    "two_gram_scores = {two_gram_dict[i]: score.item() for i, score in enumerate(probabilities)}\n",
    "threshold = 0.5\n",
    "filtered_two_gram_scores = {two_gram: score for two_gram, score in two_gram_scores.items() if score > threshold}\n",
    "print(\"Decoded 2grams: \", filtered_two_gram_scores)\n",
    "\n",
    "print(reconstruct_words(filtered_two_gram_scores))\n",
    "\n",
    "# person is: Ray Haywood 9/27/1959\n",
    "# ra -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(20, 5) # predict logits for 5 classes\n",
    "x = torch.randn(1, 20)\n",
    "print(x.shape)\n",
    "y = torch.tensor([[1., 0., 1., 0., 0.]]) # get classA and classC as active\n",
    "print(y.shape)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Loss: {:.3f}'.format(loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
