{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL): Investigating Dataset Extension Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from graphMatching.gma import run_gma\n",
    "\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset_padding import TwoStepHashDatasetPadding\n",
    "from datasets.two_step_hash_dataset_frequency_string import TwoStepHashDatasetFrequencyString\n",
    "\n",
    "from pytorch_models.bloom_filter_to_two_gram_classifier import BloomFilterToTwoGramClassifier\n",
    "from pytorch_models.tab_min_hash_to_two_gram_classifier import TabMinHashToTwoGramClassifier\n",
    "from pytorch_models.two_step_hash_to_two_gram_classifier import TwoStepHashToTwoGramClassifier\n",
    "\n",
    "print('System Version:', sys.version)\n",
    "print('PyTorch version', torch.__version__)\n",
    "print('Torchvision version', torchvision.__version__)\n",
    "print('Numpy version', np.__version__)\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/titanic_full.tsv\",\n",
    "    \"Overlap\": 0.9,\n",
    "    \"DropFrom\": \"Both\",\n",
    "    \"Verbose\": True,  # Print Status Messages\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": -1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": True,\n",
    "}\n",
    "\n",
    "# For TSH: Padding / FrequencyString\n",
    "DEA_CONFIG = {\n",
    "    \"TSHMode\": \"Padding\",\n",
    "    \"DevMode\": False,\n",
    "}\n",
    "\n",
    "ENC_CONFIG = {\n",
    "    \"AliceAlgo\": \"TwoStepHash\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"None\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "    # For BF encoding\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "    # For TMH encoding\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "    # For 2SH encoding\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    # For Node2Vec\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42\n",
    "}\n",
    "\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"]/2), #0005\n",
    "    \"RegInit\":1, # For BF 0.25\n",
    "    \"Batchsize\": 1, # 1 = 100%\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5 ,  # 800\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG)\n",
    "\n",
    "if(os.path.isfile(\"./data/available_to_eve/reidentified_individuals_%s_%s_%s_%s.tsv\" % (eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash)) & os.path.isfile(\"./data/available_to_eve/not_reidentified_individuals_%s_%s_%s_%s.h5\" % (eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash))):\n",
    "    #Load Disk From Data\n",
    "    #TODO: Replace pd.read_csv with hkl.load\n",
    "    reidentified_individuals = hkl.load('./data/available_to_eve/reidentified_individuals_%s_%s_%s_%s.h5' % (eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash))\n",
    "    df_reidentified_individuals = pd.DataFrame(reidentified_individuals[1:], columns=reidentified_individuals[0])\n",
    "\n",
    "    not_reidentified_individuals = hkl.load('./data/available_to_eve/not_reidentified_individuals_%s_%s_%s_%s.h5' % (eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash))\n",
    "    df_not_reidentified_individuals = pd.DataFrame(not_reidentified_individuals[1:], columns=not_reidentified_individuals[0])\n",
    "\n",
    "else:\n",
    "    reidentified_individuals, not_reidentified_individuals = run_gma(GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG, eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash)\n",
    "\n",
    "    df_reidentified_individuals = pd.DataFrame(reidentified_individuals[1:], columns=reidentified_individuals[0])\n",
    "    df_not_reidentified_individuals = pd.DataFrame(not_reidentified_individuals[1:], columns=not_reidentified_individuals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the 2-grams with dictionary\n",
    "\n",
    "#Generate all 2-grams\n",
    "alphabet = string.ascii_lowercase\n",
    "\n",
    "# Generate all letter-letter 2-grams (aa-zz)\n",
    "alphabet = string.ascii_lowercase\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]\n",
    "\n",
    "# Generate all digit-digit 2-grams (00-99)\n",
    "digits = string.digits\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]\n",
    "\n",
    "# Generate all letter-digit 2-grams (a0-z9)\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]\n",
    "\n",
    "# Combine all sets\n",
    "all_two_grams = letter_letter_grams  + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Get a dictionary associating each 2-gram with an index\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets based on chosen encoding\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "    data_labeled = BloomFilterDataset(df_reidentified_individuals, is_labeled=True, all_two_grams=all_two_grams, dev_mode=GLOBAL_CONFIG[\"DevMode\"])\n",
    "    data_not_labeled = BloomFilterDataset(df_not_reidentified_individuals, is_labeled=False, all_two_grams=all_two_grams, dev_mode=GLOBAL_CONFIG[\"DevMode\"])\n",
    "    bloomfilter_length = len(df_reidentified_individuals[\"bloomfilter\"][0])\n",
    "\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "    data_labeled = TabMinHashDataset(df_reidentified_individuals, is_labeled=True, all_two_grams=all_two_grams, dev_mode=GLOBAL_CONFIG[\"DevMode\"])\n",
    "    data_not_labeled = TabMinHashDataset(df_not_reidentified_individuals, is_labeled=False, all_two_grams=all_two_grams, dev_mode=GLOBAL_CONFIG[\"DevMode\"])\n",
    "    tabminhash_length = len(df_reidentified_individuals[\"tabminhash\"][0])\n",
    "\n",
    "if (ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\") & (DEA_CONFIG[\"TSHMode\"] == \"Padding\"):\n",
    "    max_length_reidentified = df_reidentified_individuals[\"twostephash\"].apply(lambda x: len(list(x))).max()\n",
    "    max_length_not_reidentified = df_not_reidentified_individuals[\"twostephash\"].apply(lambda x: len(list(x))).max()\n",
    "    max_twostephash_length = max(max_length_reidentified, max_length_not_reidentified)\n",
    "    data_labeled = TwoStepHashDatasetPadding(df_reidentified_individuals, is_labeled=True, all_two_grams=all_two_grams, max_set_size=max_twostephash_length, dev_mode=GLOBAL_CONFIG[\"DevMode\"])\n",
    "    data_not_labeled = TwoStepHashDatasetPadding(df_not_reidentified_individuals, is_labeled=False, all_two_grams=all_two_grams, max_set_size=max_twostephash_length, dev_mode=GLOBAL_CONFIG[\"DevMode\"])\n",
    "\n",
    "if (ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\") & (DEA_CONFIG[\"TSHMode\"] == \"FrequencyString\"):\n",
    "    max_length_reidentified = df_reidentified_individuals[\"twostephash\"].apply(lambda x: max(x)).max()\n",
    "    max_length_not_reidentified = df_not_reidentified_individuals[\"twostephash\"].apply(lambda x: max(x)).max()\n",
    "    max_twostephash_length = max(max_length_reidentified, max_length_not_reidentified)\n",
    "    data_labeled = TwoStepHashDatasetFrequencyString(df_reidentified_individuals, is_labeled=True, all_two_grams=all_two_grams, frequency_string_length=max_twostephash_length, dev_mode=GLOBAL_CONFIG[\"DevMode\"])\n",
    "    data_not_labeled = TwoStepHashDatasetFrequencyString(df_not_reidentified_individuals, is_labeled=False, all_two_grams=all_two_grams, frequency_string_length=max_twostephash_length, dev_mode=GLOBAL_CONFIG[\"DevMode\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split proportions\n",
    "train_size = int(0.8 * len(data_labeled))  # 80% training\n",
    "val_size = len(data_labeled) - train_size  # 20% validation\n",
    "\n",
    "#Split dataset of reidentified individuals\n",
    "data_train, data_val = random_split(data_labeled, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "dataloader_train = DataLoader(data_train, batch_size=1, shuffle=True)\n",
    "dataloader_val = DataLoader(data_val, batch_size=1, shuffle=True)\n",
    "dataloader_test = DataLoader(data_not_labeled, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Models based on chosen encoding scheme\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"BloomFilter\":\n",
    "    model = BloomFilterToTwoGramClassifier(input_dim=bloomfilter_length, num_two_grams=len(all_two_grams))\n",
    "\n",
    "if ENC_CONFIG[\"AliceAlgo\"] == \"TabMinHash\":\n",
    "    model = TabMinHashToTwoGramClassifier(input_dim=tabminhash_length, num_two_grams=len(all_two_grams))\n",
    "\n",
    "if (ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\") & (DEA_CONFIG[\"TSHMode\"] == \"Padding\"):\n",
    "    model = TwoStepHashToTwoGramClassifier(input_dim=max_twostephash_length, num_two_grams=len(all_two_grams))\n",
    "\n",
    "if (ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\") & (DEA_CONFIG[\"TSHMode\"] == \"FrequencyString\"):\n",
    "    model = TabMinHashToTwoGramClassifier(input_dim=max_twostephash_length, num_two_grams=len(all_two_grams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function for multi-label classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeled.labelTensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, labels in dataloader_train:\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "    print(data.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 15\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, labels in tqdm(dataloader_train, desc=\"Training loop\"):\n",
    "        # Move data to device\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    train_loss = running_loss / len(dataloader_train.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    #Calculate true training loss?\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in tqdm(dataloader_val, desc=\"Validation loop\"):\n",
    "            # Move data to device\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "        val_loss = running_loss / len(dataloader_val.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Losses and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Identify Not-Reidentified Individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance for Re-Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(\"Stopping execution at this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reidentified Individuals:')\n",
    "print(df_reidentified_individuals.head())\n",
    "print('Not Reidentified Individuals:')\n",
    "print(df_not_reidentified_individuals.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeled.labelTensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, labels in dataloader_train:\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "    print(data.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To Decode: \",df_not_reidentified_individuals.iloc[1])\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "#torch.set_printoptions(profile=\"default\")\n",
    "print(\"BF Tensor: \", data_not_labeled[1])\n",
    "# Apply model\n",
    "model.eval()\n",
    "logits = model(data_not_labeled[1])\n",
    "probabilities = torch.sigmoid(logits)\n",
    "print(\"Prob: \", probabilities)\n",
    "two_gram_scores = {two_gram_dict[i]: score.item() for i, score in enumerate(probabilities)}\n",
    "threshold = 0.1\n",
    "filtered_two_gram_scores = {two_gram: score for two_gram, score in two_gram_scores.items() if score > threshold}\n",
    "print(\"Decoded 2grams: \", filtered_two_gram_scores)\n",
    "\n",
    "# person is: Michael\tSchley\t1/9/1962"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(20, 5) # predict logits for 5 classes\n",
    "x = torch.randn(1, 20)\n",
    "print(x.shape)\n",
    "y = torch.tensor([[1., 0., 1., 0., 0.]]) # get classA and classC as active\n",
    "print(y.shape)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Loss: {:.3f}'.format(loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
