{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL): Investigating Dataset Extension Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Imports\n",
    "\n",
    "#### PyTorch\n",
    "- `torch`, `torch.nn`, `torch.optim`, `DataLoader`: For building, training, and evaluating neural networks for DEA.\n",
    "- `SummaryWriter`: Logs training metrics for visualization in TensorBoard.\n",
    "\n",
    "#### Ray\n",
    "- `tune`, `air`, `train`, `OptunaSearch`, `ASHAScheduler`: Used for distributed hyperparameter tuning and model optimization.\n",
    "\n",
    "#### Data Handling & Visualization\n",
    "- `pandas`, `numpy`, `matplotlib.pyplot`, `seaborn`: For data manipulation, analysis, and plotting.\n",
    "- `hickle`: Efficient binary serialization format for NumPy arrays and Python objects.\n",
    "- `tqdm.notebook`: Progress bars for loops, especially in Jupyter notebooks.\n",
    "\n",
    "#### Custom Modules\n",
    "- `utils`: A comprehensive set of utility functions for DEA-specific tasks like reconstruction and result logging.\n",
    "- `datasets`: Dataset wrappers for different encoding schemes (Bloom Filter, Tab MinHash, Two-Step Hash).\n",
    "- `pytorch_models`, `early_stopping`: Custom PyTorch model definitions and early stopping mechanism.\n",
    "- `graphMatching.gma`: Executes Graph Matching Attack (GMA) to prepare DEA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NumPy dot (Mac)\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ray libraries for hyperparameter tuning and parallelism\n",
    "import ray\n",
    "from ray import air, train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "# Custom utilities and logic\n",
    "from early_stopping.early_stopping import EarlyStopping\n",
    "from graphMatching.gma import run_gma\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset import TwoStepHashDataset\n",
    "from pytorch_models.base_model import BaseModel\n",
    "from pytorch_models_hyperparameter_optimization.base_model_hyperparameter_optimization import BaseModelHyperparameterOptimization\n",
    "from utils import (\n",
    "    calculate_performance_metrics,\n",
    "    clean_result_dict,\n",
    "    create_identifier_column_dynamic,\n",
    "    decode_labels_to_two_grams,\n",
    "    filter_high_scoring_two_grams,\n",
    "    fuzzy_reconstruction_approach,\n",
    "    get_hashes,\n",
    "    greedy_reconstruction,\n",
    "    load_dataframe,\n",
    "    lowercase_df,\n",
    "    map_probabilities_to_two_grams,\n",
    "    metrics_per_entry,\n",
    "    print_and_save_result,\n",
    "    read_header,\n",
    "    reconstruct_identities_with_llm,\n",
    "    reidentification_analysis,\n",
    "    resolve_config,\n",
    "    run_epoch,\n",
    "    save_dea_runtime_log,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Overview\n",
    "\n",
    "#### GLOBAL_CONFIG\n",
    "General control parameters for DEA runs.\n",
    "- `Data`: Path to dataset (e.g., fakename_5k.tsv).\n",
    "- `Overlap`: Proportion of shared entities between Alice and Eve.\n",
    "- `DropFrom`: Which party (Alice, Eve or both) gets the non-overlapping entities removed.\n",
    "- `MatchingMetric`, `Matching`: Used in GMA (e.g., cosine similarity, MinWeight matching).\n",
    "- `Workers`: Number of parallel threads (-1 = all available).\n",
    "- `BenchMode`, `DevMode`: Toggle benchmarking or development behaviors.\n",
    "\n",
    "#### DEA_CONFIG\n",
    "Training configuration for the neural network used in the Dataset Extension Attack.\n",
    "- `TrainSize`, `Epochs`, `Patience`: Classic train/test split and early stopping.\n",
    "- `NumSamples`: Number of tuning samples for Ray.\n",
    "- `MetricToOptimize`: Metric guiding model selection (e.g., `average_dice`).\n",
    "- `MatchingTechnique`: Post-processing method (e.g.,`fuzzy`, `greedy`, `ai`, `fuzzy_and_greedy`).\n",
    "\n",
    "#### ENC_CONFIG\n",
    "Controls how both Alice’s and Eve’s data are encoded.\n",
    "- `AliceAlgo`, `EveAlgo`: Chosen encoding methods (BloomFilter, TabMinHash, TwoStepHash).\n",
    "- Parameters are grouped by technique (BF, TMH, 2SH), but all are present to allow switching.\n",
    "\n",
    "#### EMB_CONFIG\n",
    "Defines embedding model (e.g., Node2Vec) parameters for both parties.\n",
    "- `Dim`, `Context`, `Negative`: Node2Vec embedding dimensions and context window.\n",
    "- `WalkLen`, `NWalks`, `P`, `Q`: Random walk hyperparameters.\n",
    "- `Quantile`, `Discretize`, `Normalize`: Post-embedding processing.\n",
    "\n",
    "#### ALIGN_CONFIG\n",
    "Parameters for alignment-based reconstruction.\n",
    "- `RegWS`, `RegInit`: Regularization weights.\n",
    "- `NIterWS`, `NIterInit`, `NEpochWS`: Iteration limits.\n",
    "- `Wasserstein`: Use Wasserstein-based alignment loss.\n",
    "- `EarlyStopping`, `LRDecay`: Learning stability controls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General Parameters ===\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/fakename_1k.tsv\",\n",
    "    \"Overlap\": 0.9,\n",
    "    \"DropFrom\": \"Both\",\n",
    "    \"Verbose\": False,\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": os.cpu_count() - 1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": False,\n",
    "    \"BenchMode\": True,\n",
    "    \"LoadResults\": False,\n",
    "    \"LoadPath\": \"\",\n",
    "    \"SaveResults\": True,\n",
    "}\n",
    "\n",
    "# === DEA Training Parameters ===\n",
    "DEA_CONFIG = {\n",
    "    \"TrainSize\": 0.8,\n",
    "    \"Patience\": 5,\n",
    "    \"MinDelta\": 1e-4,\n",
    "    \"NumSamples\": 25,\n",
    "    \"Epochs\": 15,\n",
    "    \"MetricToOptimize\": \"average_dice\",  # Options: \"average_dice\", \"average_precision\", ...\n",
    "    # Fuzyy works only if first three columns resemble: givenname, surname, birthdate (dd/mm/yyyy) format (naming of columns is irellevant)\n",
    "    \"MatchingTechnique\": \"fuzzy_and_greedy\",  # Options: \"ai\", \"greedy\", \"fuzzy\", ...\n",
    "}\n",
    "\n",
    "# === Encoding Parameters for Alice & Eve ===\n",
    "ENC_CONFIG = {\n",
    "    # Encoding technique\n",
    "    \"AliceAlgo\": \"BloomFilter\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"BloomFilter\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "\n",
    "    # Bloom Filter specific\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "\n",
    "    # Tabulation MinHash specific\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "\n",
    "    # Two-Step Hashing specific\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "# === Embedding Configuration (e.g., Node2Vec) ===\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42,\n",
    "}\n",
    "\n",
    "# === Graph Alignment Config ===\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"] / 2),\n",
    "    \"RegInit\": 1,\n",
    "    \"Batchsize\": 1,\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5,\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define character sets\n",
    "alphabet = string.ascii_lowercase  # 'a' to 'z'\n",
    "digits = string.digits             # '0' to '9'\n",
    "\n",
    "# Generate 2-grams\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]   # 'aa' to 'zz'\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]     # '00' to '99'\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]      # 'a0' to 'z9'\n",
    "\n",
    "# Combine all 2-gram types\n",
    "all_two_grams = letter_letter_grams + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Map index to 2-gram string\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load or Compute Graph Matching Attack (GMA) Results\n",
    "\n",
    "This step ensures GMA results are available by either loading existing output files or triggering a new attack run.\n",
    "\n",
    "1. **Start Benchmark Timing (Optional):**  \n",
    "   If benchmarking is enabled (`BenchMode=True`), timers are started to measure runtime for the GMA computation.\n",
    "\n",
    "2. **Generate Configuration Hashes:**  \n",
    "   The function `get_hashes()` generates unique identifiers (hashes) based on the encoding (`ENC_CONFIG`) and embedding (`EMB_CONFIG`) settings for both Alice and Eve.  \n",
    "   These hashes are concatenated into a unique string identifier to distinguish different experiment setups.\n",
    "\n",
    "3. **Construct File Paths:**  \n",
    "   The identifier is used to define expected file paths for:\n",
    "   - `reidentified_individuals.h5` — records successfully linked between Eve and Alice  \n",
    "   - `not_reidentified_individuals.h5` — records not matched  \n",
    "   - `alice_data_complete_with_encoding.h5` — full encoded data for Alice  \n",
    "\n",
    "4. **Check for Existing Results:**  \n",
    "   If all three expected files exist, the GMA step is skipped. This avoids redundant computation.\n",
    "\n",
    "5. **Run GMA if Needed:**  \n",
    "   If any file is missing, the Graph Matching Attack is executed using `run_gma()`, which writes the results to disk using the same naming convention based on the identifier.\n",
    "\n",
    "6. **End Benchmark Timing (Optional):**  \n",
    "   If benchmarking is active, the elapsed time for the GMA step is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3000 of 3000\n",
      "Success rate: 1.000000\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_total = time.time()\n",
    "    start_gma = time.time()\n",
    "\n",
    "# Get absolute path to data directory\n",
    "data_dir = os.path.abspath(\"./data\")\n",
    "\n",
    "# Generate encoding and embedding hashes for reproducible identifiers\n",
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(\n",
    "    GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG\n",
    ")\n",
    "identifier = f\"{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}\"\n",
    "\n",
    "# Build file paths for reidentified, not reidentified, and full encoded data\n",
    "path_reidentified = f\"{data_dir}/available_to_eve/reidentified_individuals_{identifier}.h5\"\n",
    "path_not_reidentified = f\"{data_dir}/available_to_eve/not_reidentified_individuals_{identifier}.h5\"\n",
    "path_all = f\"{data_dir}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\"\n",
    "\n",
    "# Run GMA only if the expected output files do not yet exist\n",
    "if not (\n",
    "    os.path.isfile(path_reidentified)\n",
    "    and os.path.isfile(path_not_reidentified)\n",
    "    and os.path.isfile(path_all)\n",
    "):\n",
    "    run_gma(\n",
    "        GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG,\n",
    "        eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash\n",
    "    )\n",
    "\n",
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_gma = time.time() - start_gma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Training, Validation, and Test Data\n",
    "\n",
    "This step prepares the datasets needed for training and evaluation of the Dataset Extension Attack (DEA).  \n",
    "Depending on the selected encoding algorithm and available data, different dataset loaders are used.\n",
    "\n",
    "1. **Start Benchmark Timing (Optional):**  \n",
    "   If `BenchMode` is enabled, a timer is started to track the time spent in data preparation.\n",
    "\n",
    "2. **Load Reidentified and Not Reidentified Data:**  \n",
    "   - `reidentified_individuals_*.h5`: Contains entities successfully linked by the Graph Matching Attack.\n",
    "   - `not_reidentified_individuals_*.h5` and `alice_data_complete_with_encoding_*.h5`:  \n",
    "     Used to construct a labeled test set of entities not reidentified by the GMA.\n",
    "\n",
    "3. **Select Dataset Type Based on Encoding:**\n",
    "   - If `AliceAlgo` is set to `\"BloomFilter\"`: `BloomFilterDataset` is used.\n",
    "   - If `TabMinHash`: `TabMinHashDataset` is used.\n",
    "   - If `TwoStepHash`: `TwoStepHashDataset` is used with integer feature vectors.\n",
    "\n",
    "4. **Split Labeled Data into Train and Validation Sets:**  \n",
    "   The reidentified dataset is split according to `DEA_CONFIG[\"TrainSize\"]`.\n",
    "\n",
    "5. **Return Datasets:**  \n",
    "   The function returns:\n",
    "   - `data_train`: for model training  \n",
    "   - `data_val`: for validation during training  \n",
    "   - `data_test` (optional): for evaluating the reconstruction on not reidentified entities\n",
    "\n",
    "6. **Drop Redundant Columns (if needed):**  \n",
    "   The function `load_not_reidentified_data()` ensures compatibility of test data by removing unnecessary columns.\n",
    "\n",
    "7. **End Benchmark Timing (Optional):**  \n",
    "   If benchmarking is active, the elapsed time for data preparation is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_path(data_directory, identifier, alice_enc_hash, name=\"dataset\"):\n",
    "    os.makedirs(f\"{data_directory}/cache\", exist_ok=True)\n",
    "    return os.path.join(data_directory, \"cache\", f\"{name}_{identifier}_{alice_enc_hash}.pkl\")\n",
    "\n",
    "def load_data(data_directory, alice_enc_hash, identifier, load_test=False):\n",
    "    cache_path = get_cache_path(data_directory, identifier, alice_enc_hash)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            data_train, data_val, data_test = pickle.load(f)\n",
    "        return data_train, data_val, data_test\n",
    "\n",
    "    # Load from raw files\n",
    "    df_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/reidentified_individuals_{identifier}.h5\")\n",
    "\n",
    "    df_test = None\n",
    "    if load_test:\n",
    "        df_not_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/not_reidentified_individuals_{identifier}.h5\")\n",
    "        df_all = load_dataframe(f\"{data_directory}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\")\n",
    "        df_test = df_all[df_all[\"uid\"].isin(df_not_reidentified[\"uid\"])].reset_index(drop=True)\n",
    "\n",
    "    def get_encoding_dataset_class():\n",
    "        algo = ENC_CONFIG[\"AliceAlgo\"]\n",
    "        if algo == \"BloomFilter\":\n",
    "            return BloomFilterDataset\n",
    "        elif algo == \"TabMinHash\":\n",
    "            return TabMinHashDataset\n",
    "        elif algo == \"TwoStepHash\":\n",
    "            return TwoStepHashDataset\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoding algorithm: {algo}\")\n",
    "\n",
    "    DatasetClass = get_encoding_dataset_class()\n",
    "\n",
    "    if ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\":\n",
    "        unique_ints = sorted(set().union(*df_reidentified[\"twostephash\"]))\n",
    "        dataset_args = {\"all_integers\": unique_ints}\n",
    "    else:\n",
    "        dataset_args = {}\n",
    "\n",
    "    common_args = {\n",
    "        \"is_labeled\": True,\n",
    "        \"all_two_grams\": all_two_grams,\n",
    "        \"dev_mode\": GLOBAL_CONFIG[\"DevMode\"]\n",
    "    }\n",
    "\n",
    "    data_labeled = DatasetClass(df_reidentified, **common_args, **dataset_args)\n",
    "    data_test = DatasetClass(df_test, **common_args, **dataset_args) if load_test else None\n",
    "\n",
    "    train_size = int(DEA_CONFIG[\"TrainSize\"] * len(data_labeled))\n",
    "    val_size = len(data_labeled) - train_size\n",
    "    data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "\n",
    "    # Save to cache\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump((data_train, data_val, data_test), f)\n",
    "    return data_train, data_val, data_test\n",
    "\n",
    "\n",
    "def load_not_reidentified_data(data_directory, alice_enc_hash, identifier):\n",
    "        cache_path = get_cache_path(data_directory, identifier, alice_enc_hash, name=\"not_reidentified\")\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                df_filtered = pickle.load(f)\n",
    "            return df_filtered\n",
    "\n",
    "        df_not_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/not_reidentified_individuals_{identifier}.h5\")\n",
    "        df_all = load_dataframe(f\"{data_directory}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\")\n",
    "\n",
    "        df_filtered = df_all[df_all[\"uid\"].isin(df_not_reidentified[\"uid\"])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        # Drop column by name instead of position if possible\n",
    "        drop_col = df_filtered.columns[-2]\n",
    "        df_filtered = df_filtered.drop(columns=[drop_col])\n",
    "\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(df_filtered, f)\n",
    "\n",
    "        return df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = GLOBAL_CONFIG[\"Data\"].split(\"/\")[-1].replace(\".tsv\", \"\")\n",
    "experiment_tag = \"experiment_\" + ENC_CONFIG[\"AliceAlgo\"] + \"_\" + selected_dataset + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_to = f\"experiment_results/{experiment_tag}\"\n",
    "os.makedirs(save_to, exist_ok=True)\n",
    "\n",
    "# Combine all configs into one dictionary\n",
    "all_configs = {\n",
    "    \"GLOBAL_CONFIG\": GLOBAL_CONFIG,\n",
    "    \"DEA_CONFIG\": DEA_CONFIG,\n",
    "    \"ENC_CONFIG\": ENC_CONFIG,\n",
    "    \"EMB_CONFIG\": EMB_CONFIG,\n",
    "    \"ALIGN_CONFIG\": ALIGN_CONFIG\n",
    "}\n",
    "\n",
    "# Save as a readable .txt file\n",
    "with open(os.path.join(save_to, \"config.txt\"), \"w\") as f:\n",
    "    for config_name, config_dict in all_configs.items():\n",
    "        f.write(f\"# === {config_name} ===\\n\")\n",
    "        f.write(json.dumps(config_dict, indent=4))\n",
    "        f.write(\"\\n\\n\")\n",
    "os.makedirs(f\"{save_to}/hyperparameteroptimization\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val, data_test = load_data(data_dir, alice_enc_hash, identifier, load_test=True)\n",
    "df_not_reidentified = load_not_reidentified_data(data_dir, alice_enc_hash, identifier)\n",
    "# Exit the function if any of the data frames are empty\n",
    "if len(data_train) == 0 or len(data_val) == 0 or len(data_test) == 0 or df_not_reidentified.empty:\n",
    "    log_path = os.path.join(save_to, \"termination_log.txt\")\n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(\"Training process canceled due to empty dataset.\\n\")\n",
    "        f.write(f\"Length of data_train: {len(data_train)}\\n\")\n",
    "        f.write(f\"Length of data_val: {len(data_val)}\\n\")\n",
    "        f.write(f\"Length of data_test: {len(data_test)}\\n\")\n",
    "        f.write(f\"Length of df_not_reidentified: {len(df_not_reidentified)}\\n\")\n",
    "    print(\"One or more datasets are empty. Termination log written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_hyperparameter_optimization = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, data_dir, output_dim, alice_enc_hash, identifier, patience, min_delta):\n",
    "    # Create DataLoaders for training, validation, and testing\n",
    "\n",
    "    data_train, data_val, _ = load_data(data_dir, alice_enc_hash, identifier, load_test=False)\n",
    "\n",
    "    input_dim = data_train[0][0].shape[0]  # Get the input dimension from the first sample\n",
    "\n",
    "    dataloader_train = DataLoader(\n",
    "        data_train,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,  # Important for training\n",
    "    )\n",
    "\n",
    "    dataloader_val = DataLoader(\n",
    "        data_val,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    total_precision = total_recall = total_f1 = total_dice = total_val_loss = 0.0\n",
    "    num_samples = 0\n",
    "    epochs = 0\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "\n",
    "    # Define and initialize model with hyperparameters from config\n",
    "    model = BaseModelHyperparameterOptimization(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        activation_fn=config[\"activation_fn\"]\n",
    "    )\n",
    "\n",
    "    # Set device for model (GPU or CPU)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Select loss function based on config\n",
    "    loss_functions = {\n",
    "        \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss(),\n",
    "        \"MultiLabelSoftMarginLoss\": nn.MultiLabelSoftMarginLoss(),\n",
    "        \"SoftMarginLoss\": nn.SoftMarginLoss(),\n",
    "    }\n",
    "    criterion = loss_functions[config[\"loss_fn\"]]\n",
    "\n",
    "    learning_rate = config[\"optimizer\"][\"lr\"].sample()\n",
    "    # Select optimizer based on config\n",
    "    optimizers = {\n",
    "        \"Adam\": lambda: optim.Adam(model.parameters(), lr=learning_rate),\n",
    "        \"AdamW\": lambda: optim.AdamW(model.parameters(), lr=learning_rate),\n",
    "        \"SGD\": lambda: optim.SGD(model.parameters(), lr=learning_rate, momentum=config[\"optimizer\"][\"momentum\"].sample()),\n",
    "        \"RMSprop\": lambda: optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    }\n",
    "    optimizer = optimizers[config[\"optimizer\"][\"name\"]]()\n",
    "\n",
    "    schedulers = {\n",
    "        \"StepLR\": lambda: torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config[\"lr_scheduler\"][\"step_size\"].sample(),\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ExponentialLR\": lambda: torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ReduceLROnPlateau\": lambda: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=config[\"lr_scheduler\"][\"mode\"],\n",
    "            factor=config[\"lr_scheduler\"][\"factor\"].sample(),\n",
    "            patience=config[\"lr_scheduler\"][\"patience\"].sample()\n",
    "        ),\n",
    "        \"CosineAnnealingLR\": lambda: torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config[\"lr_scheduler\"][\"T_max\"].sample(),\n",
    "            eta_min=config[\"lr_scheduler\"][\"eta_min\"].sample()\n",
    "        ),\n",
    "        \"CyclicLR\": lambda: torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=config[\"lr_scheduler\"][\"base_lr\"].sample(),\n",
    "            max_lr=config[\"lr_scheduler\"][\"max_lr\"].sample(),\n",
    "            step_size_up=config[\"lr_scheduler\"][\"step_size_up\"].sample(),\n",
    "            mode=config[\"lr_scheduler\"][\"mode_cyclic\"].sample(),\n",
    "            cycle_momentum=False\n",
    "        ),\n",
    "        \"None\": lambda: None,\n",
    "    }\n",
    "    scheduler = schedulers[config[\"lr_scheduler\"][\"name\"]]()\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(DEA_CONFIG[\"Epochs\"]):\n",
    "        epochs += 1\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = run_epoch(model, dataloader_train, criterion, optimizer, device, is_training=True, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(model, dataloader_val, criterion, optimizer, device, is_training=False, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        total_val_loss += val_loss\n",
    "\n",
    "         # Early stopping check\n",
    "        if early_stopper(val_loss):\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    # Test phase with reconstruction and evaluation\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in dataloader_val:\n",
    "\n",
    "            actual_two_grams = decode_labels_to_two_grams(two_gram_dict, labels)\n",
    "\n",
    "            # Move data to device and make predictions\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities into 2-gram scores\n",
    "            batch_two_gram_scores = map_probabilities_to_two_grams(two_gram_dict, probabilities)\n",
    "\n",
    "            # Filter out low-scoring 2-grams\n",
    "            batch_filtered_two_gram_scores = filter_high_scoring_two_grams(batch_two_gram_scores, config[\"threshold\"])\n",
    "\n",
    "            # Calculate performance metrics for evaluation\n",
    "            dice, precision, recall, f1 = calculate_performance_metrics(\n",
    "                actual_two_grams, batch_filtered_two_gram_scores)\n",
    "\n",
    "            total_dice += dice\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "            num_samples += data.size(0) # Batch Size\n",
    "\n",
    "    train.report({\n",
    "            \"average_dice\": total_dice / num_samples,\n",
    "            \"average_precision\": total_precision / num_samples,\n",
    "            \"average_recall\": total_recall / num_samples,\n",
    "            \"average_f1\": total_f1 / num_samples,\n",
    "            \"total_val_loss\": total_val_loss,\n",
    "            \"len_train\": len(dataloader_train.dataset),\n",
    "            \"len_val\": len(dataloader_val.dataset),\n",
    "            \"epochs\": epochs\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-07-07 16:23:36</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:44.11        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.9/32.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=11<br>Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: -0.9373688506086669<br>Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th>activation_fn  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_layer_size</th><th>loss_fn             </th><th>lr_scheduler/T_max  </th><th>lr_scheduler/base_lr  </th><th>lr_scheduler/eta_min  </th><th>lr_scheduler/factor  </th><th>lr_scheduler/gamma  </th><th>lr_scheduler/max_lr  </th><th>lr_scheduler/mode  </th><th>lr_scheduler/mode_cy\n",
       "clic                     </th><th>lr_scheduler/name  </th><th>lr_scheduler/patienc\n",
       "e                     </th><th>lr_scheduler/step_si\n",
       "ze                     </th><th>lr_scheduler/step_si\n",
       "ze_up                     </th><th style=\"text-align: right;\">  num_layers</th><th>optimizer/lr        </th><th>optimizer/momentum  </th><th>optimizer/name  </th><th style=\"text-align: right;\">  threshold</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  average_dice</th><th style=\"text-align: right;\">  average_precision</th><th style=\"text-align: right;\">  average_recall</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_0b93391f</td><td>TERMINATED</td><td>127.0.0.1:40622</td><td>tanh           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.116885</td><td style=\"text-align: right;\">                256</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_1870 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_2680</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1330</td><td>&lt;ray.tune.searc_15d0</td><td>SGD             </td><td style=\"text-align: right;\">   0.757601</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.8412 </td><td style=\"text-align: right;\">     0        </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">       0        </td></tr>\n",
       "<tr><td>train_model_68131dbd</td><td>TERMINATED</td><td>127.0.0.1:40647</td><td>leaky_relu     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.222965</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_cf70 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_efe0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_ece0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.518931</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.9709 </td><td style=\"text-align: right;\">     0.218253 </td><td style=\"text-align: right;\">          0.167273 </td><td style=\"text-align: right;\">       0.316983 </td></tr>\n",
       "<tr><td>train_model_1e1d5fb0</td><td>TERMINATED</td><td>127.0.0.1:40659</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.220768</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>&lt;ray.tune.searc_ac20</td><td>                      </td><td>&lt;ray.tune.searc_8be0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_b6d0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.795418</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.40972</td><td style=\"text-align: right;\">     0.241922 </td><td style=\"text-align: right;\">          0.185202 </td><td style=\"text-align: right;\">       0.352011 </td></tr>\n",
       "<tr><td>train_model_7fcd7e30</td><td>TERMINATED</td><td>127.0.0.1:40622</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.146448</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_4310  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_5b70 </td><td>                   </td><td>&lt;ray.tune.searc_4f70</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_5f60</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_77f0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.510516</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.244  </td><td style=\"text-align: right;\">     0.253258 </td><td style=\"text-align: right;\">          0.193889 </td><td style=\"text-align: right;\">       0.368461 </td></tr>\n",
       "<tr><td>train_model_cf99efc6</td><td>TERMINATED</td><td>127.0.0.1:40697</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.371079</td><td style=\"text-align: right;\">                512</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_8c40</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_8100</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.58453 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.0452 </td><td style=\"text-align: right;\">     0.12489  </td><td style=\"text-align: right;\">          0.567194 </td><td style=\"text-align: right;\">       0.0715855</td></tr>\n",
       "<tr><td>train_model_76f25870</td><td>TERMINATED</td><td>127.0.0.1:40659</td><td>leaky_relu     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.2766  </td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_d780 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_bbb0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_d930</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.48506 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        22.6397 </td><td style=\"text-align: right;\">     0.220739 </td><td style=\"text-align: right;\">          0.169141 </td><td style=\"text-align: right;\">       0.320619 </td></tr>\n",
       "<tr><td>train_model_64fa7d32</td><td>TERMINATED</td><td>127.0.0.1:40647</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.189061</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_8190 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_ae60</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_ab00</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.536818</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        36.9282 </td><td style=\"text-align: right;\">     0.122945 </td><td style=\"text-align: right;\">          0.954444 </td><td style=\"text-align: right;\">       0.0663822</td></tr>\n",
       "<tr><td>train_model_044fc1d7</td><td>TERMINATED</td><td>127.0.0.1:40723</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.127073</td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0620</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_a620</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.649088</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.6461 </td><td style=\"text-align: right;\">     0.103585 </td><td style=\"text-align: right;\">          0.945    </td><td style=\"text-align: right;\">       0.0548439</td></tr>\n",
       "<tr><td>train_model_e228ee66</td><td>TERMINATED</td><td>127.0.0.1:40753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.298638</td><td style=\"text-align: right;\">                128</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_9d20</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_a740</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.707341</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.42646</td><td style=\"text-align: right;\">     0.103585 </td><td style=\"text-align: right;\">          0.945    </td><td style=\"text-align: right;\">       0.0548439</td></tr>\n",
       "<tr><td>train_model_58b55221</td><td>TERMINATED</td><td>127.0.0.1:40697</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.238702</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_84c0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_8160 </td><td>                   </td><td>&lt;ray.tune.searc_85b0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_9c60</td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_0f40</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.338835</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.82227</td><td style=\"text-align: right;\">     0.218696 </td><td style=\"text-align: right;\">          0.167424 </td><td style=\"text-align: right;\">       0.318208 </td></tr>\n",
       "<tr><td>train_model_16ab9108</td><td>TERMINATED</td><td>127.0.0.1:40622</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.153429</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_b490 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_8400</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_8580</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.792504</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.3472 </td><td style=\"text-align: right;\">     0.20404  </td><td style=\"text-align: right;\">          0.156566 </td><td style=\"text-align: right;\">       0.295622 </td></tr>\n",
       "<tr><td>train_model_448dae6a</td><td>TERMINATED</td><td>127.0.0.1:40753</td><td>tanh           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.120023</td><td style=\"text-align: right;\">                512</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_8ac0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_9540</td><td>&lt;ray.tune.searc_91e0</td><td>SGD             </td><td style=\"text-align: right;\">   0.730782</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.9674 </td><td style=\"text-align: right;\">     0        </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">       0        </td></tr>\n",
       "<tr><td>train_model_2130d398</td><td>TERMINATED</td><td>127.0.0.1:40697</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.371573</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_bc10  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_9ae0 </td><td>                   </td><td>&lt;ray.tune.searc_ba90</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_93c0</td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_8e50</td><td>&lt;ray.tune.searc_9780</td><td>SGD             </td><td style=\"text-align: right;\">   0.512396</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.08549</td><td style=\"text-align: right;\">     0.0204837</td><td style=\"text-align: right;\">          0.0156566</td><td style=\"text-align: right;\">       0.0299103</td></tr>\n",
       "<tr><td>train_model_e6ad2ec0</td><td>TERMINATED</td><td>127.0.0.1:40697</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.328706</td><td style=\"text-align: right;\">                256</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_4e50 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_4ca0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_53f0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.300094</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.33493</td><td style=\"text-align: right;\">     0.231734 </td><td style=\"text-align: right;\">          0.177626 </td><td style=\"text-align: right;\">       0.336383 </td></tr>\n",
       "<tr><td>train_model_253c1a68</td><td>TERMINATED</td><td>127.0.0.1:40723</td><td>elu            </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.263816</td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0620</td><td>&lt;ray.tune.searc_a200</td><td>                      </td><td>&lt;ray.tune.searc_8a90  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_a0b0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.329259</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.0421 </td><td style=\"text-align: right;\">     0.420172 </td><td style=\"text-align: right;\">          0.812538 </td><td style=\"text-align: right;\">       0.293072 </td></tr>\n",
       "<tr><td>train_model_f6a0308e</td><td>TERMINATED</td><td>127.0.0.1:40622</td><td>selu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.371818</td><td style=\"text-align: right;\">                256</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_a290</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_83d0</td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_a7a0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.407302</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.96803</td><td style=\"text-align: right;\">     0.103585 </td><td style=\"text-align: right;\">          0.945    </td><td style=\"text-align: right;\">       0.0548439</td></tr>\n",
       "<tr><td>train_model_b81d00db</td><td>TERMINATED</td><td>127.0.0.1:40659</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.174456</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0620</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_3a30</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_31c0</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_9690</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.398228</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.98124</td><td style=\"text-align: right;\">     0.0297762</td><td style=\"text-align: right;\">          0.0227778</td><td style=\"text-align: right;\">       0.0434161</td></tr>\n",
       "<tr><td>train_model_8a7bef76</td><td>TERMINATED</td><td>127.0.0.1:40753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.185495</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0620</td><td>&lt;ray.tune.searc_73a0</td><td>                      </td><td>&lt;ray.tune.searc_5390  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_7880</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.361229</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.999  </td><td style=\"text-align: right;\">     0.3897   </td><td style=\"text-align: right;\">          0.969013 </td><td style=\"text-align: right;\">       0.253133 </td></tr>\n",
       "<tr><td>train_model_f3ca91f9</td><td>TERMINATED</td><td>127.0.0.1:40697</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.180512</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0620</td><td>&lt;ray.tune.searc_0bb0</td><td>                      </td><td>&lt;ray.tune.searc_0d00  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_0d60</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.3968  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.9768 </td><td style=\"text-align: right;\">     0.135476 </td><td style=\"text-align: right;\">          0.955278 </td><td style=\"text-align: right;\">       0.0738994</td></tr>\n",
       "<tr><td>train_model_1252871e</td><td>TERMINATED</td><td>127.0.0.1:40622</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.199121</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0620</td><td>&lt;ray.tune.searc_0d30</td><td>                      </td><td>&lt;ray.tune.searc_3eb0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_19c0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.416909</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.206  </td><td style=\"text-align: right;\">     0.17233  </td><td style=\"text-align: right;\">          0.972639 </td><td style=\"text-align: right;\">       0.0967123</td></tr>\n",
       "<tr><td>train_model_065cc057</td><td>TERMINATED</td><td>127.0.0.1:40659</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.186362</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0620</td><td>&lt;ray.tune.searc_1060</td><td>                      </td><td>&lt;ray.tune.searc_26b0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_3d90</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.620387</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.3989 </td><td style=\"text-align: right;\">     0.732235 </td><td style=\"text-align: right;\">          0.987263 </td><td style=\"text-align: right;\">       0.592352 </td></tr>\n",
       "<tr><td>train_model_67d99ffc</td><td>TERMINATED</td><td>127.0.0.1:40723</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.188145</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>&lt;ray.tune.searc_e740</td><td>                      </td><td>&lt;ray.tune.searc_ecb0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_ff40</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.599047</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.8904 </td><td style=\"text-align: right;\">     0.235938 </td><td style=\"text-align: right;\">          0.180606 </td><td style=\"text-align: right;\">       0.343345 </td></tr>\n",
       "<tr><td>train_model_817db7f8</td><td>TERMINATED</td><td>127.0.0.1:40647</td><td>elu            </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.181968</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0620</td><td>&lt;ray.tune.searc_0970</td><td>                      </td><td>&lt;ray.tune.searc_2950  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1c00</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.421002</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        50.1648 </td><td style=\"text-align: right;\">     0.876774 </td><td style=\"text-align: right;\">          0.953754 </td><td style=\"text-align: right;\">       0.817033 </td></tr>\n",
       "<tr><td>train_model_f04a3191</td><td>TERMINATED</td><td>127.0.0.1:40753</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.275027</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0620</td><td>&lt;ray.tune.searc_12a0</td><td>                      </td><td>&lt;ray.tune.searc_26b0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_2c20</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.433618</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        19.7306 </td><td style=\"text-align: right;\">     0.678944 </td><td style=\"text-align: right;\">          0.946507 </td><td style=\"text-align: right;\">       0.539749 </td></tr>\n",
       "<tr><td>train_model_eed02add</td><td>TERMINATED</td><td>127.0.0.1:40697</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.26332 </td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0620</td><td>&lt;ray.tune.searc_d810</td><td>                      </td><td>&lt;ray.tune.searc_d3f0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_ce80</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.411387</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.59338</td><td style=\"text-align: right;\">     0.180559 </td><td style=\"text-align: right;\">          0.953208 </td><td style=\"text-align: right;\">       0.102525 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define search space for hyperparameter optimization\n",
    "search_space = {\n",
    "    \"output_dim\": len(all_two_grams),  # Output dimension is also the number of unique 2-grams\n",
    "    \"num_layers\": tune.randint(1, 4),  # Vary the number of layers in the model\n",
    "    #\"num_layers\": tune.randint(1, 2),\n",
    "    \"hidden_layer_size\": tune.choice([128, 256, 512, 1024, 2048]),  # Different sizes for hidden layers\n",
    "    #\"hidden_layer_size\": tune.choice([1024, 2048]),  # Different sizes for hidden layers\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.4),  # Dropout rate between 0.1 and 0.4\n",
    "    \"activation_fn\": tune.choice([\"relu\", \"leaky_relu\", \"gelu\", \"elu\", \"selu\", \"tanh\"]),  # Activation functions to choose from\n",
    "    \"optimizer\": tune.choice([\n",
    "        {\"name\": \"Adam\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"AdamW\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"SGD\", \"lr\": tune.loguniform(1e-4, 1e-2), \"momentum\": tune.uniform(0.0, 0.99)},\n",
    "        {\"name\": \"RMSprop\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "    ]),\n",
    "    \"loss_fn\": tune.choice([\"BCEWithLogitsLoss\", \"MultiLabelSoftMarginLoss\", \"SoftMarginLoss\"]),\n",
    "    \"threshold\": tune.uniform(0.3, 0.8),\n",
    "    \"lr_scheduler\": tune.choice([\n",
    "        {\"name\": \"StepLR\", \"step_size\": tune.choice([5, 10, 20]), \"gamma\": tune.uniform(0.1, 0.9)},\n",
    "        {\"name\": \"ExponentialLR\", \"gamma\": tune.uniform(0.85, 0.99)},\n",
    "        {\"name\": \"ReduceLROnPlateau\", \"mode\": \"min\", \"factor\": tune.uniform(0.1, 0.5), \"patience\": tune.choice([5, 10, 15])},\n",
    "        {\"name\": \"CosineAnnealingLR\", \"T_max\": tune.loguniform(10, 50) , \"eta_min\": tune.choice([1e-5, 1e-6, 0])},\n",
    "        {\"name\": \"CyclicLR\", \"base_lr\": tune.loguniform(1e-5, 1e-3), \"max_lr\": tune.loguniform(1e-3, 1e-1), \"step_size_up\": tune.choice([2000, 4000]), \"mode_cyclic\": tune.choice([\"triangular\", \"triangular2\", \"exp_range\"]) },\n",
    "        {\"name\": \"None\"}  # No scheduler\n",
    "    ]),\n",
    "    \"batch_size\": tune.choice([8, 16, 32, 64]),  # Batch sizes to test\n",
    "}\n",
    "\n",
    "# Initialize Ray for hyperparameter optimization\n",
    "ray.init(ignore_reinit_error=True, logging_level=\"ERROR\")\n",
    "\n",
    "# Optuna Search Algorithm for optimizing the hyperparameters\n",
    "optuna_search = OptunaSearch(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"max\")\n",
    "\n",
    "# Use ASHAScheduler to manage trials and early stopping\n",
    "scheduler = ASHAScheduler(metric=\"total_val_loss\", mode=\"min\")\n",
    "\n",
    "\n",
    "\n",
    "# Define and configure the Tuner for Ray Tune\n",
    "tuner = tune.Tuner(\n",
    "    partial(train_model, data_dir=data_dir, output_dim=len(all_two_grams),alice_enc_hash=alice_enc_hash, identifier=identifier, patience=DEA_CONFIG[\"Patience\"], min_delta=DEA_CONFIG[\"MinDelta\"]),  # The function to optimize (training function)\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,  # Search strategy using Optuna\n",
    "        scheduler=scheduler,  # Use ASHA to manage the trials\n",
    "        num_samples=DEA_CONFIG[\"NumSamples\"],  # Number of trials to run\n",
    "        max_concurrent_trials=GLOBAL_CONFIG[\"Workers\"],\n",
    "    ),\n",
    "    param_space=search_space  # Pass in the defined hyperparameter search space\n",
    "\n",
    ")\n",
    "\n",
    "# Run the tuner\n",
    "results = tuner.fit()\n",
    "\n",
    "# Shut down Ray after finishing the optimization\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = results\n",
    "best_result = result_grid.get_best_result(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_hyperparameter_optimization = time.time() - start_hyperparameter_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to all_trial_results.csv\n",
      "\n",
      "🔍 Best_Result\n",
      "----------------------------------------\n",
      "Config: {'output_dim': 1036, 'num_layers': 2, 'hidden_layer_size': 2048, 'dropout_rate': 0.1819677527081685, 'activation_fn': 'elu', 'optimizer': {'name': 'AdamW', 'lr': 2.0010766161326e-05}, 'loss_fn': 'MultiLabelSoftMarginLoss', 'threshold': 0.4210017125168848, 'lr_scheduler': {'name': 'CosineAnnealingLR', 'T_max': 10.52894340497391, 'eta_min': 0}, 'batch_size': 16}\n",
      "Average Dice: 0.8768\n",
      "Average Precision: 0.9538\n",
      "Average Recall: 0.8170\n",
      "Average F1: 0.8768\n",
      "\n",
      "🔍 Worst_Result\n",
      "----------------------------------------\n",
      "Config: {'output_dim': 1036, 'num_layers': 2, 'hidden_layer_size': 256, 'dropout_rate': 0.11688479553741313, 'activation_fn': 'tanh', 'optimizer': {'name': 'SGD', 'lr': 0.00837722176045546, 'momentum': 0.09800032051895437}, 'loss_fn': 'BCEWithLogitsLoss', 'threshold': 0.7576008268959075, 'lr_scheduler': {'name': 'ReduceLROnPlateau', 'mode': 'min', 'factor': 0.45185760345859116, 'patience': 5}, 'batch_size': 8}\n",
      "Average Dice: 0.0000\n",
      "Average Precision: 0.0000\n",
      "Average Recall: 0.0000\n",
      "Average F1: 0.0000\n",
      "\n",
      "📊 Average Metrics Across All Trials\n",
      "----------------------------------------\n",
      "Average_dice: 0.2408\n",
      "Average_precision: 0.5337\n",
      "Average_recall: 0.2318\n",
      "Average_f1: 0.2408\n",
      "📊 Saved plot: metric_distributions.png\n",
      "📌 Saved heatmap: correlation_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    worst_result = result_grid.get_best_result(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"min\")\n",
    "\n",
    "    # Combine configs and metrics into a DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            **clean_result_dict(resolve_config(result.config)),\n",
    "            **{k: result.metrics.get(k) for k in [\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]},\n",
    "        }\n",
    "        for result in result_grid\n",
    "    ])\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(f\"{save_to}/hyperparameteroptimization/all_trial_results.csv\", index=False)\n",
    "    print(\"✅ Results saved to all_trial_results.csv\")\n",
    "\n",
    "    print_and_save_result(\"Best_Result\", best_result, f\"{save_to}/hyperparameteroptimization\")\n",
    "    print_and_save_result(\"Worst_Result\", worst_result, f\"{save_to}/hyperparameteroptimization\")\n",
    "\n",
    "    # Compute and print average metrics\n",
    "    print(\"\\n📊 Average Metrics Across All Trials\")\n",
    "    avg_metrics = df[[\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]].mean()\n",
    "    print(\"-\" * 40)\n",
    "    for key, value in avg_metrics.items():\n",
    "        print(f\"{key.capitalize()}: {value:.4f}\")\n",
    "\n",
    "    # --- 📈 Plotting performance metrics ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df[[\"average_dice\", \"average_recall\", \"average_f1\", \"average_precision\"]])\n",
    "    plt.title(\"Distribution of Performance Metrics Across Trials\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{save_to}/hyperparameteroptimization/metric_distributions.png\")\n",
    "    plt.close()\n",
    "    print(\"📊 Saved plot: metric_distributions.png\")\n",
    "\n",
    "    # --- 📌 Correlation between config params and performance ---\n",
    "    # Only include numeric config columns\n",
    "    exclude_cols = {\"input_dim\", \"output_dim\"}\n",
    "    numeric_config_cols = [\n",
    "        col for col in df.columns\n",
    "        if pd.api.types.is_numeric_dtype(df[col]) and col not in exclude_cols\n",
    "    ]\n",
    "    correlation_df = df[numeric_config_cols].corr()\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Between Parameters and Metrics\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_to}/hyperparameteroptimization/correlation_heatmap.png\")\n",
    "    plt.close()\n",
    "    print(\"📌 Saved heatmap: correlation_heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "\n",
    "The neural network model is selected dynamically based on the encoding technique used for Alice’s data.\n",
    "\n",
    "### Supported Models:\n",
    "\n",
    "- **BloomFilter** → `BloomFilterToTwoGramClassifier`  \n",
    "  - Input: Binary vector (Bloom filter)  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TabMinHash** → `TabMinHashToTwoGramClassifier`  \n",
    "  - Input: Tabulated MinHash signature  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TwoStepHash** → `TwoStepHashToTwoGramClassifier`  \n",
    "  - Input: Length of the unique integers present\n",
    "  - Output: 2-gram predicition\n",
    "    \n",
    "Each model outputs predictions over the set of all possible 2-grams (`all_two_grams`), and the input dimension is dynamically configured based on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_model_training = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = resolve_config(best_result.config)\n",
    "data_train, data_val, data_test = load_data(data_dir, alice_enc_hash, identifier, load_test=True)\n",
    "input_dim=data_train[0][0].shape[0]\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    data_train,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),  # Default to 32 if not specified\n",
    "    shuffle=True  # Important for training\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    data_val,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),\n",
    "    shuffle=False  # Allows variation in validation batches\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    data_test,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),\n",
    "    shuffle=False  # Allows variation in validation batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Dropout(p=0.1819677527081685, inplace=False)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (4): ELU(alpha=1.0)\n",
      "    (5): Dropout(p=0.1819677527081685, inplace=False)\n",
      "    (6): Linear(in_features=2048, out_features=1036, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=len(all_two_grams),\n",
    "            hidden_layer=best_config.get(\"hidden_layer_size\", 128),  # Default to 128 if not specified\n",
    "            num_layers=best_config.get(\"num_layers\", 2),  # Default to 2 if not specified\n",
    "            dropout_rate=best_config.get(\"dropout_rate\", 0.2),  # Default to 0.2 if not specified\n",
    "            activation_fn=best_config.get(\"activation_fn\", \"relu\")  # Default to 'relu' if not specified\n",
    "        )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Environment Setup\n",
    "This code initializes the core components needed for training a neural network model.\n",
    "\n",
    "1. TensorBoard Setup\n",
    "    - Creates unique run name by combining:\n",
    "    - Loss function type\n",
    "    - Optimizer choice\n",
    "    - Alice's algorithm\n",
    "    - Initializes TensorBoard writer in runs directory\n",
    "2. Device Configuration\n",
    "    - Automatically selects GPU if available, falls back to CPU\n",
    "    - Moves model to selected device\n",
    "3. Loss Functions\n",
    "    - `BCEWithLogitsLoss`: Binary Cross Entropy with Logits\n",
    "    - `MultiLabelSoftMarginLoss`: Multi-Label Soft Margin Loss\n",
    "4. Optimizers:\n",
    "    - `Adam`: Adaptive Moment Estimation\n",
    "    - `AdamW`: Adam with Weight Decay\n",
    "    - `SGD`: Stochastic Gradient Descent (with momentum)\n",
    "    - `RMSprop`: Root Mean Square Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    # Setup tensorboard logging\n",
    "    run_name = \"\".join([\n",
    "        best_config.get(\"loss_fn\", \"MultiLabelSoftMarginLoss\"),\n",
    "        best_config.get(\"optimizer\").get(\"name\", \"Adam\"),\n",
    "        ENC_CONFIG[\"AliceAlgo\"],\n",
    "        best_config.get(\"activation_fn\", \"relu\"),\n",
    "    ])\n",
    "    tb_writer = SummaryWriter(f\"{save_to}/{run_name}\")\n",
    "\n",
    "# Setup compute device (GPU/CPU)\n",
    "compute_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(compute_device)\n",
    "\n",
    "# Initialize loss function\n",
    "match best_config.get(\"loss_fn\", \"MultiLabelSoftMarginLoss\"):\n",
    "    case \"BCEWithLogitsLoss\":\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    case \"MultiLabelSoftMarginLoss\":\n",
    "        criterion = nn.MultiLabelSoftMarginLoss(reduction='mean')\n",
    "    case \"SoftMarginLoss\":\n",
    "        criterion = nn.SoftMarginLoss()\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported loss function: {best_config.get('loss_fn', 'MultiLabelSoftMarginLoss')}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "match best_config.get(\"optimizer\").get(\"name\", \"Adam\"):\n",
    "    case \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                            lr=best_config.get(\"optimizer\").get(\"lr\"),\n",
    "                            momentum=best_config.get(\"optimizer\").get(\"momentum\"))\n",
    "    case \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported optimizer: {best_config.get('optimizer').get('name', 'Adam')}\")\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "match best_config.get(\"lr_scheduler\").get(\"name\", \"None\"):\n",
    "    case \"StepLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=best_config.get(\"lr_scheduler\").get(\"step_size\"),\n",
    "            gamma=best_config.get(\"lr_scheduler\").get(\"gamma\")\n",
    "        )\n",
    "    case \"ExponentialLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=best_config.get(\"lr_scheduler\").get(\"gamma\")\n",
    "        )\n",
    "    case \"ReduceLROnPlateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=best_config.get(\"lr_scheduler\").get(\"mode\"),\n",
    "            factor=best_config.get(\"lr_scheduler\").get(\"factor\"),\n",
    "            patience=best_config.get(\"lr_scheduler\").get(\"patience\")\n",
    "        )\n",
    "    case \"CosineAnnealingLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=best_config.get(\"lr_scheduler\").get(\"T_max\")\n",
    "        )\n",
    "    case \"CyclicLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=best_config.get(\"lr_scheduler\").get(\"base_lr\"),\n",
    "            max_lr=best_config.get(\"lr_scheduler\").get(\"max_lr\"),\n",
    "            step_size_up=best_config.get(\"lr_scheduler\").get(\"step_size_up\"),\n",
    "            mode=best_config.get(\"lr_scheduler\").get(\"mode_cyclic\"),\n",
    "            cycle_momentum=False  # usually False for Adam/AdamW\n",
    "        )\n",
    "    case None | \"None\":\n",
    "        scheduler = None\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported LR scheduler: {best_config.get('lr_scheduler').get('name', 'None')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Early Stopping\n",
    "\n",
    "The function `train_model` orchestrates the training process for the neural network, including both training and validation phases for each epoch. It also utilizes **early stopping** to halt training when the validation loss fails to improve over multiple epochs, avoiding overfitting.\n",
    "\n",
    "### Key Phases:\n",
    "1. **Training Phase**: \n",
    "   - The model is trained on the `dataloader_train`, computing the training loss using the specified loss function (`criterion`) and optimizer. Gradients are calculated, and the model parameters are updated.\n",
    "  \n",
    "2. **Validation Phase**:\n",
    "   - The model is evaluated on the `dataloader_val` without updating weights. The validation loss is computed to track model performance on unseen data.\n",
    "\n",
    "3. **Logging**: \n",
    "   - Training and validation losses are logged to both the console and **TensorBoard** for tracking model performance during training.\n",
    "\n",
    "4. **Early Stopping**: \n",
    "   - If the validation loss does not improve after a certain number of epochs (defined by `DEA_CONFIG[\"Patience\"]`), the training process is halted to prevent overfitting.\n",
    "\n",
    "### Helper Functions:\n",
    "- `run_epoch`: Handles a single epoch, either for training or validation, depending on the flag `is_training`.\n",
    "- `log_metrics`: Logs the training and validation losses to the console and TensorBoard for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_epoch_metrics(epoch, total_epochs, train_loss, val_loss):\n",
    "    epoch_str = f\"[{epoch + 1}/{total_epochs}]\"\n",
    "    print(f\"{epoch_str} 🔧 Train Loss: {train_loss:.4f} | 🔍 Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if DEA_CONFIG.get(\"SaveResults\", False) and 'tb_writer' in globals():\n",
    "        tb_writer.add_scalar(\"Loss/train\", train_loss, epoch + 1)\n",
    "        tb_writer.add_scalar(\"Loss/validation\", val_loss, epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, device, scheduler=None):\n",
    "    num_epochs = best_config.get(\"epochs\", DEA_CONFIG[\"Epochs\"])\n",
    "    verbose = GLOBAL_CONFIG[\"Verbose\"]\n",
    "    patience = DEA_CONFIG[\"Patience\"]\n",
    "    min_delta = DEA_CONFIG[\"MinDelta\"]\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=min_delta, verbose=verbose)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- Training ----\n",
    "        model.train()\n",
    "        train_loss = run_epoch(\n",
    "            model, dataloader_train, criterion, optimizer,\n",
    "            device, is_training=True, verbose=verbose, scheduler=scheduler\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = run_epoch(\n",
    "                model, dataloader_val, criterion, optimizer,\n",
    "                device, is_training=False, verbose=verbose, scheduler=scheduler\n",
    "            )\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # ---- Scheduler step ----\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # ---- Logging ----\n",
    "        _log_epoch_metrics(epoch, num_epochs, train_loss, val_loss)\n",
    "\n",
    "        # ---- Early stopping ----\n",
    "        if early_stopper(val_loss):\n",
    "            if verbose:\n",
    "                print(f\"⏹️ Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/15] 🔧 Train Loss: 0.3276 | 🔍 Val Loss: 0.0705\n",
      "[2/15] 🔧 Train Loss: 0.0676 | 🔍 Val Loss: 0.0650\n",
      "[3/15] 🔧 Train Loss: 0.0652 | 🔍 Val Loss: 0.0641\n",
      "[4/15] 🔧 Train Loss: 0.0647 | 🔍 Val Loss: 0.0638\n",
      "[5/15] 🔧 Train Loss: 0.0642 | 🔍 Val Loss: 0.0634\n",
      "[6/15] 🔧 Train Loss: 0.0640 | 🔍 Val Loss: 0.0632\n",
      "[7/15] 🔧 Train Loss: 0.0636 | 🔍 Val Loss: 0.0629\n",
      "[8/15] 🔧 Train Loss: 0.0633 | 🔍 Val Loss: 0.0626\n",
      "[9/15] 🔧 Train Loss: 0.0629 | 🔍 Val Loss: 0.0622\n",
      "[10/15] 🔧 Train Loss: 0.0625 | 🔍 Val Loss: 0.0618\n",
      "[11/15] 🔧 Train Loss: 0.0620 | 🔍 Val Loss: 0.0614\n",
      "[12/15] 🔧 Train Loss: 0.0615 | 🔍 Val Loss: 0.0610\n",
      "[13/15] 🔧 Train Loss: 0.0608 | 🔍 Val Loss: 0.0604\n",
      "[14/15] 🔧 Train Loss: 0.0602 | 🔍 Val Loss: 0.0597\n",
      "[15/15] 🔧 Train Loss: 0.0592 | 🔍 Val Loss: 0.0589\n"
     ]
    }
   ],
   "source": [
    "model, train_losses, val_losses = train_model(\n",
    "    model, dataloader_train, dataloader_val,\n",
    "    criterion, optimizer, compute_device,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_model_training = time.time() - start_model_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Visualization over Epochs\n",
    "\n",
    "This code snippet generates a plot to visualize the **training loss** and **validation loss** across epochs. It's useful for tracking model performance during training and evaluating if overfitting is occurring (i.e., when validation loss starts increasing while training loss continues to decrease).\n",
    "\n",
    "### Key Elements:\n",
    "1. **Plotting the Losses**: \n",
    "   - The `train_losses` and `val_losses` are plotted over the epochs. \n",
    "   - The **blue line** represents the training loss, and the **red line** represents the validation loss.\n",
    "\n",
    "2. **Legend**: \n",
    "   - A legend is added to distinguish between training and validation losses.\n",
    "\n",
    "3. **Title and Labels**: \n",
    "   - The plot is titled \"Training and Validation Loss over Epochs\" for context.\n",
    "   - **X-axis** represents the epoch number, and **Y-axis** represents the loss value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation loss', color='red')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "if DEA_CONFIG.get(\"SaveResults\", False):\n",
    "    plt.savefig(f\"{save_to}/loss_curve.png\")\n",
    "    plt.close()\n",
    "    print(\"📉 Saved loss curve to loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Application to Encoded Data\n",
    "\n",
    "This code performs inference on the test data and compares the predicted 2-grams with the actual 2-grams, providing a performance evaluation based on the **Dice similarity coefficient**.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Prepare for Evaluation**:\n",
    "   - The model is switched to **evaluation mode** (`model.eval()`), ensuring no gradient computation.\n",
    "   \n",
    "2. **Thresholding**:\n",
    "   - A threshold (`DEA_CONFIG[\"FilterThreshold\"]`) is applied to filter out low-probability predictions, retaining only the most confident predictions.\n",
    "\n",
    "3. **Inference and 2-Gram Scoring**:\n",
    "   - The model is applied to the batch, and the **logits** are converted into probabilities using the **sigmoid function**.\n",
    "   - The probabilities are then mapped to **2-gram scores**, and scores below the threshold are discarded.\n",
    "\n",
    "4. **Reconstructing Words**:\n",
    "   - For each sample in the batch, **2-grams** are reconstructed into words based on the filtered scores.\n",
    "\n",
    "5. **Performance Metrics**:\n",
    "   - The actual 2-grams (from the test dataset) are compared with the predicted 2-grams, and the **Dice similarity coefficient** is calculated for each sample.\n",
    "\n",
    "### Result:\n",
    "- The code generates a list `combined_results_performance`, which contains a detailed comparison for each UID, including:\n",
    "  - **Actual 2-grams** (from the test data)\n",
    "  - **Predicted 2-grams** (from the model)\n",
    "  - **Dice similarity** score indicating how similar the actual and predicted 2-grams are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(\n",
    "    GLOBAL_CONFIG[\"LoadPath\"] if GLOBAL_CONFIG[\"LoadResults\"] else save_to,\n",
    "    \"trained_model\"\n",
    ")\n",
    "model_file   = f\"{base_path}/model.pt\"\n",
    "config_file  = f\"{base_path}/config.json\"\n",
    "result_file  = f\"{base_path}/result.json\"\n",
    "metrics_file = f\"{base_path}/metrics.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_config(model, config, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(path, \"model.pt\"))\n",
    "    with open(os.path.join(path, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    print(f\"✅ Saved model and config to {path}\")\n",
    "\n",
    "\n",
    "def load_model_and_config(model_cls, path, input_dim, output_dim):\n",
    "    with open(os.path.join(path, \"config.json\")) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    model = model_cls(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        hidden_layer=config.get(\"hidden_layer_size\", 128),\n",
    "        num_layers=config.get(\"num_layers\", 2),\n",
    "        dropout_rate=config.get(\"dropout_rate\", 0.2),\n",
    "        activation_fn=config.get(\"activation_fn\", \"relu\")\n",
    "    )\n",
    "    model.load_state_dict(torch.load(os.path.join(path, \"model.pt\")))\n",
    "    model.eval()\n",
    "    return model, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model and config to experiment_results/experiment_BloomFilter_fakename_5k_2025-07-07_16-21-34/trained_model\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    save_model_and_config(model, best_config, base_path)\n",
    "\n",
    "if GLOBAL_CONFIG[\"LoadResults\"]:\n",
    "    #TODO: how to figure out input_dim without loading dataset\n",
    "    model, best_config = load_model_and_config(BaseModel, base_path, input_dim=1024, output_dim=len(all_two_grams))\n",
    "    model.to(compute_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_application_to_encoded_data = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Test Metrics:\n",
      "  Dice:      0.1208\n",
      "  Precision: 0.9477\n",
      "  Recall:    0.0651\n",
      "  F1 Score:  0.1208\n"
     ]
    }
   ],
   "source": [
    "# Initialize metric accumulators\n",
    "total_dice = total_precision = total_recall = total_f1 = 0.0\n",
    "num_samples = 0\n",
    "results = []\n",
    "\n",
    "threshold = best_config.get(\"threshold\", 0.5)\n",
    "model.eval()\n",
    "\n",
    "# Progress bar only if verbose\n",
    "dataloader_iter = tqdm(dataloader_test, desc=\"Test loop\") if GLOBAL_CONFIG[\"Verbose\"] else dataloader_test\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels, uids in dataloader_iter:\n",
    "        data, labels = data.to(compute_device), labels.to(compute_device)\n",
    "\n",
    "        logits = model(data)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Actual and predicted 2-grams\n",
    "        actual_two_grams = decode_labels_to_two_grams(two_gram_dict, labels)\n",
    "        predicted_scores = map_probabilities_to_two_grams(two_gram_dict, probs)\n",
    "        predicted_filtered = filter_high_scoring_two_grams(predicted_scores, threshold)\n",
    "\n",
    "        # Batch metrics\n",
    "        bs = data.size(0)\n",
    "        dice, precision, recall, f1 = calculate_performance_metrics(actual_two_grams, predicted_filtered)\n",
    "\n",
    "        total_dice += dice\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        num_samples += bs\n",
    "\n",
    "        # Store per-sample predictions\n",
    "        for uid, actual, predicted in zip(uids, actual_two_grams, predicted_filtered):\n",
    "            metrics = metrics_per_entry(actual, predicted)\n",
    "            results.append({\n",
    "                \"uid\": uid,\n",
    "                \"actual_two_grams\": actual,\n",
    "                \"predicted_two_grams\": predicted,\n",
    "                \"precision\": metrics[\"precision\"],\n",
    "                \"recall\": metrics[\"recall\"],\n",
    "                \"f1\": metrics[\"f1\"],\n",
    "                \"dice\": metrics[\"dice\"],\n",
    "                \"jaccard\": metrics[\"jaccard\"]\n",
    "            })\n",
    "\n",
    "# Avoid division by zero\n",
    "if num_samples > 0:\n",
    "    avg_dice = total_dice / num_samples\n",
    "    avg_precision = total_precision / num_samples\n",
    "    avg_recall = total_recall / num_samples\n",
    "    avg_f1 = total_f1 / num_samples\n",
    "else:\n",
    "    avg_dice = avg_precision = avg_recall = avg_f1 = 0.0\n",
    "\n",
    "# Logging\n",
    "print(f\"\\n📊 Final Test Metrics:\")\n",
    "print(f\"  Dice:      {avg_dice:.4f}\")\n",
    "print(f\"  Precision: {avg_precision:.4f}\")\n",
    "print(f\"  Recall:    {avg_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {avg_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_application_to_encoded_data = time.time() - start_application_to_encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE - Metrics and Result\n",
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        f.write(f\"Average Precision: {avg_precision:.4f}\\n\")\n",
    "        f.write(f\"Average Recall: {avg_recall:.4f}\\n\")\n",
    "        f.write(f\"Average F1 Score: {avg_f1:.4f}\\n\")\n",
    "        f.write(f\"Average Dice Similarity: {avg_dice:.4f}\\n\")\n",
    "\n",
    "    with open(result_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD - Result\n",
    "if GLOBAL_CONFIG[\"LoadResults\"]:\n",
    "    with open(result_file, 'r', encoding='utf-8') as f:\n",
    "        result = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance for Re-Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Sample Reconstructions (first 5)\n",
      "UID: 96557\n",
      "  Actual 2-grams:    ['az', 'di', 'ia', 'll', 'ly', 'mo', 'ol', 'yd', 'z6', '19', '61', '88', '91', '98']\n",
      "  Predicted 2-grams: ['19']\n",
      "------------------------------------------------------------\n",
      "UID: 27434\n",
      "  Actual 2-grams:    ['el', 'es', 'gr', 'gu', 'ig', 'im', 'lg', 'me', 'mi', 'ri', 'ue', 's9', '13', '19', '31', '91', '95', '99']\n",
      "  Predicted 2-grams: ['19']\n",
      "------------------------------------------------------------\n",
      "UID: 33465\n",
      "  Actual 2-grams:    ['at', 'ci', 'da', 'er', 'in', 'le', 'lu', 'nd', 'ty', 'uc', 'yl', 'r4', '00', '03', '20', '46', '62']\n",
      "  Predicted 2-grams: ['19', 'er']\n",
      "------------------------------------------------------------\n",
      "UID: 99822\n",
      "  Actual 2-grams:    ['as', 'ba', 'ei', 'gh', 'hb', 'ig', 'le', 'ss', 's1', '17', '19', '71', '97', '99']\n",
      "  Predicted 2-grams: ['19']\n",
      "------------------------------------------------------------\n",
      "UID: 11254\n",
      "  Actual 2-grams:    ['ad', 'ap', 'da', 'dr', 'id', 'lo', 'oi', 'pa', 'ro', 'o8', '13', '19', '31', '81', '91', '99']\n",
      "  Predicted 2-grams: ['19']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.json_normalize(results) # ≈2× faster than DataFrame(list)\n",
    "\n",
    "metric_cols = [\"precision\", \"recall\", \"f1\", \"dice\", \"jaccard\"]        # keys created earlier\n",
    "melted = results_df.melt(value_vars=metric_cols,\n",
    "                         var_name=\"metric\",\n",
    "                         value_name=\"score\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=melted,\n",
    "             x=\"score\",\n",
    "             hue=\"metric\",\n",
    "             bins=20,\n",
    "             element=\"step\",\n",
    "             fill=False,\n",
    "             kde=True,\n",
    "             palette=\"Set2\")\n",
    "plt.title(\"Distribution of Precision / Recall / F1 across Samples\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "if DEA_CONFIG.get(\"SaveResults\", False):\n",
    "    plt.savefig(f\"{save_to}/metric_distributions.png\")\n",
    "    print(\"📊  Saved plot: metric_distributions.png\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n🔍 Sample Reconstructions (first 5)\")\n",
    "for _, row in results_df.iloc[:5].iterrows():\n",
    "    print(f\"UID: {row.uid}\")\n",
    "    print(f\"  Actual 2-grams:    {row.actual_two_grams}\")\n",
    "    print(f\"  Predicted 2-grams: {row.predicted_two_grams}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Refinement and Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_refinement_and_reconstruction = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Reidentification Analysis:\n",
      "Total Reidentified Individuals: 0\n",
      "Total Not Reidentified Individuals: 2000\n",
      "Reidentification Rate: 0.00%\n",
      "\n",
      "🔄 Reconstructing results using fuzzy matching (entry-wise, parallelized)...\n",
      "\n",
      "🔍 Reidentification Analysis:\n",
      "Total Reidentified Individuals: 0\n",
      "Total Not Reidentified Individuals: 2000\n",
      "Reidentification Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_not_reidentified_df(data_dir: str, identifier: str) -> pd.DataFrame:\n",
    "    df = load_not_reidentified_data(data_dir, alice_enc_hash, identifier)\n",
    "    return lowercase_df(df)\n",
    "\n",
    "def create_identifier(df: pd.DataFrame, comps):\n",
    "    df = df.copy()\n",
    "    df[\"identifier\"] = create_identifier_column_dynamic(df, comps)\n",
    "    return df[[\"uid\", \"identifier\"]]\n",
    "\n",
    "def run_reidentification_once(reconstructed, df_not_reidentified, merge_cols, technique, identifier_components=None):\n",
    "\n",
    "    df_reconstructed = lowercase_df(pd.DataFrame(reconstructed, columns=merge_cols))\n",
    "\n",
    "    if(identifier_components):\n",
    "        df_not_reidentified = create_identifier(df_not_reidentified, identifier_components)\n",
    "\n",
    "    return reidentification_analysis(\n",
    "        df_reconstructed,\n",
    "        df_not_reidentified,\n",
    "        merge_cols,\n",
    "        len(df_not_reidentified),\n",
    "        technique,\n",
    "        save_path=f\"{save_to}/re_identification_results\"\n",
    "    )\n",
    "\n",
    "header = read_header(GLOBAL_CONFIG[\"Data\"])\n",
    "\n",
    "TECHNIQUES = {\n",
    "    \"ai\": {\n",
    "        \"fn\": reconstruct_identities_with_llm,\n",
    "        \"merge_cols\": header[:3] + [header[-1]],\n",
    "        \"identifier_comps\": None,\n",
    "    },\n",
    "    \"greedy\": {\n",
    "        \"fn\": greedy_reconstruction,\n",
    "        \"merge_cols\": [\"uid\", \"identifier\"],\n",
    "        \"identifier_comps\": header[:-1],\n",
    "    },\n",
    "    \"fuzzy\": {\n",
    "        \"fn\": fuzzy_reconstruction_approach,\n",
    "        \"merge_cols\": header[:3] + [header[-1]],\n",
    "        \"identifier_comps\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "selected = DEA_CONFIG[\"MatchingTechnique\"]\n",
    "df_not_reid_cached = get_not_reidentified_df(data_dir, identifier)\n",
    "save_dir = f\"{save_to}/re_identification_results\"\n",
    "\n",
    "if selected == \"fuzzy_and_greedy\":\n",
    "    reidentified = {}\n",
    "    for name in (\"greedy\", \"fuzzy\"):\n",
    "        info = TECHNIQUES[name]\n",
    "        if name == \"fuzzy\":\n",
    "            recon = info[\"fn\"](results, GLOBAL_CONFIG[\"Workers\"])\n",
    "        else:\n",
    "            recon = info[\"fn\"](results)\n",
    "        reidentified[name] = run_reidentification_once(\n",
    "            recon,\n",
    "            df_not_reid_cached,\n",
    "            info[\"merge_cols\"],\n",
    "            name,\n",
    "            info[\"identifier_comps\"],\n",
    "        )\n",
    "else:\n",
    "    # single technique path\n",
    "    print(selected)\n",
    "    print(TECHNIQUES[selected])\n",
    "    if selected not in TECHNIQUES:\n",
    "        raise ValueError(f\"Unsupported matching technique: {selected}\")\n",
    "    info = TECHNIQUES[selected]\n",
    "    if selected == \"fuzzy\":\n",
    "        recon = info[\"fn\"](results, GLOBAL_CONFIG[\"Workers\"])\n",
    "    if selected == \"ai\":\n",
    "        recon = info[\"fn\"](results, info[\"merge_cols\"][:-1])\n",
    "    else:\n",
    "        recon = info[\"fn\"](results)\n",
    "    reidentified = run_reidentification_once(\n",
    "        recon,\n",
    "        df_not_reid_cached,\n",
    "        info[\"merge_cols\"],\n",
    "        selected,\n",
    "        info[\"identifier_comps\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Combined Reidentification (greedy ∪ fuzzy):\n",
      "Total not re-identified individuals: 2000\n",
      "Total Unique Reidentified Individuals: 0\n",
      "Combined Reidentification Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "if selected == \"fuzzy_and_greedy\":\n",
    "    # Extract UIDs from both methods\n",
    "    uids_greedy = set(reidentified[\"greedy\"][\"uid\"])\n",
    "    uids_fuzzy = set(reidentified[\"fuzzy\"][\"uid\"])\n",
    "\n",
    "    # Combine them\n",
    "    combined_uids = uids_greedy.union(uids_fuzzy)\n",
    "    total_reidentified_combined = len(combined_uids)\n",
    "\n",
    "    # Get not re-identified count\n",
    "    df_not_reid_cached = get_not_reidentified_df(data_dir, identifier)\n",
    "    len_not_reidentified = len(df_not_reid_cached)\n",
    "\n",
    "    # Compute rate\n",
    "    reidentification_rate_combined = (total_reidentified_combined / len_not_reidentified) * 100\n",
    "\n",
    "    # Print\n",
    "    print(\"\\n🔁 Combined Reidentification (greedy ∪ fuzzy):\")\n",
    "    print(f\"Total not re-identified individuals: {len_not_reidentified}\")\n",
    "    print(f\"Total Unique Reidentified Individuals: {total_reidentified_combined}\")\n",
    "    print(f\"Combined Reidentification Rate: {reidentification_rate_combined:.2f}%\")\n",
    "\n",
    "    # Save UIDs to CSV\n",
    "    save_dir = os.path.join(save_to, \"re_identification_results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame({\"uid\": list(combined_uids)}).to_csv(\n",
    "        os.path.join(save_dir, \"result_fuzzy_and_greedy.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Save summary to TXT\n",
    "    summary_path = os.path.join(save_dir, \"summary_fuzzy_and_greedy.txt\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        f.write(\"Reidentification Method: fuzzy_and_greedy\\n\")\n",
    "        f.write(f\"Total not re-identified individuals: {len_not_reidentified}\\n\")\n",
    "        f.write(f\"Total Unique Reidentified Individuals: {total_reidentified_combined}\\n\")\n",
    "        f.write(f\"Combined Reidentification Rate: {reidentification_rate_combined:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_refinement_and_reconstruction = time.time() - start_refinement_and_reconstruction\n",
    "    elapsed_total = time.time() - start_total\n",
    "    save_dea_runtime_log(\n",
    "        elapsed_gma=elapsed_gma,\n",
    "        elapsed_hyperparameter_optimization=elapsed_hyperparameter_optimization,\n",
    "        elapsed_model_training=elapsed_model_training,\n",
    "        elapsed_application_to_encoded_data=elapsed_application_to_encoded_data,\n",
    "        elapsed_refinement_and_reconstruction=elapsed_refinement_and_reconstruction,\n",
    "        elapsed_total=elapsed_total,\n",
    "        output_dir=save_to\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
