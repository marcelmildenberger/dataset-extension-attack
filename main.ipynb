{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL): Investigating Dataset Extension Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Imports\n",
    "\n",
    "#### PyTorch\n",
    "- `torch`, `torch.nn`, `torch.optim`, `DataLoader`: For building, training, and evaluating neural networks for DEA.\n",
    "- `SummaryWriter`: Logs training metrics for visualization in TensorBoard.\n",
    "\n",
    "#### Ray\n",
    "- `tune`, `air`, `train`, `OptunaSearch`, `ASHAScheduler`: Used for distributed hyperparameter tuning and model optimization.\n",
    "\n",
    "#### Data Handling & Visualization\n",
    "- `pandas`, `numpy`, `matplotlib.pyplot`, `seaborn`: For data manipulation, analysis, and plotting.\n",
    "- `hickle`: Efficient binary serialization format for NumPy arrays and Python objects.\n",
    "- `tqdm.notebook`: Progress bars for loops, especially in Jupyter notebooks.\n",
    "\n",
    "#### Custom Modules\n",
    "- `utils`: A comprehensive set of utility functions for DEA-specific tasks like reconstruction and result logging.\n",
    "- `datasets`: Dataset wrappers for different encoding schemes (Bloom Filter, Tab MinHash, Two-Step Hash).\n",
    "- `pytorch_models`, `early_stopping`: Custom PyTorch model definitions and early stopping mechanism.\n",
    "- `graphMatching.gma`: Executes Graph Matching Attack (GMA) to prepare DEA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ray libraries for hyperparameter tuning and parallelism\n",
    "import ray\n",
    "from ray import air, train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "# Custom utilities and logic\n",
    "from early_stopping.early_stopping import EarlyStopping\n",
    "from graphMatching.gma import run_gma\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset import TwoStepHashDataset\n",
    "from pytorch_models.base_model import BaseModel\n",
    "from pytorch_models_hyperparameter_optimization.base_model_hyperparameter_optimization import BaseModelHyperparameterOptimization\n",
    "from utils import (\n",
    "    calculate_performance_metrics,\n",
    "    clean_result_dict,\n",
    "    create_identifier_column_dynamic,\n",
    "    decode_labels_to_two_grams,\n",
    "    filter_high_scoring_two_grams,\n",
    "    fuzzy_reconstruction_approach,\n",
    "    get_hashes,\n",
    "    greedy_reconstruction,\n",
    "    load_dataframe,\n",
    "    lowercase_df,\n",
    "    map_probabilities_to_two_grams,\n",
    "    metrics_per_entry,\n",
    "    print_and_save_result,\n",
    "    read_header,\n",
    "    reconstruct_identities_with_llm,\n",
    "    reidentification_analysis,\n",
    "    resolve_config,\n",
    "    run_epoch,\n",
    "    save_dea_runtime_log,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Overview\n",
    "\n",
    "#### GLOBAL_CONFIG\n",
    "General control parameters for DEA runs.\n",
    "- `Data`: Path to dataset (e.g., fakename_5k.tsv).\n",
    "- `Overlap`: Proportion of shared entities between Alice and Eve.\n",
    "- `DropFrom`: Which party (Alice, Eve or both) gets the non-overlapping entities removed.\n",
    "- `MatchingMetric`, `Matching`: Used in GMA (e.g., cosine similarity, MinWeight matching).\n",
    "- `Workers`: Number of parallel threads (-1 = all available).\n",
    "- `BenchMode`, `DevMode`: Toggle benchmarking or development behaviors.\n",
    "\n",
    "#### DEA_CONFIG\n",
    "Training configuration for the neural network used in the Dataset Extension Attack.\n",
    "- `TrainSize`, `Epochs`, `Patience`: Classic train/test split and early stopping.\n",
    "- `NumSamples`: Number of tuning samples for Ray.\n",
    "- `MetricToOptimize`: Metric guiding model selection (e.g., `average_dice`).\n",
    "- `MatchingTechnique`: Post-processing method (e.g.,`fuzzy`, `greedy`, `ai`, `fuzzy_and_greedy`).\n",
    "\n",
    "#### ENC_CONFIG\n",
    "Controls how both Alice’s and Eve’s data are encoded.\n",
    "- `AliceAlgo`, `EveAlgo`: Chosen encoding methods (BloomFilter, TabMinHash, TwoStepHash).\n",
    "- Parameters are grouped by technique (BF, TMH, 2SH), but all are present to allow switching.\n",
    "\n",
    "#### EMB_CONFIG\n",
    "Defines embedding model (e.g., Node2Vec) parameters for both parties.\n",
    "- `Dim`, `Context`, `Negative`: Node2Vec embedding dimensions and context window.\n",
    "- `WalkLen`, `NWalks`, `P`, `Q`: Random walk hyperparameters.\n",
    "- `Quantile`, `Discretize`, `Normalize`: Post-embedding processing.\n",
    "\n",
    "#### ALIGN_CONFIG\n",
    "Parameters for alignment-based reconstruction.\n",
    "- `RegWS`, `RegInit`: Regularization weights.\n",
    "- `NIterWS`, `NIterInit`, `NEpochWS`: Iteration limits.\n",
    "- `Wasserstein`: Use Wasserstein-based alignment loss.\n",
    "- `EarlyStopping`, `LRDecay`: Learning stability controls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General Parameters ===\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/fakename_1k.tsv\",\n",
    "    \"Overlap\": 0.9,\n",
    "    \"DropFrom\": \"Both\",\n",
    "    \"Verbose\": False,\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": os.cpu_count() - 1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": False,\n",
    "    \"BenchMode\": True,\n",
    "    \"LoadResults\": False,\n",
    "    \"LoadPath\": \"\",\n",
    "    \"SaveResults\": True,\n",
    "}\n",
    "\n",
    "# === DEA Training Parameters ===\n",
    "DEA_CONFIG = {\n",
    "    \"TrainSize\": 0.8,\n",
    "    \"Patience\": 5,\n",
    "    \"MinDelta\": 1e-4,\n",
    "    \"NumSamples\": 25,\n",
    "    \"Epochs\": 15,\n",
    "    \"MetricToOptimize\": \"average_dice\",  # Options: \"average_dice\", \"average_precision\", ...\n",
    "    # Fuzyy works only if first three columns resemble: givenname, surname, birthdate (dd/mm/yyyy) format (naming of columns is irellevant)\n",
    "    \"MatchingTechnique\": \"fuzzy_and_greedy\",  # Options: \"ai\", \"greedy\", \"fuzzy\", ...\n",
    "}\n",
    "\n",
    "# === Encoding Parameters for Alice & Eve ===\n",
    "ENC_CONFIG = {\n",
    "    # Encoding technique\n",
    "    \"AliceAlgo\": \"BloomFilter\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"BloomFilter\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "\n",
    "    # Bloom Filter specific\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "\n",
    "    # Tabulation MinHash specific\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "\n",
    "    # Two-Step Hashing specific\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "# === Embedding Configuration (e.g., Node2Vec) ===\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42,\n",
    "}\n",
    "\n",
    "# === Graph Alignment Config ===\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"] / 2),\n",
    "    \"RegInit\": 1,\n",
    "    \"Batchsize\": 1,\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5,\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define character sets\n",
    "alphabet = string.ascii_lowercase  # 'a' to 'z'\n",
    "digits = string.digits             # '0' to '9'\n",
    "\n",
    "# Generate 2-grams\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]   # 'aa' to 'zz'\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]     # '00' to '99'\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]      # 'a0' to 'z9'\n",
    "\n",
    "# Combine all 2-gram types\n",
    "all_two_grams = letter_letter_grams + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Map index to 2-gram string\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load or Compute Graph Matching Attack (GMA) Results\n",
    "\n",
    "This step ensures GMA results are available by either loading existing output files or triggering a new attack run.\n",
    "\n",
    "1. **Start Benchmark Timing (Optional):**  \n",
    "   If benchmarking is enabled (`BenchMode=True`), timers are started to measure runtime for the GMA computation.\n",
    "\n",
    "2. **Generate Configuration Hashes:**  \n",
    "   The function `get_hashes()` generates unique identifiers (hashes) based on the encoding (`ENC_CONFIG`) and embedding (`EMB_CONFIG`) settings for both Alice and Eve.  \n",
    "   These hashes are concatenated into a unique string identifier to distinguish different experiment setups.\n",
    "\n",
    "3. **Construct File Paths:**  \n",
    "   The identifier is used to define expected file paths for:\n",
    "   - `reidentified_individuals.h5` — records successfully linked between Eve and Alice  \n",
    "   - `not_reidentified_individuals.h5` — records not matched  \n",
    "   - `alice_data_complete_with_encoding.h5` — full encoded data for Alice  \n",
    "\n",
    "4. **Check for Existing Results:**  \n",
    "   If all three expected files exist, the GMA step is skipped. This avoids redundant computation.\n",
    "\n",
    "5. **Run GMA if Needed:**  \n",
    "   If any file is missing, the Graph Matching Attack is executed using `run_gma()`, which writes the results to disk using the same naming convention based on the identifier.\n",
    "\n",
    "6. **End Benchmark Timing (Optional):**  \n",
    "   If benchmarking is active, the elapsed time for the GMA step is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 20 of 818\n",
      "Success rate: 0.024450\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_total = time.time()\n",
    "    start_gma = time.time()\n",
    "\n",
    "# Get absolute path to data directory\n",
    "data_dir = os.path.abspath(\"./data\")\n",
    "\n",
    "# Generate encoding and embedding hashes for reproducible identifiers\n",
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(\n",
    "    GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG\n",
    ")\n",
    "identifier = f\"{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}\"\n",
    "\n",
    "# Build file paths for reidentified, not reidentified, and full encoded data\n",
    "path_reidentified = f\"{data_dir}/available_to_eve/reidentified_individuals_{identifier}.h5\"\n",
    "path_not_reidentified = f\"{data_dir}/available_to_eve/not_reidentified_individuals_{identifier}.h5\"\n",
    "path_all = f\"{data_dir}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\"\n",
    "\n",
    "# Run GMA only if the expected output files do not yet exist\n",
    "if not (\n",
    "    os.path.isfile(path_reidentified)\n",
    "    and os.path.isfile(path_not_reidentified)\n",
    "    and os.path.isfile(path_all)\n",
    "):\n",
    "    run_gma(\n",
    "        GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG,\n",
    "        eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash\n",
    "    )\n",
    "\n",
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_gma = time.time() - start_gma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Training, Validation, and Test Data\n",
    "\n",
    "This step prepares the datasets needed for training and evaluation of the Dataset Extension Attack (DEA).  \n",
    "Depending on the selected encoding algorithm and available data, different dataset loaders are used.\n",
    "\n",
    "1. **Start Benchmark Timing (Optional):**  \n",
    "   If `BenchMode` is enabled, a timer is started to track the time spent in data preparation.\n",
    "\n",
    "2. **Load Reidentified and Not Reidentified Data:**  \n",
    "   - `reidentified_individuals_*.h5`: Contains entities successfully linked by the Graph Matching Attack.\n",
    "   - `not_reidentified_individuals_*.h5` and `alice_data_complete_with_encoding_*.h5`:  \n",
    "     Used to construct a labeled test set of entities not reidentified by the GMA.\n",
    "\n",
    "3. **Select Dataset Type Based on Encoding:**\n",
    "   - If `AliceAlgo` is set to `\"BloomFilter\"`: `BloomFilterDataset` is used.\n",
    "   - If `TabMinHash`: `TabMinHashDataset` is used.\n",
    "   - If `TwoStepHash`: `TwoStepHashDataset` is used with integer feature vectors.\n",
    "\n",
    "4. **Split Labeled Data into Train and Validation Sets:**  \n",
    "   The reidentified dataset is split according to `DEA_CONFIG[\"TrainSize\"]`.\n",
    "\n",
    "5. **Return Datasets:**  \n",
    "   The function returns:\n",
    "   - `data_train`: for model training  \n",
    "   - `data_val`: for validation during training  \n",
    "   - `data_test` (optional): for evaluating the reconstruction on not reidentified entities\n",
    "\n",
    "6. **Drop Redundant Columns (if needed):**  \n",
    "   The function `load_not_reidentified_data()` ensures compatibility of test data by removing unnecessary columns.\n",
    "\n",
    "7. **End Benchmark Timing (Optional):**  \n",
    "   If benchmarking is active, the elapsed time for data preparation is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_path(data_directory, identifier, alice_enc_hash, name=\"dataset\"):\n",
    "    os.makedirs(f\"{data_directory}/cache\", exist_ok=True)\n",
    "    return os.path.join(data_directory, \"cache\", f\"{name}_{identifier}_{alice_enc_hash}.pkl\")\n",
    "\n",
    "def load_data(data_directory, alice_enc_hash, identifier, load_test=False):\n",
    "    cache_path = get_cache_path(data_directory, identifier, alice_enc_hash)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            data_train, data_val, data_test = pickle.load(f)\n",
    "        return data_train, data_val, data_test\n",
    "\n",
    "    # Load from raw files\n",
    "    df_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/reidentified_individuals_{identifier}.h5\")\n",
    "\n",
    "    df_test = None\n",
    "    if load_test:\n",
    "        df_not_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/not_reidentified_individuals_{identifier}.h5\")\n",
    "        df_all = load_dataframe(f\"{data_directory}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\")\n",
    "        df_test = df_all[df_all[\"uid\"].isin(df_not_reidentified[\"uid\"])].reset_index(drop=True)\n",
    "\n",
    "    def get_encoding_dataset_class():\n",
    "        algo = ENC_CONFIG[\"AliceAlgo\"]\n",
    "        if algo == \"BloomFilter\":\n",
    "            return BloomFilterDataset\n",
    "        elif algo == \"TabMinHash\":\n",
    "            return TabMinHashDataset\n",
    "        elif algo == \"TwoStepHash\":\n",
    "            return TwoStepHashDataset\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoding algorithm: {algo}\")\n",
    "\n",
    "    DatasetClass = get_encoding_dataset_class()\n",
    "\n",
    "    if ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\":\n",
    "        unique_ints = sorted(set().union(*df_reidentified[\"twostephash\"]))\n",
    "        dataset_args = {\"all_integers\": unique_ints}\n",
    "    else:\n",
    "        dataset_args = {}\n",
    "\n",
    "    common_args = {\n",
    "        \"is_labeled\": True,\n",
    "        \"all_two_grams\": all_two_grams,\n",
    "        \"dev_mode\": GLOBAL_CONFIG[\"DevMode\"]\n",
    "    }\n",
    "\n",
    "    data_labeled = DatasetClass(df_reidentified, **common_args, **dataset_args)\n",
    "    data_test = DatasetClass(df_test, **common_args, **dataset_args) if load_test else None\n",
    "\n",
    "    train_size = int(DEA_CONFIG[\"TrainSize\"] * len(data_labeled))\n",
    "    val_size = len(data_labeled) - train_size\n",
    "    data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "\n",
    "    # Save to cache\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump((data_train, data_val, data_test), f)\n",
    "    return data_train, data_val, data_test\n",
    "\n",
    "\n",
    "def load_not_reidentified_data(data_directory, alice_enc_hash, identifier):\n",
    "        cache_path = get_cache_path(data_directory, identifier, alice_enc_hash, name=\"not_reidentified\")\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                df_filtered = pickle.load(f)\n",
    "            return df_filtered\n",
    "\n",
    "        df_not_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/not_reidentified_individuals_{identifier}.h5\")\n",
    "        df_all = load_dataframe(f\"{data_directory}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\")\n",
    "\n",
    "        df_filtered = df_all[df_all[\"uid\"].isin(df_not_reidentified[\"uid\"])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        # Drop column by name instead of position if possible\n",
    "        drop_col = df_filtered.columns[-2]\n",
    "        df_filtered = df_filtered.drop(columns=[drop_col])\n",
    "\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(df_filtered, f)\n",
    "\n",
    "        return df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = GLOBAL_CONFIG[\"Data\"].split(\"/\")[-1].replace(\".tsv\", \"\")\n",
    "experiment_tag = \"experiment_\" + ENC_CONFIG[\"AliceAlgo\"] + \"_\" + selected_dataset + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_to = f\"experiment_results/{experiment_tag}\"\n",
    "os.makedirs(save_to, exist_ok=True)\n",
    "\n",
    "# Combine all configs into one dictionary\n",
    "all_configs = {\n",
    "    \"GLOBAL_CONFIG\": GLOBAL_CONFIG,\n",
    "    \"DEA_CONFIG\": DEA_CONFIG,\n",
    "    \"ENC_CONFIG\": ENC_CONFIG,\n",
    "    \"EMB_CONFIG\": EMB_CONFIG,\n",
    "    \"ALIGN_CONFIG\": ALIGN_CONFIG\n",
    "}\n",
    "\n",
    "# Save as a readable .txt file\n",
    "with open(os.path.join(save_to, \"config.txt\"), \"w\") as f:\n",
    "    for config_name, config_dict in all_configs.items():\n",
    "        f.write(f\"# === {config_name} ===\\n\")\n",
    "        f.write(json.dumps(config_dict, indent=4))\n",
    "        f.write(\"\\n\\n\")\n",
    "os.makedirs(f\"{save_to}/hyperparameteroptimization\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val, data_test = load_data(data_dir, alice_enc_hash, identifier, load_test=True)\n",
    "df_not_reidentified = load_not_reidentified_data(data_dir, alice_enc_hash, identifier)\n",
    "# Exit the function if any of the data frames are empty\n",
    "if len(data_train) == 0 or len(data_val) == 0 or len(data_test) == 0 or df_not_reidentified.empty:\n",
    "    log_path = os.path.join(save_to, \"termination_log.txt\")\n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(\"Training process canceled due to empty dataset.\\n\")\n",
    "        f.write(f\"Length of data_train: {len(data_train)}\\n\")\n",
    "        f.write(f\"Length of data_val: {len(data_val)}\\n\")\n",
    "        f.write(f\"Length of data_test: {len(data_test)}\\n\")\n",
    "        f.write(f\"Length of df_not_reidentified: {len(df_not_reidentified)}\\n\")\n",
    "    print(\"One or more datasets are empty. Termination log written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_hyperparameter_optimization = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, data_dir, output_dim, alice_enc_hash, identifier, patience, min_delta):\n",
    "    # Create DataLoaders for training, validation, and testing\n",
    "\n",
    "    data_train, data_val, _ = load_data(data_dir, alice_enc_hash, identifier, load_test=False)\n",
    "\n",
    "    input_dim = data_train[0][0].shape[0]  # Get the input dimension from the first sample\n",
    "\n",
    "    dataloader_train = DataLoader(\n",
    "        data_train,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,  # Important for training\n",
    "    )\n",
    "\n",
    "    dataloader_val = DataLoader(\n",
    "        data_val,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    total_precision = total_recall = total_f1 = total_dice = total_val_loss = 0.0\n",
    "    num_samples = 0\n",
    "    epochs = 0\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "\n",
    "    # Define and initialize model with hyperparameters from config\n",
    "    model = BaseModelHyperparameterOptimization(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        activation_fn=config[\"activation_fn\"]\n",
    "    )\n",
    "\n",
    "    # Set device for model (GPU or CPU)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Select loss function based on config\n",
    "    loss_functions = {\n",
    "        \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss(),\n",
    "        \"MultiLabelSoftMarginLoss\": nn.MultiLabelSoftMarginLoss(),\n",
    "        \"SoftMarginLoss\": nn.SoftMarginLoss(),\n",
    "    }\n",
    "    criterion = loss_functions[config[\"loss_fn\"]]\n",
    "\n",
    "    learning_rate = config[\"optimizer\"][\"lr\"].sample()\n",
    "    # Select optimizer based on config\n",
    "    optimizers = {\n",
    "        \"Adam\": lambda: optim.Adam(model.parameters(), lr=learning_rate),\n",
    "        \"AdamW\": lambda: optim.AdamW(model.parameters(), lr=learning_rate),\n",
    "        \"SGD\": lambda: optim.SGD(model.parameters(), lr=learning_rate, momentum=config[\"optimizer\"][\"momentum\"].sample()),\n",
    "        \"RMSprop\": lambda: optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    }\n",
    "    optimizer = optimizers[config[\"optimizer\"][\"name\"]]()\n",
    "\n",
    "    schedulers = {\n",
    "        \"StepLR\": lambda: torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config[\"lr_scheduler\"][\"step_size\"].sample(),\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ExponentialLR\": lambda: torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ReduceLROnPlateau\": lambda: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=config[\"lr_scheduler\"][\"mode\"],\n",
    "            factor=config[\"lr_scheduler\"][\"factor\"].sample(),\n",
    "            patience=config[\"lr_scheduler\"][\"patience\"].sample()\n",
    "        ),\n",
    "        \"CosineAnnealingLR\": lambda: torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config[\"lr_scheduler\"][\"T_max\"].sample(),\n",
    "            eta_min=config[\"lr_scheduler\"][\"eta_min\"].sample()\n",
    "        ),\n",
    "        \"CyclicLR\": lambda: torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=config[\"lr_scheduler\"][\"base_lr\"].sample(),\n",
    "            max_lr=config[\"lr_scheduler\"][\"max_lr\"].sample(),\n",
    "            step_size_up=config[\"lr_scheduler\"][\"step_size_up\"].sample(),\n",
    "            mode=config[\"lr_scheduler\"][\"mode_cyclic\"].sample(),\n",
    "            cycle_momentum=False\n",
    "        ),\n",
    "        \"None\": lambda: None,\n",
    "    }\n",
    "    scheduler = schedulers[config[\"lr_scheduler\"][\"name\"]]()\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(DEA_CONFIG[\"Epochs\"]):\n",
    "        epochs += 1\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = run_epoch(model, dataloader_train, criterion, optimizer, device, is_training=True, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(model, dataloader_val, criterion, optimizer, device, is_training=False, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        total_val_loss += val_loss\n",
    "\n",
    "         # Early stopping check\n",
    "        if early_stopper(val_loss):\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    # Test phase with reconstruction and evaluation\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in dataloader_val:\n",
    "\n",
    "            actual_two_grams = decode_labels_to_two_grams(two_gram_dict, labels)\n",
    "\n",
    "            # Move data to device and make predictions\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities into 2-gram scores\n",
    "            batch_two_gram_scores = map_probabilities_to_two_grams(two_gram_dict, probabilities)\n",
    "\n",
    "            # Filter out low-scoring 2-grams\n",
    "            batch_filtered_two_gram_scores = filter_high_scoring_two_grams(batch_two_gram_scores, config[\"threshold\"])\n",
    "\n",
    "            # Calculate performance metrics for evaluation\n",
    "            dice, precision, recall, f1 = calculate_performance_metrics(\n",
    "                actual_two_grams, batch_filtered_two_gram_scores)\n",
    "\n",
    "            total_dice += dice\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "            num_samples += data.size(0) # Batch Size\n",
    "\n",
    "    train.report({\n",
    "            \"average_dice\": total_dice / num_samples,\n",
    "            \"average_precision\": total_precision / num_samples,\n",
    "            \"average_recall\": total_recall / num_samples,\n",
    "            \"average_f1\": total_f1 / num_samples,\n",
    "            \"total_val_loss\": total_val_loss,\n",
    "            \"len_train\": len(dataloader_train.dataset),\n",
    "            \"len_val\": len(dataloader_val.dataset),\n",
    "            \"epochs\": epochs\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-07-07 16:40:23</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:18.94        </td></tr>\n",
       "<tr><td>Memory:      </td><td>19.3/32.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=19<br>Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: -4.158035635948181<br>Logical resource usage: 4.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th>activation_fn  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_layer_size</th><th>loss_fn             </th><th>lr_scheduler/T_max  </th><th>lr_scheduler/base_lr  </th><th>lr_scheduler/eta_min  </th><th>lr_scheduler/factor  </th><th>lr_scheduler/gamma  </th><th>lr_scheduler/max_lr  </th><th>lr_scheduler/mode  </th><th>lr_scheduler/mode_cy\n",
       "clic                     </th><th>lr_scheduler/name  </th><th>lr_scheduler/patienc\n",
       "e                     </th><th>lr_scheduler/step_si\n",
       "ze                     </th><th>lr_scheduler/step_si\n",
       "ze_up                     </th><th style=\"text-align: right;\">  num_layers</th><th>optimizer/lr        </th><th>optimizer/momentum  </th><th>optimizer/name  </th><th style=\"text-align: right;\">  threshold</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  average_dice</th><th style=\"text-align: right;\">  average_precision</th><th style=\"text-align: right;\">  average_recall</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_16316b29</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>gelu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.261029</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0620</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_39d0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_3be0</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_1570</td><td>&lt;ray.tune.searc_1540</td><td>SGD             </td><td style=\"text-align: right;\">   0.635295</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       1.72221  </td><td style=\"text-align: right;\">      0       </td><td style=\"text-align: right;\">         0         </td><td style=\"text-align: right;\">       0        </td></tr>\n",
       "<tr><td>train_model_6de2b867</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.38324 </td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_1ba0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_0910</td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_3700</td><td>&lt;ray.tune.searc_2b60</td><td>SGD             </td><td style=\"text-align: right;\">   0.460878</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.290228 </td><td style=\"text-align: right;\">      0.0102  </td><td style=\"text-align: right;\">         0.00757576</td><td style=\"text-align: right;\">       0.015625 </td></tr>\n",
       "<tr><td>train_model_bc554e3d</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.133857</td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0620</td><td>&lt;ray.tune.searc_0a00</td><td>                      </td><td>&lt;ray.tune.searc_05e0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_2da0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.470947</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.11542  </td><td style=\"text-align: right;\">      0.050125</td><td style=\"text-align: right;\">         0.208333  </td><td style=\"text-align: right;\">       0.0287829</td></tr>\n",
       "<tr><td>train_model_a0411dc8</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.373152</td><td style=\"text-align: right;\">                512</td><td>SoftMarginLoss      </td><td>&lt;ray.tune.searc_3c40</td><td>                      </td><td>&lt;ray.tune.searc_3820  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_d8d0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.444136</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.114967 </td><td style=\"text-align: right;\">      0.0706  </td><td style=\"text-align: right;\">         0.0530303 </td><td style=\"text-align: right;\">       0.105699 </td></tr>\n",
       "<tr><td>train_model_5a917c3c</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.305448</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>&lt;ray.tune.searc_2da0</td><td>                      </td><td>&lt;ray.tune.searc_0bb0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_0b20</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.652977</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.202046 </td><td style=\"text-align: right;\">      0       </td><td style=\"text-align: right;\">         0         </td><td style=\"text-align: right;\">       0        </td></tr>\n",
       "<tr><td>train_model_bdb4b7c7</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>elu            </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.202596</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_34c0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_3010 </td><td>                   </td><td>&lt;ray.tune.searc_2dd0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_3f40</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_2050</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.551308</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.255078 </td><td style=\"text-align: right;\">      0.2005  </td><td style=\"text-align: right;\">         0.151515  </td><td style=\"text-align: right;\">       0.297117 </td></tr>\n",
       "<tr><td>train_model_11088893</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>elu            </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.162519</td><td style=\"text-align: right;\">                256</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_68f0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_78b0</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_d870</td><td>&lt;ray.tune.searc_5660</td><td>SGD             </td><td style=\"text-align: right;\">   0.593505</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.0878701</td><td style=\"text-align: right;\">      0       </td><td style=\"text-align: right;\">         0         </td><td style=\"text-align: right;\">       0        </td></tr>\n",
       "<tr><td>train_model_d3395c5e</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.242049</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0620</td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_67a0 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_50f0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_7190</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.554664</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.232172 </td><td style=\"text-align: right;\">      0.0544  </td><td style=\"text-align: right;\">         0.5       </td><td style=\"text-align: right;\">       0.0287829</td></tr>\n",
       "<tr><td>train_model_4d32d7c8</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>leaky_relu     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.281704</td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0620</td><td>&lt;ray.tune.searc_f010</td><td>                      </td><td>&lt;ray.tune.searc_df60  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_3970</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.719031</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.134736 </td><td style=\"text-align: right;\">      0       </td><td style=\"text-align: right;\">         0         </td><td style=\"text-align: right;\">       0        </td></tr>\n",
       "<tr><td>train_model_537e1f9a</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.356361</td><td style=\"text-align: right;\">                128</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_cc40</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_d300</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_d750</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.421166</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.101541 </td><td style=\"text-align: right;\">      0.0304  </td><td style=\"text-align: right;\">         0.0227273 </td><td style=\"text-align: right;\">       0.0459559</td></tr>\n",
       "<tr><td>train_model_b7f8cce1</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.214125</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_2680</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_2c50</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.42115 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.207095 </td><td style=\"text-align: right;\">      0.150075</td><td style=\"text-align: right;\">         0.113636  </td><td style=\"text-align: right;\">       0.221749 </td></tr>\n",
       "<tr><td>train_model_e72b6e22</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>elu            </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.184737</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>&lt;ray.tune.searc_e080  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_c550 </td><td>                   </td><td>&lt;ray.tune.searc_c700</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_c760</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_e9e0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.34412 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.233736 </td><td style=\"text-align: right;\">      0.079525</td><td style=\"text-align: right;\">         0.5       </td><td style=\"text-align: right;\">       0.0434888</td></tr>\n",
       "<tr><td>train_model_25402d00</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.196212</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_3c10</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_28f0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.333625</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.106536 </td><td style=\"text-align: right;\">      0.12065 </td><td style=\"text-align: right;\">         0.0909091 </td><td style=\"text-align: right;\">       0.179808 </td></tr>\n",
       "<tr><td>train_model_7ae6763b</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.208014</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_a800</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_3460</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.333346</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.0866728</td><td style=\"text-align: right;\">      0       </td><td style=\"text-align: right;\">         0         </td><td style=\"text-align: right;\">       0        </td></tr>\n",
       "<tr><td>train_model_0c8c4f37</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.106531</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_fa60</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.523623</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.333464 </td><td style=\"text-align: right;\">      0.180075</td><td style=\"text-align: right;\">         0.136364  </td><td style=\"text-align: right;\">       0.265867 </td></tr>\n",
       "<tr><td>train_model_b58a8bca</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.101274</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_4d60  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_5660 </td><td>                   </td><td>&lt;ray.tune.searc_5390</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_7d60</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_73a0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.517966</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.354306 </td><td style=\"text-align: right;\">      0.22955 </td><td style=\"text-align: right;\">         0.174242  </td><td style=\"text-align: right;\">       0.33751  </td></tr>\n",
       "<tr><td>train_model_eb983f2f</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.108678</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_6440</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.791454</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.453914 </td><td style=\"text-align: right;\">      0       </td><td style=\"text-align: right;\">         0         </td><td style=\"text-align: right;\">       0        </td></tr>\n",
       "<tr><td>train_model_fec924e0</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.10497 </td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>&lt;ray.tune.searc_6c80  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_4b50 </td><td>                   </td><td>&lt;ray.tune.searc_6bc0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_5db0</td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_5bd0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.787148</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.454519 </td><td style=\"text-align: right;\">      0       </td><td style=\"text-align: right;\">         0         </td><td style=\"text-align: right;\">       0        </td></tr>\n",
       "<tr><td>train_model_264d92a1</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.152054</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>&lt;ray.tune.searc_7be0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_7e20 </td><td>                   </td><td>&lt;ray.tune.searc_62f0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_77f0</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_cf70</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.541424</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.368797 </td><td style=\"text-align: right;\">      0.0544  </td><td style=\"text-align: right;\">         0.5       </td><td style=\"text-align: right;\">       0.0287829</td></tr>\n",
       "<tr><td>train_model_7026b7ef</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.152513</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_1570  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_2d10 </td><td>                   </td><td>&lt;ray.tune.searc_3a90</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_0700</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1d50</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.526987</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.494996 </td><td style=\"text-align: right;\">      0.210325</td><td style=\"text-align: right;\">         0.159091  </td><td style=\"text-align: right;\">       0.311194 </td></tr>\n",
       "<tr><td>train_model_a02e7767</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.297984</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_12a0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_2530 </td><td>                   </td><td>&lt;ray.tune.searc_0ac0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_3af0</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1720</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.660675</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.268297 </td><td style=\"text-align: right;\">      0.1903  </td><td style=\"text-align: right;\">         0.143939  </td><td style=\"text-align: right;\">       0.281492 </td></tr>\n",
       "<tr><td>train_model_1d972dca</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.147458</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_f520  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_fc10 </td><td>                   </td><td>&lt;ray.tune.searc_d780</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_d930</td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_a530</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.627814</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.484783 </td><td style=\"text-align: right;\">      0.169725</td><td style=\"text-align: right;\">         0.128788  </td><td style=\"text-align: right;\">       0.249613 </td></tr>\n",
       "<tr><td>train_model_33ec158a</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.143999</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_7580  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_d2d0 </td><td>                   </td><td>&lt;ray.tune.searc_d270</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_fdf0</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1a20</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.506176</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.405454 </td><td style=\"text-align: right;\">      0.18935 </td><td style=\"text-align: right;\">         0.143939  </td><td style=\"text-align: right;\">       0.277477 </td></tr>\n",
       "<tr><td>train_model_49f8f8ac</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>leaky_relu     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.179715</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_e500  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_d330 </td><td>                   </td><td>&lt;ray.tune.searc_d0f0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_e290</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_2170</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.502745</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.357583 </td><td style=\"text-align: right;\">      0.189725</td><td style=\"text-align: right;\">         0.143939  </td><td style=\"text-align: right;\">       0.279025 </td></tr>\n",
       "<tr><td>train_model_6e9698ba</td><td>TERMINATED</td><td>127.0.0.1:46404</td><td>leaky_relu     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.175309</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_d840  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_f100 </td><td>                   </td><td>&lt;ray.tune.searc_c880</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_c1f0</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_d5a0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.580172</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.324392 </td><td style=\"text-align: right;\">      0       </td><td style=\"text-align: right;\">         0         </td><td style=\"text-align: right;\">       0        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define search space for hyperparameter optimization\n",
    "search_space = {\n",
    "    \"output_dim\": len(all_two_grams),  # Output dimension is also the number of unique 2-grams\n",
    "    \"num_layers\": tune.randint(1, 4),  # Vary the number of layers in the model\n",
    "    #\"num_layers\": tune.randint(1, 2),\n",
    "    \"hidden_layer_size\": tune.choice([128, 256, 512, 1024, 2048]),  # Different sizes for hidden layers\n",
    "    #\"hidden_layer_size\": tune.choice([1024, 2048]),  # Different sizes for hidden layers\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.4),  # Dropout rate between 0.1 and 0.4\n",
    "    \"activation_fn\": tune.choice([\"relu\", \"leaky_relu\", \"gelu\", \"elu\", \"selu\", \"tanh\"]),  # Activation functions to choose from\n",
    "    \"optimizer\": tune.choice([\n",
    "        {\"name\": \"Adam\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"AdamW\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"SGD\", \"lr\": tune.loguniform(1e-4, 1e-2), \"momentum\": tune.uniform(0.0, 0.99)},\n",
    "        {\"name\": \"RMSprop\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "    ]),\n",
    "    \"loss_fn\": tune.choice([\"BCEWithLogitsLoss\", \"MultiLabelSoftMarginLoss\", \"SoftMarginLoss\"]),\n",
    "    \"threshold\": tune.uniform(0.3, 0.8),\n",
    "    \"lr_scheduler\": tune.choice([\n",
    "        {\"name\": \"StepLR\", \"step_size\": tune.choice([5, 10, 20]), \"gamma\": tune.uniform(0.1, 0.9)},\n",
    "        {\"name\": \"ExponentialLR\", \"gamma\": tune.uniform(0.85, 0.99)},\n",
    "        {\"name\": \"ReduceLROnPlateau\", \"mode\": \"min\", \"factor\": tune.uniform(0.1, 0.5), \"patience\": tune.choice([5, 10, 15])},\n",
    "        {\"name\": \"CosineAnnealingLR\", \"T_max\": tune.loguniform(10, 50) , \"eta_min\": tune.choice([1e-5, 1e-6, 0])},\n",
    "        {\"name\": \"CyclicLR\", \"base_lr\": tune.loguniform(1e-5, 1e-3), \"max_lr\": tune.loguniform(1e-3, 1e-1), \"step_size_up\": tune.choice([2000, 4000]), \"mode_cyclic\": tune.choice([\"triangular\", \"triangular2\", \"exp_range\"]) },\n",
    "        {\"name\": \"None\"}  # No scheduler\n",
    "    ]),\n",
    "    \"batch_size\": tune.choice([8, 16, 32, 64]),  # Batch sizes to test\n",
    "}\n",
    "\n",
    "# Initialize Ray for hyperparameter optimization\n",
    "ray.init(ignore_reinit_error=True, logging_level=\"ERROR\")\n",
    "\n",
    "# Optuna Search Algorithm for optimizing the hyperparameters\n",
    "optuna_search = OptunaSearch(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"max\")\n",
    "\n",
    "# Use ASHAScheduler to manage trials and early stopping\n",
    "scheduler = ASHAScheduler(metric=\"total_val_loss\", mode=\"min\")\n",
    "\n",
    "\n",
    "\n",
    "# Define and configure the Tuner for Ray Tune\n",
    "tuner = tune.Tuner(\n",
    "    partial(train_model, data_dir=data_dir, output_dim=len(all_two_grams),alice_enc_hash=alice_enc_hash, identifier=identifier, patience=DEA_CONFIG[\"Patience\"], min_delta=DEA_CONFIG[\"MinDelta\"]),  # The function to optimize (training function)\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,  # Search strategy using Optuna\n",
    "        scheduler=scheduler,  # Use ASHA to manage the trials\n",
    "        num_samples=DEA_CONFIG[\"NumSamples\"],  # Number of trials to run\n",
    "        max_concurrent_trials=GLOBAL_CONFIG[\"Workers\"],\n",
    "    ),\n",
    "    param_space=search_space  # Pass in the defined hyperparameter search space\n",
    "\n",
    ")\n",
    "\n",
    "# Run the tuner\n",
    "results = tuner.fit()\n",
    "\n",
    "# Shut down Ray after finishing the optimization\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = results\n",
    "best_result = result_grid.get_best_result(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_hyperparameter_optimization = time.time() - start_hyperparameter_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to all_trial_results.csv\n",
      "\n",
      "🔍 Best_Result\n",
      "----------------------------------------\n",
      "Config: {'output_dim': 1036, 'num_layers': 2, 'hidden_layer_size': 1024, 'dropout_rate': 0.1012739321006422, 'activation_fn': 'tanh', 'optimizer': {'name': 'Adam', 'lr': 1.1599271686225065e-05}, 'loss_fn': 'SoftMarginLoss', 'threshold': 0.5179659899004138, 'lr_scheduler': {'name': 'CyclicLR', 'base_lr': 8.415305496975828e-05, 'max_lr': 0.004235947607397567, 'step_size_up': 2000, 'mode_cyclic': 'triangular2'}, 'batch_size': 16}\n",
      "Average Dice: 0.2295\n",
      "Average Precision: 0.1742\n",
      "Average Recall: 0.3375\n",
      "Average F1: 0.2296\n",
      "\n",
      "🔍 Worst_Result\n",
      "----------------------------------------\n",
      "Config: {'output_dim': 1036, 'num_layers': 1, 'hidden_layer_size': 512, 'dropout_rate': 0.2610290050829065, 'activation_fn': 'gelu', 'optimizer': {'name': 'SGD', 'lr': 0.004778138951107689, 'momentum': 0.35677434034100386}, 'loss_fn': 'MultiLabelSoftMarginLoss', 'threshold': 0.635295442893812, 'lr_scheduler': {'name': 'StepLR', 'step_size': 5, 'gamma': 0.8855342276011314}, 'batch_size': 8}\n",
      "Average Dice: 0.0000\n",
      "Average Precision: 0.0000\n",
      "Average Recall: 0.0000\n",
      "Average F1: 0.0000\n",
      "\n",
      "📊 Average Metrics Across All Trials\n",
      "----------------------------------------\n",
      "Average_dice: 0.0872\n",
      "Average_precision: 0.1271\n",
      "Average_recall: 0.1199\n",
      "Average_f1: 0.0872\n",
      "📊 Saved plot: metric_distributions.png\n",
      "📌 Saved heatmap: correlation_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    worst_result = result_grid.get_best_result(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"min\")\n",
    "\n",
    "    # Combine configs and metrics into a DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            **clean_result_dict(resolve_config(result.config)),\n",
    "            **{k: result.metrics.get(k) for k in [\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]},\n",
    "        }\n",
    "        for result in result_grid\n",
    "    ])\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(f\"{save_to}/hyperparameteroptimization/all_trial_results.csv\", index=False)\n",
    "    print(\"✅ Results saved to all_trial_results.csv\")\n",
    "\n",
    "    print_and_save_result(\"Best_Result\", best_result, f\"{save_to}/hyperparameteroptimization\")\n",
    "    print_and_save_result(\"Worst_Result\", worst_result, f\"{save_to}/hyperparameteroptimization\")\n",
    "\n",
    "    # Compute and print average metrics\n",
    "    print(\"\\n📊 Average Metrics Across All Trials\")\n",
    "    avg_metrics = df[[\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]].mean()\n",
    "    print(\"-\" * 40)\n",
    "    for key, value in avg_metrics.items():\n",
    "        print(f\"{key.capitalize()}: {value:.4f}\")\n",
    "\n",
    "    # --- 📈 Plotting performance metrics ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df[[\"average_dice\", \"average_recall\", \"average_f1\", \"average_precision\"]])\n",
    "    plt.title(\"Distribution of Performance Metrics Across Trials\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{save_to}/hyperparameteroptimization/metric_distributions.png\")\n",
    "    plt.close()\n",
    "    print(\"📊 Saved plot: metric_distributions.png\")\n",
    "\n",
    "    # --- 📌 Correlation between config params and performance ---\n",
    "    # Only include numeric config columns\n",
    "    exclude_cols = {\"input_dim\", \"output_dim\"}\n",
    "    numeric_config_cols = [\n",
    "        col for col in df.columns\n",
    "        if pd.api.types.is_numeric_dtype(df[col]) and col not in exclude_cols\n",
    "    ]\n",
    "    correlation_df = df[numeric_config_cols].corr()\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Between Parameters and Metrics\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_to}/hyperparameteroptimization/correlation_heatmap.png\")\n",
    "    plt.close()\n",
    "    print(\"📌 Saved heatmap: correlation_heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "\n",
    "The neural network model is selected dynamically based on the encoding technique used for Alice’s data.\n",
    "\n",
    "### Supported Models:\n",
    "\n",
    "- **BloomFilter** → `BloomFilterToTwoGramClassifier`  \n",
    "  - Input: Binary vector (Bloom filter)  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TabMinHash** → `TabMinHashToTwoGramClassifier`  \n",
    "  - Input: Tabulated MinHash signature  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TwoStepHash** → `TwoStepHashToTwoGramClassifier`  \n",
    "  - Input: Length of the unique integers present\n",
    "  - Output: 2-gram predicition\n",
    "    \n",
    "Each model outputs predictions over the set of all possible 2-grams (`all_two_grams`), and the input dimension is dynamically configured based on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_model_training = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = resolve_config(best_result.config)\n",
    "data_train, data_val, data_test = load_data(data_dir, alice_enc_hash, identifier, load_test=True)\n",
    "input_dim=data_train[0][0].shape[0]\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    data_train,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),  # Default to 32 if not specified\n",
    "    shuffle=True  # Important for training\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    data_val,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),\n",
    "    shuffle=False  # Allows variation in validation batches\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    data_test,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),\n",
    "    shuffle=False  # Allows variation in validation batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.1012739321006422, inplace=False)\n",
      "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (4): Tanh()\n",
      "    (5): Dropout(p=0.1012739321006422, inplace=False)\n",
      "    (6): Linear(in_features=1024, out_features=1036, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=len(all_two_grams),\n",
    "            hidden_layer=best_config.get(\"hidden_layer_size\", 128),  # Default to 128 if not specified\n",
    "            num_layers=best_config.get(\"num_layers\", 2),  # Default to 2 if not specified\n",
    "            dropout_rate=best_config.get(\"dropout_rate\", 0.2),  # Default to 0.2 if not specified\n",
    "            activation_fn=best_config.get(\"activation_fn\", \"relu\")  # Default to 'relu' if not specified\n",
    "        )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Environment Setup\n",
    "This code initializes the core components needed for training a neural network model.\n",
    "\n",
    "1. TensorBoard Setup\n",
    "    - Creates unique run name by combining:\n",
    "    - Loss function type\n",
    "    - Optimizer choice\n",
    "    - Alice's algorithm\n",
    "    - Initializes TensorBoard writer in runs directory\n",
    "2. Device Configuration\n",
    "    - Automatically selects GPU if available, falls back to CPU\n",
    "    - Moves model to selected device\n",
    "3. Loss Functions\n",
    "    - `BCEWithLogitsLoss`: Binary Cross Entropy with Logits\n",
    "    - `MultiLabelSoftMarginLoss`: Multi-Label Soft Margin Loss\n",
    "4. Optimizers:\n",
    "    - `Adam`: Adaptive Moment Estimation\n",
    "    - `AdamW`: Adam with Weight Decay\n",
    "    - `SGD`: Stochastic Gradient Descent (with momentum)\n",
    "    - `RMSprop`: Root Mean Square Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    # Setup tensorboard logging\n",
    "    run_name = \"\".join([\n",
    "        best_config.get(\"loss_fn\", \"MultiLabelSoftMarginLoss\"),\n",
    "        best_config.get(\"optimizer\").get(\"name\", \"Adam\"),\n",
    "        ENC_CONFIG[\"AliceAlgo\"],\n",
    "        best_config.get(\"activation_fn\", \"relu\"),\n",
    "    ])\n",
    "    tb_writer = SummaryWriter(f\"{save_to}/{run_name}\")\n",
    "\n",
    "# Setup compute device (GPU/CPU)\n",
    "compute_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(compute_device)\n",
    "\n",
    "# Initialize loss function\n",
    "match best_config.get(\"loss_fn\", \"MultiLabelSoftMarginLoss\"):\n",
    "    case \"BCEWithLogitsLoss\":\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    case \"MultiLabelSoftMarginLoss\":\n",
    "        criterion = nn.MultiLabelSoftMarginLoss(reduction='mean')\n",
    "    case \"SoftMarginLoss\":\n",
    "        criterion = nn.SoftMarginLoss()\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported loss function: {best_config.get('loss_fn', 'MultiLabelSoftMarginLoss')}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "match best_config.get(\"optimizer\").get(\"name\", \"Adam\"):\n",
    "    case \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                            lr=best_config.get(\"optimizer\").get(\"lr\"),\n",
    "                            momentum=best_config.get(\"optimizer\").get(\"momentum\"))\n",
    "    case \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported optimizer: {best_config.get('optimizer').get('name', 'Adam')}\")\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "match best_config.get(\"lr_scheduler\").get(\"name\", \"None\"):\n",
    "    case \"StepLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=best_config.get(\"lr_scheduler\").get(\"step_size\"),\n",
    "            gamma=best_config.get(\"lr_scheduler\").get(\"gamma\")\n",
    "        )\n",
    "    case \"ExponentialLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=best_config.get(\"lr_scheduler\").get(\"gamma\")\n",
    "        )\n",
    "    case \"ReduceLROnPlateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=best_config.get(\"lr_scheduler\").get(\"mode\"),\n",
    "            factor=best_config.get(\"lr_scheduler\").get(\"factor\"),\n",
    "            patience=best_config.get(\"lr_scheduler\").get(\"patience\")\n",
    "        )\n",
    "    case \"CosineAnnealingLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=best_config.get(\"lr_scheduler\").get(\"T_max\")\n",
    "        )\n",
    "    case \"CyclicLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=best_config.get(\"lr_scheduler\").get(\"base_lr\"),\n",
    "            max_lr=best_config.get(\"lr_scheduler\").get(\"max_lr\"),\n",
    "            step_size_up=best_config.get(\"lr_scheduler\").get(\"step_size_up\"),\n",
    "            mode=best_config.get(\"lr_scheduler\").get(\"mode_cyclic\"),\n",
    "            cycle_momentum=False  # usually False for Adam/AdamW\n",
    "        )\n",
    "    case None | \"None\":\n",
    "        scheduler = None\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported LR scheduler: {best_config.get('lr_scheduler').get('name', 'None')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Early Stopping\n",
    "\n",
    "The function `train_model` orchestrates the training process for the neural network, including both training and validation phases for each epoch. It also utilizes **early stopping** to halt training when the validation loss fails to improve over multiple epochs, avoiding overfitting.\n",
    "\n",
    "### Key Phases:\n",
    "1. **Training Phase**: \n",
    "   - The model is trained on the `dataloader_train`, computing the training loss using the specified loss function (`criterion`) and optimizer. Gradients are calculated, and the model parameters are updated.\n",
    "  \n",
    "2. **Validation Phase**:\n",
    "   - The model is evaluated on the `dataloader_val` without updating weights. The validation loss is computed to track model performance on unseen data.\n",
    "\n",
    "3. **Logging**: \n",
    "   - Training and validation losses are logged to both the console and **TensorBoard** for tracking model performance during training.\n",
    "\n",
    "4. **Early Stopping**: \n",
    "   - If the validation loss does not improve after a certain number of epochs (defined by `DEA_CONFIG[\"Patience\"]`), the training process is halted to prevent overfitting.\n",
    "\n",
    "### Helper Functions:\n",
    "- `run_epoch`: Handles a single epoch, either for training or validation, depending on the flag `is_training`.\n",
    "- `log_metrics`: Logs the training and validation losses to the console and TensorBoard for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_epoch_metrics(epoch, total_epochs, train_loss, val_loss):\n",
    "    epoch_str = f\"[{epoch + 1}/{total_epochs}]\"\n",
    "    print(f\"{epoch_str} 🔧 Train Loss: {train_loss:.4f} | 🔍 Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if DEA_CONFIG.get(\"SaveResults\", False) and 'tb_writer' in globals():\n",
    "        tb_writer.add_scalar(\"Loss/train\", train_loss, epoch + 1)\n",
    "        tb_writer.add_scalar(\"Loss/validation\", val_loss, epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, device, scheduler=None):\n",
    "    num_epochs = best_config.get(\"epochs\", DEA_CONFIG[\"Epochs\"])\n",
    "    verbose = GLOBAL_CONFIG[\"Verbose\"]\n",
    "    patience = DEA_CONFIG[\"Patience\"]\n",
    "    min_delta = DEA_CONFIG[\"MinDelta\"]\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=min_delta, verbose=verbose)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- Training ----\n",
    "        model.train()\n",
    "        train_loss = run_epoch(\n",
    "            model, dataloader_train, criterion, optimizer,\n",
    "            device, is_training=True, verbose=verbose, scheduler=scheduler\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = run_epoch(\n",
    "                model, dataloader_val, criterion, optimizer,\n",
    "                device, is_training=False, verbose=verbose, scheduler=scheduler\n",
    "            )\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # ---- Scheduler step ----\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # ---- Logging ----\n",
    "        _log_epoch_metrics(epoch, num_epochs, train_loss, val_loss)\n",
    "\n",
    "        # ---- Early stopping ----\n",
    "        if early_stopper(val_loss):\n",
    "            if verbose:\n",
    "                print(f\"⏹️ Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/15] 🔧 Train Loss: 0.6932 | 🔍 Val Loss: 0.6923\n",
      "[2/15] 🔧 Train Loss: 0.6907 | 🔍 Val Loss: 0.6913\n",
      "[3/15] 🔧 Train Loss: 0.6886 | 🔍 Val Loss: 0.6902\n",
      "[4/15] 🔧 Train Loss: 0.6865 | 🔍 Val Loss: 0.6889\n",
      "[5/15] 🔧 Train Loss: 0.6848 | 🔍 Val Loss: 0.6879\n",
      "[6/15] 🔧 Train Loss: 0.6837 | 🔍 Val Loss: 0.6872\n",
      "[7/15] 🔧 Train Loss: 0.6829 | 🔍 Val Loss: 0.6867\n",
      "[8/15] 🔧 Train Loss: 0.6824 | 🔍 Val Loss: 0.6864\n",
      "[9/15] 🔧 Train Loss: 0.6821 | 🔍 Val Loss: 0.6863\n",
      "[10/15] 🔧 Train Loss: 0.6820 | 🔍 Val Loss: 0.6862\n",
      "[11/15] 🔧 Train Loss: 0.6819 | 🔍 Val Loss: 0.6861\n",
      "[12/15] 🔧 Train Loss: 0.6818 | 🔍 Val Loss: 0.6861\n",
      "[13/15] 🔧 Train Loss: 0.6818 | 🔍 Val Loss: 0.6861\n",
      "[14/15] 🔧 Train Loss: 0.6817 | 🔍 Val Loss: 0.6861\n",
      "[15/15] 🔧 Train Loss: 0.6817 | 🔍 Val Loss: 0.6861\n"
     ]
    }
   ],
   "source": [
    "model, train_losses, val_losses = train_model(\n",
    "    model, dataloader_train, dataloader_val,\n",
    "    criterion, optimizer, compute_device,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_model_training = time.time() - start_model_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Visualization over Epochs\n",
    "\n",
    "This code snippet generates a plot to visualize the **training loss** and **validation loss** across epochs. It's useful for tracking model performance during training and evaluating if overfitting is occurring (i.e., when validation loss starts increasing while training loss continues to decrease).\n",
    "\n",
    "### Key Elements:\n",
    "1. **Plotting the Losses**: \n",
    "   - The `train_losses` and `val_losses` are plotted over the epochs. \n",
    "   - The **blue line** represents the training loss, and the **red line** represents the validation loss.\n",
    "\n",
    "2. **Legend**: \n",
    "   - A legend is added to distinguish between training and validation losses.\n",
    "\n",
    "3. **Title and Labels**: \n",
    "   - The plot is titled \"Training and Validation Loss over Epochs\" for context.\n",
    "   - **X-axis** represents the epoch number, and **Y-axis** represents the loss value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation loss', color='red')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "if DEA_CONFIG.get(\"SaveResults\", False):\n",
    "    plt.savefig(f\"{save_to}/loss_curve.png\")\n",
    "    plt.close()\n",
    "    print(\"📉 Saved loss curve to loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Application to Encoded Data\n",
    "\n",
    "This code performs inference on the test data and compares the predicted 2-grams with the actual 2-grams, providing a performance evaluation based on the **Dice similarity coefficient**.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Prepare for Evaluation**:\n",
    "   - The model is switched to **evaluation mode** (`model.eval()`), ensuring no gradient computation.\n",
    "   \n",
    "2. **Thresholding**:\n",
    "   - A threshold (`DEA_CONFIG[\"FilterThreshold\"]`) is applied to filter out low-probability predictions, retaining only the most confident predictions.\n",
    "\n",
    "3. **Inference and 2-Gram Scoring**:\n",
    "   - The model is applied to the batch, and the **logits** are converted into probabilities using the **sigmoid function**.\n",
    "   - The probabilities are then mapped to **2-gram scores**, and scores below the threshold are discarded.\n",
    "\n",
    "4. **Reconstructing Words**:\n",
    "   - For each sample in the batch, **2-grams** are reconstructed into words based on the filtered scores.\n",
    "\n",
    "5. **Performance Metrics**:\n",
    "   - The actual 2-grams (from the test dataset) are compared with the predicted 2-grams, and the **Dice similarity coefficient** is calculated for each sample.\n",
    "\n",
    "### Result:\n",
    "- The code generates a list `combined_results_performance`, which contains a detailed comparison for each UID, including:\n",
    "  - **Actual 2-grams** (from the test data)\n",
    "  - **Predicted 2-grams** (from the model)\n",
    "  - **Dice similarity** score indicating how similar the actual and predicted 2-grams are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(\n",
    "    GLOBAL_CONFIG[\"LoadPath\"] if GLOBAL_CONFIG[\"LoadResults\"] else save_to,\n",
    "    \"trained_model\"\n",
    ")\n",
    "model_file   = f\"{base_path}/model.pt\"\n",
    "config_file  = f\"{base_path}/config.json\"\n",
    "result_file  = f\"{base_path}/result.json\"\n",
    "metrics_file = f\"{base_path}/metrics.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_config(model, config, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(path, \"model.pt\"))\n",
    "    with open(os.path.join(path, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    print(f\"✅ Saved model and config to {path}\")\n",
    "\n",
    "\n",
    "def load_model_and_config(model_cls, path, input_dim, output_dim):\n",
    "    with open(os.path.join(path, \"config.json\")) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    model = model_cls(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        hidden_layer=config.get(\"hidden_layer_size\", 128),\n",
    "        num_layers=config.get(\"num_layers\", 2),\n",
    "        dropout_rate=config.get(\"dropout_rate\", 0.2),\n",
    "        activation_fn=config.get(\"activation_fn\", \"relu\")\n",
    "    )\n",
    "    model.load_state_dict(torch.load(os.path.join(path, \"model.pt\")))\n",
    "    model.eval()\n",
    "    return model, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model and config to experiment_results/experiment_BloomFilter_fakename_1k_2025-07-07_16-39-56/trained_model\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    save_model_and_config(model, best_config, base_path)\n",
    "\n",
    "if GLOBAL_CONFIG[\"LoadResults\"]:\n",
    "    #TODO: how to figure out input_dim without loading dataset\n",
    "    model, best_config = load_model_and_config(BaseModel, base_path, input_dim=1024, output_dim=len(all_two_grams))\n",
    "    model.to(compute_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_application_to_encoded_data = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Test Metrics:\n",
      "  Dice:      0.1808\n",
      "  Precision: 0.1388\n",
      "  Recall:    0.2618\n",
      "  F1 Score:  0.1808\n"
     ]
    }
   ],
   "source": [
    "# Initialize metric accumulators\n",
    "total_dice = total_precision = total_recall = total_f1 = 0.0\n",
    "num_samples = 0\n",
    "results = []\n",
    "\n",
    "threshold = best_config.get(\"threshold\", 0.5)\n",
    "model.eval()\n",
    "\n",
    "# Progress bar only if verbose\n",
    "dataloader_iter = tqdm(dataloader_test, desc=\"Test loop\") if GLOBAL_CONFIG[\"Verbose\"] else dataloader_test\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels, uids in dataloader_iter:\n",
    "        data, labels = data.to(compute_device), labels.to(compute_device)\n",
    "\n",
    "        logits = model(data)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Actual and predicted 2-grams\n",
    "        actual_two_grams = decode_labels_to_two_grams(two_gram_dict, labels)\n",
    "        predicted_scores = map_probabilities_to_two_grams(two_gram_dict, probs)\n",
    "        predicted_filtered = filter_high_scoring_two_grams(predicted_scores, threshold)\n",
    "\n",
    "        # Batch metrics\n",
    "        bs = data.size(0)\n",
    "        dice, precision, recall, f1 = calculate_performance_metrics(actual_two_grams, predicted_filtered)\n",
    "\n",
    "        total_dice += dice\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        num_samples += bs\n",
    "\n",
    "        # Store per-sample predictions\n",
    "        for uid, actual, predicted in zip(uids, actual_two_grams, predicted_filtered):\n",
    "            metrics = metrics_per_entry(actual, predicted)\n",
    "            results.append({\n",
    "                \"uid\": uid,\n",
    "                \"actual_two_grams\": actual,\n",
    "                \"predicted_two_grams\": predicted,\n",
    "                \"precision\": metrics[\"precision\"],\n",
    "                \"recall\": metrics[\"recall\"],\n",
    "                \"f1\": metrics[\"f1\"],\n",
    "                \"dice\": metrics[\"dice\"],\n",
    "                \"jaccard\": metrics[\"jaccard\"]\n",
    "            })\n",
    "\n",
    "# Avoid division by zero\n",
    "if num_samples > 0:\n",
    "    avg_dice = total_dice / num_samples\n",
    "    avg_precision = total_precision / num_samples\n",
    "    avg_recall = total_recall / num_samples\n",
    "    avg_f1 = total_f1 / num_samples\n",
    "else:\n",
    "    avg_dice = avg_precision = avg_recall = avg_f1 = 0.0\n",
    "\n",
    "# Logging\n",
    "print(f\"\\n📊 Final Test Metrics:\")\n",
    "print(f\"  Dice:      {avg_dice:.4f}\")\n",
    "print(f\"  Precision: {avg_precision:.4f}\")\n",
    "print(f\"  Recall:    {avg_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {avg_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_application_to_encoded_data = time.time() - start_application_to_encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE - Metrics and Result\n",
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        f.write(f\"Average Precision: {avg_precision:.4f}\\n\")\n",
    "        f.write(f\"Average Recall: {avg_recall:.4f}\\n\")\n",
    "        f.write(f\"Average F1 Score: {avg_f1:.4f}\\n\")\n",
    "        f.write(f\"Average Dice Similarity: {avg_dice:.4f}\\n\")\n",
    "\n",
    "    with open(result_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD - Result\n",
    "if GLOBAL_CONFIG[\"LoadResults\"]:\n",
    "    with open(result_file, 'r', encoding='utf-8') as f:\n",
    "        result = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance for Re-Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Sample Reconstructions (first 5)\n",
      "UID: 15011\n",
      "  Actual 2-grams:    ['ep', 'er', 'he', 'in', 'nh', 'or', 'pw', 'ri', 'rt', 'th', 'wo', 'h4', '19', '28', '42', '81', '92', '99']\n",
      "  Predicted 2-grams: ['19', 'an', 'al', '01', '20', '10', '30', 'os', '21', '02', 'er', 'jo', '41', 'ra', '03', 'le', 'da', '99', 'y1', 'a1', 'ph', '11', '96', 'se', 'nd', '97', '14', '55', 'll', 'ew', 'dr', 're', 'yd']\n",
      "------------------------------------------------------------\n",
      "UID: 13658\n",
      "  Actual 2-grams:    ['ac', 'ck', 'go', 'ja', 'kg', 'or', 're', 'e5', '16', '19', '51', '61', '69', '96']\n",
      "  Predicted 2-grams: ['19', 'an', '01', 'al', '20', '10', '30', 'jo', 'os', '21', '41', 'er', '02', 'ra', '11', '96', 'nd', 'a1', '03', 'le', 'da', 'ph', 'se', 'y1', '99', 'ew', 'll', '55', '97', 're', 'n1', '14', 'la']\n",
      "------------------------------------------------------------\n",
      "UID: 24989\n",
      "  Actual 2-grams:    ['aj', 'da', 'es', 'in', 'jo', 'li', 'nd', 'ne', 'on', 's8', '11', '19', '21', '80', '82', '98']\n",
      "  Predicted 2-grams: ['19', 'an', 'al', '01', '10', '30', '20', 'os', 'jo', '02', '21', '41', 'er', 'ra', '11', '03', 'da', 'y1', 'le', 'a1', 'll', '96', 'nd', 'se', 'ph', 'ew', '55', '98', '99', 'la', '97', 'n1', 'yd']\n",
      "------------------------------------------------------------\n",
      "UID: 43684\n",
      "  Actual 2-grams:    ['ah', 'al', 'do', 'er', 'he', 'ia', 'ly', 'nd', 'on', 'rn', 'si', 'ys', 'n4', '19', '24', '38', '41', '42', '93']\n",
      "  Predicted 2-grams: ['19', 'an', 'al', '01', '20', '10', '30', '02', 'os', 'er', '21', 'ra', '41', 'jo', '03', 'da', 'nd', 'y1', 'le', 'a1', '96', '11', 'ew', 'se', 'dr', 'ph', '55', 're', '97', '00', '99', 'yd', 'll']\n",
      "------------------------------------------------------------\n",
      "UID: 31743\n",
      "  Actual 2-grams:    ['ab', 'bu', 'gg', 'gs', 'is', 'li', 'sa', 'ug', 's7', '19', '28', '72', '81', '98']\n",
      "  Predicted 2-grams: ['19', 'al', 'an', '01', '20', '10', '30', 'os', '02', 'er', 'ra', '21', 'jo', '41', 'a1', '03', 'da', 'y1', 'nd', 'le', '11', 'se', 'ew', '96', 'ph', '55', 'll', '97', '00', '99', 'la', 'dr', 'yd']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.json_normalize(results) # ≈2× faster than DataFrame(list)\n",
    "\n",
    "metric_cols = [\"precision\", \"recall\", \"f1\", \"dice\", \"jaccard\"]        # keys created earlier\n",
    "melted = results_df.melt(value_vars=metric_cols,\n",
    "                         var_name=\"metric\",\n",
    "                         value_name=\"score\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=melted,\n",
    "             x=\"score\",\n",
    "             hue=\"metric\",\n",
    "             bins=20,\n",
    "             element=\"step\",\n",
    "             fill=False,\n",
    "             kde=True,\n",
    "             palette=\"Set2\")\n",
    "plt.title(\"Distribution of Precision / Recall / F1 across Samples\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "if DEA_CONFIG.get(\"SaveResults\", False):\n",
    "    plt.savefig(f\"{save_to}/metric_distributions.png\")\n",
    "    print(\"📊  Saved plot: metric_distributions.png\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n🔍 Sample Reconstructions (first 5)\")\n",
    "for _, row in results_df.iloc[:5].iterrows():\n",
    "    print(f\"UID: {row.uid}\")\n",
    "    print(f\"  Actual 2-grams:    {row.actual_two_grams}\")\n",
    "    print(f\"  Predicted 2-grams: {row.predicted_two_grams}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Refinement and Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_refinement_and_reconstruction = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Reidentification Analysis:\n",
      "Total Reidentified Individuals: 0\n",
      "Total Not Reidentified Individuals: 889\n",
      "Reidentification Rate: 0.00%\n",
      "\n",
      "🔄 Reconstructing results using fuzzy matching (entry-wise, parallelized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Reidentification Analysis:\n",
      "Total Reidentified Individuals: 0\n",
      "Total Not Reidentified Individuals: 889\n",
      "Reidentification Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_not_reidentified_df(data_dir: str, identifier: str) -> pd.DataFrame:\n",
    "    df = load_not_reidentified_data(data_dir, alice_enc_hash, identifier)\n",
    "    return lowercase_df(df)\n",
    "\n",
    "def create_identifier(df: pd.DataFrame, comps):\n",
    "    df = df.copy()\n",
    "    df[\"identifier\"] = create_identifier_column_dynamic(df, comps)\n",
    "    return df[[\"uid\", \"identifier\"]]\n",
    "\n",
    "def run_reidentification_once(reconstructed, df_not_reidentified, merge_cols, technique, identifier_components=None):\n",
    "\n",
    "    df_reconstructed = lowercase_df(pd.DataFrame(reconstructed, columns=merge_cols))\n",
    "\n",
    "    if(identifier_components):\n",
    "        df_not_reidentified = create_identifier(df_not_reidentified, identifier_components)\n",
    "\n",
    "    return reidentification_analysis(\n",
    "        df_reconstructed,\n",
    "        df_not_reidentified,\n",
    "        merge_cols,\n",
    "        len(df_not_reidentified),\n",
    "        technique,\n",
    "        save_path=f\"{save_to}/re_identification_results\"\n",
    "    )\n",
    "\n",
    "header = read_header(GLOBAL_CONFIG[\"Data\"])\n",
    "\n",
    "TECHNIQUES = {\n",
    "    \"ai\": {\n",
    "        \"fn\": reconstruct_identities_with_llm,\n",
    "        \"merge_cols\": header[:3] + [header[-1]],\n",
    "        \"identifier_comps\": None,\n",
    "    },\n",
    "    \"greedy\": {\n",
    "        \"fn\": greedy_reconstruction,\n",
    "        \"merge_cols\": [\"uid\", \"identifier\"],\n",
    "        \"identifier_comps\": header[:-1],\n",
    "    },\n",
    "    \"fuzzy\": {\n",
    "        \"fn\": fuzzy_reconstruction_approach,\n",
    "        \"merge_cols\": header[:3] + [header[-1]],\n",
    "        \"identifier_comps\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "selected = DEA_CONFIG[\"MatchingTechnique\"]\n",
    "df_not_reid_cached = get_not_reidentified_df(data_dir, identifier)\n",
    "save_dir = f\"{save_to}/re_identification_results\"\n",
    "\n",
    "if selected == \"fuzzy_and_greedy\":\n",
    "    reidentified = {}\n",
    "    for name in (\"greedy\", \"fuzzy\"):\n",
    "        info = TECHNIQUES[name]\n",
    "        if name == \"fuzzy\":\n",
    "            recon = info[\"fn\"](results, GLOBAL_CONFIG[\"Workers\"])\n",
    "        else:\n",
    "            recon = info[\"fn\"](results)\n",
    "        reidentified[name] = run_reidentification_once(\n",
    "            recon,\n",
    "            df_not_reid_cached,\n",
    "            info[\"merge_cols\"],\n",
    "            name,\n",
    "            info[\"identifier_comps\"],\n",
    "        )\n",
    "else:\n",
    "    # single technique path\n",
    "    print(selected)\n",
    "    print(TECHNIQUES[selected])\n",
    "    if selected not in TECHNIQUES:\n",
    "        raise ValueError(f\"Unsupported matching technique: {selected}\")\n",
    "    info = TECHNIQUES[selected]\n",
    "    if selected == \"fuzzy\":\n",
    "        recon = info[\"fn\"](results, GLOBAL_CONFIG[\"Workers\"])\n",
    "    if selected == \"ai\":\n",
    "        recon = info[\"fn\"](results, info[\"merge_cols\"][:-1])\n",
    "    else:\n",
    "        recon = info[\"fn\"](results)\n",
    "    reidentified = run_reidentification_once(\n",
    "        recon,\n",
    "        df_not_reid_cached,\n",
    "        info[\"merge_cols\"],\n",
    "        selected,\n",
    "        info[\"identifier_comps\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Combined Reidentification (greedy ∪ fuzzy):\n",
      "Total not re-identified individuals: 889\n",
      "Total Unique Reidentified Individuals: 0\n",
      "Combined Reidentification Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "if selected == \"fuzzy_and_greedy\":\n",
    "    # Extract UIDs from both methods\n",
    "    uids_greedy = set(reidentified[\"greedy\"][\"uid\"])\n",
    "    uids_fuzzy = set(reidentified[\"fuzzy\"][\"uid\"])\n",
    "\n",
    "    # Combine them\n",
    "    combined_uids = uids_greedy.union(uids_fuzzy)\n",
    "    total_reidentified_combined = len(combined_uids)\n",
    "\n",
    "    # Get not re-identified count\n",
    "    df_not_reid_cached = get_not_reidentified_df(data_dir, identifier)\n",
    "    len_not_reidentified = len(df_not_reid_cached)\n",
    "\n",
    "    # Compute rate\n",
    "    reidentification_rate_combined = (total_reidentified_combined / len_not_reidentified) * 100\n",
    "\n",
    "    # Print\n",
    "    print(\"\\n🔁 Combined Reidentification (greedy ∪ fuzzy):\")\n",
    "    print(f\"Total not re-identified individuals: {len_not_reidentified}\")\n",
    "    print(f\"Total Unique Reidentified Individuals: {total_reidentified_combined}\")\n",
    "    print(f\"Combined Reidentification Rate: {reidentification_rate_combined:.2f}%\")\n",
    "\n",
    "    # Save UIDs to CSV\n",
    "    save_dir = os.path.join(save_to, \"re_identification_results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame({\"uid\": list(combined_uids)}).to_csv(\n",
    "        os.path.join(save_dir, \"result_fuzzy_and_greedy.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Save summary to TXT\n",
    "    summary_path = os.path.join(save_dir, \"summary_fuzzy_and_greedy.txt\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        f.write(\"Reidentification Method: fuzzy_and_greedy\\n\")\n",
    "        f.write(f\"Total not re-identified individuals: {len_not_reidentified}\\n\")\n",
    "        f.write(f\"Total Unique Reidentified Individuals: {total_reidentified_combined}\\n\")\n",
    "        f.write(f\"Combined Reidentification Rate: {reidentification_rate_combined:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_refinement_and_reconstruction = time.time() - start_refinement_and_reconstruction\n",
    "    elapsed_total = time.time() - start_total\n",
    "    save_dea_runtime_log(\n",
    "        elapsed_gma=elapsed_gma,\n",
    "        elapsed_hyperparameter_optimization=elapsed_hyperparameter_optimization,\n",
    "        elapsed_model_training=elapsed_model_training,\n",
    "        elapsed_application_to_encoded_data=elapsed_application_to_encoded_data,\n",
    "        elapsed_refinement_and_reconstruction=elapsed_refinement_and_reconstruction,\n",
    "        elapsed_total=elapsed_total,\n",
    "        output_dir=save_to\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
