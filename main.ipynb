{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL): Investigating Dataset Extension Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print('System Version:', sys.version)\n",
    "print('PyTorch version', torch.__version__)\n",
    "print('Torchvision version', torchvision.__version__)\n",
    "print('Numpy version', np.__version__)\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./graphMatching/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloomFilterDataset(Dataset):\n",
    "    def __init__(self, data_dir, isLabeled=False, all_two_grams=None):\n",
    "        self.isLabeled = isLabeled\n",
    "        self.allTwoGrams = all_two_grams\n",
    "        self.data = pd.read_csv(data_dir, sep='\\t')  # Load TSV data\n",
    "        if self.isLabeled:\n",
    "            # For reidentified data, extract labels (2-grams) from names\n",
    "            self.data['label'] = self.data.apply(lambda row: self.extract_two_grams(row['surname'], row['firstname']), axis=1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        bloom_filter = self.data.iloc[idx]['bloomfilter']\n",
    "        bloom_filter_tensor = self.bloomfilter_to_tensor(bloom_filter)\n",
    "\n",
    "        if self.isLabeled:\n",
    "            label = self.data.iloc[idx]['label']\n",
    "            label_tensor = self.label_to_tensor(label)\n",
    "            return bloom_filter_tensor, label_tensor\n",
    "        else:\n",
    "            # For unlabeled data (not_reidentified_individuals.tsv), just return the Bloom Filter\n",
    "            return bloom_filter_tensor\n",
    "    \n",
    "    def extract_two_grams(self, surname, firstname):\n",
    "        full_name = f\"{surname} {firstname}\".replace('\"', '').replace('.', '').replace('/', '').strip()\n",
    "        full_name = full_name.lower()  # Normalize to lowercase for consistency\n",
    "        return [full_name[i:i+2] for i in range(len(full_name)-1) if ' ' not in full_name[i:i+2]]\n",
    "    \n",
    "    def bloomfilter_to_tensor(self, bloom_filter_str):\n",
    "        bloom_filter_array = np.array([int(bit) for bit in bloom_filter_str], dtype=np.float32)\n",
    "        return torch.tensor(bloom_filter_array)\n",
    "    \n",
    "    def label_to_tensor(self, label_two_grams):\n",
    "        label_vector = np.zeros(len(self.allTwoGrams), dtype=np.float32)\n",
    "        \n",
    "        # Set 1 for the 2-grams present in the name\n",
    "        for gram in label_two_grams:\n",
    "            if gram in self.allTwoGrams:\n",
    "                index = self.allTwoGrams.index(gram)\n",
    "                label_vector[index] = 1\n",
    "        \n",
    "        return torch.tensor(label_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate all 2-grams\n",
    "alphabet = string.ascii_lowercase\n",
    "all_two_grams = []\n",
    "for a in alphabet:\n",
    "    for b in alphabet:\n",
    "        all_two_grams.append(a+b)\n",
    "# Get a dictionary associating each 2-gram with an index\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}\n",
    "# Create Datasets\n",
    "data_labeled = BloomFilterDataset('./data/eves_information/reidentified_individuals.tsv', isLabeled=True, all_two_grams=all_two_grams)\n",
    "data_not_labeled = BloomFilterDataset('./data/eves_information/not_reidentified_individuals.tsv', isLabeled=False, all_two_grams=all_two_grams)\n",
    "bloomfilter_length = len(data_labeled[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split proportions\n",
    "train_size = int(0.8 * len(data_labeled))  # 80% training\n",
    "val_size = len(data_labeled) - train_size  # 20% validation\n",
    "\n",
    "# Split dataset\n",
    "data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "dataloader_train = DataLoader(data_train, batch_size=16, shuffle=True)\n",
    "dataloader_val = DataLoader(data_val, batch_size=16, shuffle=True)\n",
    "dataloader_test = DataLoader(data_not_labeled, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloomFilterToTwoGramClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_two_grams=676):\n",
    "        super(BloomFilterToTwoGramClassifier, self).__init__()\n",
    "        \n",
    "        # Define the layers for multi-label classification of 2-grams\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),   # Input => first hidden layer\n",
    "            nn.ReLU(),                       \n",
    "            nn.Dropout(0.1),                  \n",
    "            nn.Linear(hidden_dim, hidden_dim),  # First hidden layer => second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, num_two_grams), # Second hidden layer => output layer\n",
    "            nn.Sigmoid()                       \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the model\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BloomFilterToTwoGramClassifier(input_dim=bloomfilter_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function for multi-label classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for bloom_filters, labels in tqdm(dataloader_train, desc=\"Training loop\"):\n",
    "        # Move data to device\n",
    "        bloom_filters, labels = bloom_filters.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(bloom_filters)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    train_loss = running_loss / len(dataloader_train.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for bloom_filters, labels in tqdm(dataloader_val, desc=\"Validation loop\"):\n",
    "            # Move data to device\n",
    "            bloom_filters, labels = bloom_filters.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(bloom_filters)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "        val_loss = running_loss / len(dataloader_val.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First entry for reference (labeled data):\n",
    "# surname                                                  Hegarty\n",
    "# firstname                                    Miss. Hanora \"Nora\"\n",
    "# bloomfilter    0000000010100011000100000101000000000000100000001000000000100111011000001001100100000100000000001000000000000000001000100000000000010010000000000000100010001110110111000000000000100000000100000001010000000000100101000011000010001010000001000000000000000000001000011010011001000100000011100100000000000011000100100000110011000000000010000000000010000000000000110000000110000000000010000000011100000001000000100000001100101011001000000000010000001000000000001000010110110000000001001000100001010111010000000010000000111000000000010010110000000000001000000101010001000000001000001000010000100110000111001110000000001010011110000100000000000100000001100001100000000000010000000000000000000000100000000010000001000000000011100000000000001000101000010100001001000011000000000010001100000000100000001000001000000000100000101000000000000000000010000000100000000100001000000100000000000000011100000001001000000001100010000001000001000000000000010100100000000110101110010000010000010100000000011000001000000001110000101001000010101111\n",
    "# uid                                                          654\n",
    "# Name: 0, dtype: object\n",
    "#print('Length Labeled data:', len(data_labeled))\n",
    "#print('Length Unlabeled data:', len(data_not_labeled))\n",
    "\n",
    "#bloomfilter_tensor, label_tensor = data_labeled[0]\n",
    "\n",
    "#print('Bloom Filter Tensor:', bloomfilter_tensor)\n",
    "#print('Bloom Filter Tensor Shape:', bloomfilter_tensor.shape)\n",
    "#print('Label Tensor:', label_tensor)\n",
    "#print('Label Tensor Shape:', label_tensor.shape)\n",
    "\n",
    "#for bloomfilter_tensors, label_tensors in dataloader_train:\n",
    "#    print('Bloom Filter Tensor Shape:', bloomfilter_tensors.shape)\n",
    "#    print('Label Tensor Shape:', label_tensors.shape)\n",
    "#    print(label_tensors)\n",
    "#    break\n",
    "\n",
    "#print(str(model)[:500])\n",
    "#example_bloom, example_label = data_train[0]\n",
    "#example_out = model(example_bloom)\n",
    "#print(example_out.shape) \n",
    "#loss_function_applied = criterion(example_out, example_label)\n",
    "#print(loss_function_applied)\n",
    "#print(example_out)\n",
    " \n",
    "# Apply model\n",
    "bloomfilter_tensor = data_not_labeled[0]\n",
    "result = model(bloomfilter_tensor)\n",
    "# Result = Tensor of shape 676 with prob. for each 2gram\n",
    "two_gram_scores = {two_gram_dict[i]: score.item() for i, score in enumerate(result)}\n",
    "\n",
    "threshold = 0.000000001\n",
    "filtered_two_gram_scores = {two_gram: score for two_gram, score in two_gram_scores.items() if score > threshold}\n",
    "filtered_two_gram_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
