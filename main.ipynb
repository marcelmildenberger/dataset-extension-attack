{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL): Investigating Dataset Extension Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Imports\n",
    "\n",
    "#### PyTorch\n",
    "- `torch`, `torch.nn`, `torch.optim`, `DataLoader`: For building, training, and evaluating neural networks for DEA.\n",
    "- `SummaryWriter`: Logs training metrics for visualization in TensorBoard.\n",
    "\n",
    "#### Ray\n",
    "- `tune`, `air`, `train`, `OptunaSearch`, `ASHAScheduler`: Used for distributed hyperparameter tuning and model optimization.\n",
    "\n",
    "#### Data Handling & Visualization\n",
    "- `pandas`, `numpy`, `matplotlib.pyplot`, `seaborn`: For data manipulation, analysis, and plotting.\n",
    "- `hickle`: Efficient binary serialization format for NumPy arrays and Python objects.\n",
    "- `tqdm.notebook`: Progress bars for loops, especially in Jupyter notebooks.\n",
    "\n",
    "#### Custom Modules\n",
    "- `utils`: A comprehensive set of utility functions for DEA-specific tasks like reconstruction and result logging.\n",
    "- `datasets`: Dataset wrappers for different encoding schemes (Bloom Filter, Tab MinHash, Two-Step Hash).\n",
    "- `pytorch_models`, `early_stopping`: Custom PyTorch model definitions and early stopping mechanism.\n",
    "- `graphMatching.gma`: Executes Graph Matching Attack (GMA) to prepare DEA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ray libraries for hyperparameter tuning and parallelism\n",
    "import ray\n",
    "from ray import air, train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "# Custom utilities and logic\n",
    "from early_stopping.early_stopping import EarlyStopping\n",
    "from graphMatching.gma import run_gma\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset import TwoStepHashDataset\n",
    "from pytorch_models.base_model import BaseModel\n",
    "from pytorch_models_hyperparameter_optimization.base_model_hyperparameter_optimization import BaseModelHyperparameterOptimization\n",
    "from utils import (\n",
    "    calculate_performance_metrics,\n",
    "    clean_result_dict,\n",
    "    create_identifier_column_dynamic,\n",
    "    decode_labels_to_two_grams,\n",
    "    filter_high_scoring_two_grams,\n",
    "    fuzzy_reconstruction_approach,\n",
    "    get_hashes,\n",
    "    greedy_reconstruction,\n",
    "    load_dataframe,\n",
    "    lowercase_df,\n",
    "    map_probabilities_to_two_grams,\n",
    "    metrics_per_entry,\n",
    "    print_and_save_result,\n",
    "    read_header,\n",
    "    reconstruct_identities_with_llm,\n",
    "    reidentification_analysis,\n",
    "    resolve_config,\n",
    "    run_epoch,\n",
    "    save_dea_runtime_log,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Overview\n",
    "\n",
    "#### GLOBAL_CONFIG\n",
    "General control parameters for DEA runs.\n",
    "- `Data`: Path to dataset (e.g., fakename_5k.tsv).\n",
    "- `Overlap`: Proportion of shared entities between Alice and Eve.\n",
    "- `DropFrom`: Which party (Alice, Eve or both) gets the non-overlapping entities removed.\n",
    "- `MatchingMetric`, `Matching`: Used in GMA (e.g., cosine similarity, MinWeight matching).\n",
    "- `Workers`: Number of parallel threads (-1 = all available).\n",
    "- `BenchMode`, `DevMode`: Toggle benchmarking or development behaviors.\n",
    "\n",
    "#### DEA_CONFIG\n",
    "Training configuration for the neural network used in the Dataset Extension Attack.\n",
    "- `TrainSize`, `Epochs`, `Patience`: Classic train/test split and early stopping.\n",
    "- `NumSamples`: Number of tuning samples for Ray.\n",
    "- `MetricToOptimize`: Metric guiding model selection (e.g., `average_dice`).\n",
    "- `MatchingTechnique`: Post-processing method (e.g.,`fuzzy`, `greedy`, `ai`, `fuzzy_and_greedy`).\n",
    "\n",
    "#### ENC_CONFIG\n",
    "Controls how both Alice’s and Eve’s data are encoded.\n",
    "- `AliceAlgo`, `EveAlgo`: Chosen encoding methods (BloomFilter, TabMinHash, TwoStepHash).\n",
    "- Parameters are grouped by technique (BF, TMH, 2SH), but all are present to allow switching.\n",
    "\n",
    "#### EMB_CONFIG\n",
    "Defines embedding model (e.g., Node2Vec) parameters for both parties.\n",
    "- `Dim`, `Context`, `Negative`: Node2Vec embedding dimensions and context window.\n",
    "- `WalkLen`, `NWalks`, `P`, `Q`: Random walk hyperparameters.\n",
    "- `Quantile`, `Discretize`, `Normalize`: Post-embedding processing.\n",
    "\n",
    "#### ALIGN_CONFIG\n",
    "Parameters for alignment-based reconstruction.\n",
    "- `RegWS`, `RegInit`: Regularization weights.\n",
    "- `NIterWS`, `NIterInit`, `NEpochWS`: Iteration limits.\n",
    "- `Wasserstein`: Use Wasserstein-based alignment loss.\n",
    "- `EarlyStopping`, `LRDecay`: Learning stability controls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General Parameters ===\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/euro_person_5k.tsv\",\n",
    "    \"Overlap\": 0.8,\n",
    "    \"DropFrom\": \"Eve\",\n",
    "    \"Verbose\": False,\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": os.cpu_count() - 1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": False,\n",
    "    \"BenchMode\": True,\n",
    "    \"LoadResults\": False,\n",
    "    \"LoadPath\": \"\",\n",
    "    \"SaveResults\": True,\n",
    "}\n",
    "\n",
    "# === DEA Training Parameters ===\n",
    "DEA_CONFIG = {\n",
    "    \"TrainSize\": 0.8,\n",
    "    \"Patience\": 5,\n",
    "    \"MinDelta\": 1e-4,\n",
    "    \"NumSamples\": 30,\n",
    "    \"Epochs\": 20,\n",
    "    \"MetricToOptimize\": \"average_dice\",  # Options: \"average_dice\", \"average_precision\", ...\n",
    "    # Fuzyy works only if first three columns resemble: givenname, surname, birthdate (dd/mm/yyyy) format (naming of columns is irellevant)\n",
    "    \"MatchingTechnique\": \"fuzzy_and_greedy\",  # Options: \"ai\", \"greedy\", \"fuzzy\", ...\n",
    "}\n",
    "\n",
    "# === Encoding Parameters for Alice & Eve ===\n",
    "ENC_CONFIG = {\n",
    "    # Encoding technique\n",
    "    \"AliceAlgo\": \"BloomFilter\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"BloomFilter\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "\n",
    "    # Bloom Filter specific\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "\n",
    "    # Tabulation MinHash specific\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "\n",
    "    # Two-Step Hashing specific\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "# === Embedding Configuration (e.g., Node2Vec) ===\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42,\n",
    "}\n",
    "\n",
    "# === Graph Alignment Config ===\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"] / 2),\n",
    "    \"RegInit\": 1,\n",
    "    \"Batchsize\": 1,\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5,\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define character sets\n",
    "alphabet = string.ascii_lowercase  # 'a' to 'z'\n",
    "digits = string.digits             # '0' to '9'\n",
    "\n",
    "# Generate 2-grams\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]   # 'aa' to 'zz'\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]     # '00' to '99'\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]      # 'a0' to 'z9'\n",
    "\n",
    "# Combine all 2-gram types\n",
    "all_two_grams = letter_letter_grams + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Map index to 2-gram string\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load or Compute Graph Matching Attack (GMA) Results\n",
    "\n",
    "This step ensures GMA results are available by either loading existing output files or triggering a new attack run.\n",
    "\n",
    "1. **Start Benchmark Timing (Optional):**  \n",
    "   If benchmarking is enabled (`BenchMode=True`), timers are started to measure runtime for the GMA computation.\n",
    "\n",
    "2. **Generate Configuration Hashes:**  \n",
    "   The function `get_hashes()` generates unique identifiers (hashes) based on the encoding (`ENC_CONFIG`) and embedding (`EMB_CONFIG`) settings for both Alice and Eve.  \n",
    "   These hashes are concatenated into a unique string identifier to distinguish different experiment setups.\n",
    "\n",
    "3. **Construct File Paths:**  \n",
    "   The identifier is used to define expected file paths for:\n",
    "   - `reidentified_individuals.h5` — records successfully linked between Eve and Alice  \n",
    "   - `not_reidentified_individuals.h5` — records not matched  \n",
    "   - `alice_data_complete_with_encoding.h5` — full encoded data for Alice  \n",
    "\n",
    "4. **Check for Existing Results:**  \n",
    "   If all three expected files exist, the GMA step is skipped. This avoids redundant computation.\n",
    "\n",
    "5. **Run GMA if Needed:**  \n",
    "   If any file is missing, the Graph Matching Attack is executed using `run_gma()`, which writes the results to disk using the same naming convention based on the identifier.\n",
    "\n",
    "6. **End Benchmark Timing (Optional):**  \n",
    "   If benchmarking is active, the elapsed time for the GMA step is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2 of 290\n",
      "Success rate: 0.006897\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_total = time.time()\n",
    "    start_gma = time.time()\n",
    "\n",
    "# Get absolute path to data directory\n",
    "data_dir = os.path.abspath(\"./data\")\n",
    "\n",
    "# Generate encoding and embedding hashes for reproducible identifiers\n",
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(\n",
    "    GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG\n",
    ")\n",
    "identifier = f\"{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}\"\n",
    "\n",
    "# Build file paths for reidentified, not reidentified, and full encoded data\n",
    "path_reidentified = f\"{data_dir}/available_to_eve/reidentified_individuals_{identifier}.h5\"\n",
    "path_not_reidentified = f\"{data_dir}/available_to_eve/not_reidentified_individuals_{identifier}.h5\"\n",
    "path_all = f\"{data_dir}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\"\n",
    "\n",
    "# Run GMA only if the expected output files do not yet exist\n",
    "if not (\n",
    "    os.path.isfile(path_reidentified)\n",
    "    and os.path.isfile(path_not_reidentified)\n",
    "    and os.path.isfile(path_all)\n",
    "):\n",
    "    run_gma(\n",
    "        GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG,\n",
    "        eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash\n",
    "    )\n",
    "\n",
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_gma = time.time() - start_gma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Training, Validation, and Test Data\n",
    "\n",
    "This step prepares the datasets needed for training and evaluation of the Dataset Extension Attack (DEA).  \n",
    "Depending on the selected encoding algorithm and available data, different dataset loaders are used.\n",
    "\n",
    "1. **Start Benchmark Timing (Optional):**  \n",
    "   If `BenchMode` is enabled, a timer is started to track the time spent in data preparation.\n",
    "\n",
    "2. **Load Reidentified and Not Reidentified Data:**  \n",
    "   - `reidentified_individuals_*.h5`: Contains entities successfully linked by the Graph Matching Attack.\n",
    "   - `not_reidentified_individuals_*.h5` and `alice_data_complete_with_encoding_*.h5`:  \n",
    "     Used to construct a labeled test set of entities not reidentified by the GMA.\n",
    "\n",
    "3. **Select Dataset Type Based on Encoding:**\n",
    "   - If `AliceAlgo` is set to `\"BloomFilter\"`: `BloomFilterDataset` is used.\n",
    "   - If `TabMinHash`: `TabMinHashDataset` is used.\n",
    "   - If `TwoStepHash`: `TwoStepHashDataset` is used with integer feature vectors.\n",
    "\n",
    "4. **Split Labeled Data into Train and Validation Sets:**  \n",
    "   The reidentified dataset is split according to `DEA_CONFIG[\"TrainSize\"]`.\n",
    "\n",
    "5. **Return Datasets:**  \n",
    "   The function returns:\n",
    "   - `data_train`: for model training  \n",
    "   - `data_val`: for validation during training  \n",
    "   - `data_test` (optional): for evaluating the reconstruction on not reidentified entities\n",
    "\n",
    "6. **Drop Redundant Columns (if needed):**  \n",
    "   The function `load_not_reidentified_data()` ensures compatibility of test data by removing unnecessary columns.\n",
    "\n",
    "7. **End Benchmark Timing (Optional):**  \n",
    "   If benchmarking is active, the elapsed time for data preparation is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_path(data_directory, identifier, alice_enc_hash, name=\"dataset\"):\n",
    "    os.makedirs(f\"{data_directory}/cache\", exist_ok=True)\n",
    "    return os.path.join(data_directory, \"cache\", f\"{name}_{identifier}_{alice_enc_hash}.pkl\")\n",
    "\n",
    "def load_data(data_directory, alice_enc_hash, identifier, load_test=False):\n",
    "    cache_path = get_cache_path(data_directory, identifier, alice_enc_hash)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            data_train, data_val, data_test = pickle.load(f)\n",
    "        return data_train, data_val, data_test\n",
    "\n",
    "    # Load from raw files\n",
    "    df_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/reidentified_individuals_{identifier}.h5\")\n",
    "\n",
    "    df_test = None\n",
    "    if load_test:\n",
    "        df_not_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/not_reidentified_individuals_{identifier}.h5\")\n",
    "        df_all = load_dataframe(f\"{data_directory}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\")\n",
    "        df_test = df_all[df_all[\"uid\"].isin(df_not_reidentified[\"uid\"])].reset_index(drop=True)\n",
    "\n",
    "    def get_encoding_dataset_class():\n",
    "        algo = ENC_CONFIG[\"AliceAlgo\"]\n",
    "        if algo == \"BloomFilter\":\n",
    "            return BloomFilterDataset\n",
    "        elif algo == \"TabMinHash\":\n",
    "            return TabMinHashDataset\n",
    "        elif algo == \"TwoStepHash\":\n",
    "            return TwoStepHashDataset\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoding algorithm: {algo}\")\n",
    "\n",
    "    DatasetClass = get_encoding_dataset_class()\n",
    "\n",
    "    if ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\":\n",
    "        unique_ints = sorted(set().union(*df_reidentified[\"twostephash\"]))\n",
    "        dataset_args = {\"all_integers\": unique_ints}\n",
    "    else:\n",
    "        dataset_args = {}\n",
    "\n",
    "    common_args = {\n",
    "        \"is_labeled\": True,\n",
    "        \"all_two_grams\": all_two_grams,\n",
    "        \"dev_mode\": GLOBAL_CONFIG[\"DevMode\"]\n",
    "    }\n",
    "\n",
    "    data_labeled = DatasetClass(df_reidentified, **common_args, **dataset_args)\n",
    "    data_test = DatasetClass(df_test, **common_args, **dataset_args) if load_test else None\n",
    "\n",
    "    train_size = int(DEA_CONFIG[\"TrainSize\"] * len(data_labeled))\n",
    "    val_size = len(data_labeled) - train_size\n",
    "    data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "\n",
    "    # Save to cache\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump((data_train, data_val, data_test), f)\n",
    "    return data_train, data_val, data_test\n",
    "\n",
    "\n",
    "def load_not_reidentified_data(data_directory, alice_enc_hash, identifier):\n",
    "        cache_path = get_cache_path(data_directory, identifier, alice_enc_hash, name=\"not_reidentified\")\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                df_filtered = pickle.load(f)\n",
    "            return df_filtered\n",
    "\n",
    "        df_not_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/not_reidentified_individuals_{identifier}.h5\")\n",
    "        df_all = load_dataframe(f\"{data_directory}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\")\n",
    "\n",
    "        df_filtered = df_all[df_all[\"uid\"].isin(df_not_reidentified[\"uid\"])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        # Drop column by name instead of position if possible\n",
    "        drop_col = df_filtered.columns[-2]\n",
    "        df_filtered = df_filtered.drop(columns=[drop_col])\n",
    "\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(df_filtered, f)\n",
    "\n",
    "        return df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = GLOBAL_CONFIG[\"Data\"].split(\"/\")[-1].replace(\".tsv\", \"\")\n",
    "experiment_tag = \"experiment_\" + ENC_CONFIG[\"AliceAlgo\"] + \"_\" + selected_dataset + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_to = f\"experiment_results/{experiment_tag}\"\n",
    "os.makedirs(save_to, exist_ok=True)\n",
    "\n",
    "# Combine all configs into one dictionary\n",
    "all_configs = {\n",
    "    \"GLOBAL_CONFIG\": GLOBAL_CONFIG,\n",
    "    \"DEA_CONFIG\": DEA_CONFIG,\n",
    "    \"ENC_CONFIG\": ENC_CONFIG,\n",
    "    \"EMB_CONFIG\": EMB_CONFIG,\n",
    "    \"ALIGN_CONFIG\": ALIGN_CONFIG\n",
    "}\n",
    "\n",
    "# Save as a readable .txt file\n",
    "with open(os.path.join(save_to, \"config.txt\"), \"w\") as f:\n",
    "    for config_name, config_dict in all_configs.items():\n",
    "        f.write(f\"# === {config_name} ===\\n\")\n",
    "        f.write(json.dumps(config_dict, indent=4))\n",
    "        f.write(\"\\n\\n\")\n",
    "os.makedirs(f\"{save_to}/hyperparameteroptimization\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val, data_test = load_data(data_dir, alice_enc_hash, identifier, load_test=True)\n",
    "df_not_reidentified = load_not_reidentified_data(data_dir, alice_enc_hash, identifier)\n",
    "# Exit the function if any of the data frames are empty\n",
    "if len(data_train) == 0 or len(data_val) == 0 or len(data_test) == 0 or df_not_reidentified.empty:\n",
    "    log_path = os.path.join(save_to, \"termination_log.txt\")\n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(\"Training process canceled due to empty dataset.\\n\")\n",
    "        f.write(f\"Length of data_train: {len(data_train)}\\n\")\n",
    "        f.write(f\"Length of data_val: {len(data_val)}\\n\")\n",
    "        f.write(f\"Length of data_test: {len(data_test)}\\n\")\n",
    "        f.write(f\"Length of df_not_reidentified: {len(df_not_reidentified)}\\n\")\n",
    "    print(\"One or more datasets are empty. Termination log written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_hyperparameter_optimization = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, data_dir, output_dim, alice_enc_hash, identifier, patience, min_delta):\n",
    "    # Create DataLoaders for training, validation, and testing\n",
    "\n",
    "    data_train, data_val, _ = load_data(data_dir, alice_enc_hash, identifier, load_test=False)\n",
    "\n",
    "    input_dim = data_train[0][0].shape[0]  # Get the input dimension from the first sample\n",
    "\n",
    "    dataloader_train = DataLoader(\n",
    "        data_train,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,  # Important for training\n",
    "    )\n",
    "\n",
    "    dataloader_val = DataLoader(\n",
    "        data_val,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    total_precision = total_recall = total_f1 = total_dice = total_val_loss = 0.0\n",
    "    num_samples = 0\n",
    "    epochs = 0\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "\n",
    "    # Define and initialize model with hyperparameters from config\n",
    "    model = BaseModelHyperparameterOptimization(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        activation_fn=config[\"activation_fn\"]\n",
    "    )\n",
    "\n",
    "    # Set device for model (GPU or CPU)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Select loss function based on config\n",
    "    loss_functions = {\n",
    "        \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss(),\n",
    "        \"MultiLabelSoftMarginLoss\": nn.MultiLabelSoftMarginLoss(),\n",
    "        \"SoftMarginLoss\": nn.SoftMarginLoss(),\n",
    "    }\n",
    "    criterion = loss_functions[config[\"loss_fn\"]]\n",
    "\n",
    "    learning_rate = config[\"optimizer\"][\"lr\"].sample()\n",
    "    # Select optimizer based on config\n",
    "    optimizers = {\n",
    "        \"Adam\": lambda: optim.Adam(model.parameters(), lr=learning_rate),\n",
    "        \"AdamW\": lambda: optim.AdamW(model.parameters(), lr=learning_rate),\n",
    "        \"SGD\": lambda: optim.SGD(model.parameters(), lr=learning_rate, momentum=config[\"optimizer\"][\"momentum\"].sample()),\n",
    "        \"RMSprop\": lambda: optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    }\n",
    "    optimizer = optimizers[config[\"optimizer\"][\"name\"]]()\n",
    "\n",
    "    schedulers = {\n",
    "        \"StepLR\": lambda: torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config[\"lr_scheduler\"][\"step_size\"].sample(),\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ExponentialLR\": lambda: torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ReduceLROnPlateau\": lambda: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=config[\"lr_scheduler\"][\"mode\"],\n",
    "            factor=config[\"lr_scheduler\"][\"factor\"].sample(),\n",
    "            patience=config[\"lr_scheduler\"][\"patience\"].sample()\n",
    "        ),\n",
    "        \"CosineAnnealingLR\": lambda: torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config[\"lr_scheduler\"][\"T_max\"].sample(),\n",
    "            eta_min=config[\"lr_scheduler\"][\"eta_min\"].sample()\n",
    "        ),\n",
    "        \"CyclicLR\": lambda: torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=config[\"lr_scheduler\"][\"base_lr\"].sample(),\n",
    "            max_lr=config[\"lr_scheduler\"][\"max_lr\"].sample(),\n",
    "            step_size_up=config[\"lr_scheduler\"][\"step_size_up\"].sample(),\n",
    "            mode=config[\"lr_scheduler\"][\"mode_cyclic\"].sample(),\n",
    "            cycle_momentum=False\n",
    "        ),\n",
    "        \"None\": lambda: None,\n",
    "    }\n",
    "    scheduler = schedulers[config[\"lr_scheduler\"][\"name\"]]()\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(DEA_CONFIG[\"Epochs\"]):\n",
    "        epochs += 1\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = run_epoch(model, dataloader_train, criterion, optimizer, device, is_training=True, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(model, dataloader_val, criterion, optimizer, device, is_training=False, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        total_val_loss += val_loss\n",
    "\n",
    "         # Early stopping check\n",
    "        if early_stopper(val_loss):\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    # Test phase with reconstruction and evaluation\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in dataloader_val:\n",
    "\n",
    "            actual_two_grams = decode_labels_to_two_grams(two_gram_dict, labels)\n",
    "\n",
    "            # Move data to device and make predictions\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities into 2-gram scores\n",
    "            batch_two_gram_scores = map_probabilities_to_two_grams(two_gram_dict, probabilities)\n",
    "\n",
    "            # Filter out low-scoring 2-grams\n",
    "            batch_filtered_two_gram_scores = filter_high_scoring_two_grams(batch_two_gram_scores, config[\"threshold\"])\n",
    "\n",
    "            # Calculate performance metrics for evaluation\n",
    "            dice, precision, recall, f1 = calculate_performance_metrics(\n",
    "                actual_two_grams, batch_filtered_two_gram_scores)\n",
    "\n",
    "            total_dice += dice\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "            num_samples += data.size(0) # Batch Size\n",
    "\n",
    "    train.report({\n",
    "            \"average_dice\": total_dice / num_samples,\n",
    "            \"average_precision\": total_precision / num_samples,\n",
    "            \"average_recall\": total_recall / num_samples,\n",
    "            \"average_f1\": total_f1 / num_samples,\n",
    "            \"total_val_loss\": total_val_loss,\n",
    "            \"len_train\": len(dataloader_train.dataset),\n",
    "            \"len_val\": len(dataloader_val.dataset),\n",
    "            \"epochs\": epochs\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-07-11 11:58:33</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:59.41        </td></tr>\n",
       "<tr><td>Memory:      </td><td>19.1/32.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=112<br>Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: -4.149024844169617<br>Logical resource usage: 5.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th>activation_fn  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_layer_size</th><th>loss_fn             </th><th>lr_scheduler/T_max  </th><th>lr_scheduler/base_lr  </th><th>lr_scheduler/eta_min  </th><th>lr_scheduler/factor  </th><th>lr_scheduler/gamma  </th><th>lr_scheduler/max_lr  </th><th>lr_scheduler/mode  </th><th>lr_scheduler/mode_cy\n",
       "clic                     </th><th>lr_scheduler/name  </th><th>lr_scheduler/patienc\n",
       "e                     </th><th>lr_scheduler/step_si\n",
       "ze                     </th><th>lr_scheduler/step_si\n",
       "ze_up                     </th><th style=\"text-align: right;\">  num_layers</th><th>optimizer/lr        </th><th>optimizer/momentum  </th><th>optimizer/name  </th><th style=\"text-align: right;\">  threshold</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  average_dice</th><th style=\"text-align: right;\">  average_precision</th><th style=\"text-align: right;\">  average_recall</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_ad3f4efc</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.171471</td><td style=\"text-align: right;\">                512</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_7250</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_6020</td><td>&lt;ray.tune.searc_5f00</td><td>SGD             </td><td style=\"text-align: right;\">   0.439164</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       1.61152  </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_90fba646</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.323835</td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_77c0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_6e90</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_7910</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.519637</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.175573 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_50740b49</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.314265</td><td style=\"text-align: right;\">                512</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_56f0</td><td>                      </td><td>&lt;ray.tune.searc_4ee0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_7b20</td><td>&lt;ray.tune.searc_4b80</td><td>SGD             </td><td style=\"text-align: right;\">   0.698838</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.0692286</td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_7a9199bc</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.345289</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_1c90 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_31c0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_3520</td><td>&lt;ray.tune.searc_12d0</td><td>SGD             </td><td style=\"text-align: right;\">   0.69074 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.115052 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_9e8a81cc</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.325462</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_f820</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_fb20</td><td>&lt;ray.tune.searc_d4e0</td><td>SGD             </td><td style=\"text-align: right;\">   0.631077</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.10661  </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_3ab10f89</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.324725</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_69e0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.559094</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.839456 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_bc918a6c</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.153005</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_1cc0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_28f0</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_1c60</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.344246</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.258232 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_6b21eb07</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.245962</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_53c0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.693008</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.327983 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_58560dc3</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.161832</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_1600</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_1270</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_7cd0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.589088</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.244641 </td><td style=\"text-align: right;\">        0.125 </td><td style=\"text-align: right;\">          0.125    </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_a53fe934</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.312499</td><td style=\"text-align: right;\">                256</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>&lt;ray.tune.searc_69b0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_4160 </td><td>                   </td><td>&lt;ray.tune.searc_6b60</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_4c10</td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_8430</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.635688</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.124098 </td><td style=\"text-align: right;\">        0.1143</td><td style=\"text-align: right;\">          0.105263 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_4e2b868f</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.114152</td><td style=\"text-align: right;\">                512</td><td>SoftMarginLoss      </td><td>&lt;ray.tune.searc_9870</td><td>                      </td><td>&lt;ray.tune.searc_31c0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_0d00</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.693285</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.091327 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_621b0d10</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.100695</td><td style=\"text-align: right;\">                128</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_a7d0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_b160</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_b100</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.792418</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.0868847</td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_702034fd</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.390823</td><td style=\"text-align: right;\">                128</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_55d0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.529118</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.103905 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_c30e1fe0</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.395339</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_d060</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.52719 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.415    </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_faf2e393</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.239868</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_f220</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.469579</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.553669 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_2a37499a</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.238304</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_9750 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_93f0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_98d0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.426761</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.349333 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_2f13e5fe</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.19753 </td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_5720 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_5210</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6260</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.600667</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.359701 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_c6162e1f</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.192641</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_5180  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_41f0 </td><td>                   </td><td>&lt;ray.tune.searc_53c0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_68f0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_4a30</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.618706</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.283833 </td><td style=\"text-align: right;\">        0.1212</td><td style=\"text-align: right;\">          0.117647 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_15de6210</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.267669</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_47c0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_4640 </td><td>                   </td><td>&lt;ray.tune.searc_6590</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_44c0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_4940</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.318346</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.292032 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_2971a206</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.202221</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_ab00  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_a5f0 </td><td>                   </td><td>&lt;ray.tune.searc_a590</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_a620</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_9f90</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.305195</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.217257 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_677399c2</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.185781</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_1ab0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_1660 </td><td>                   </td><td>&lt;ray.tune.searc_36a0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_0940</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_0970</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.771725</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.217193 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_38c1af12</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.141106</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_7850  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_5210 </td><td>                   </td><td>&lt;ray.tune.searc_7a30</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_7970</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_7790</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.766361</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.171183 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_f534e988</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.139991</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_ceb0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_d270</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_cf70</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.587866</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.0956521</td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_77094745</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.274388</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_7c10</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_7a90</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6da0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.591486</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.360527 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_d48de504</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.276804</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_6440</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_c5b0</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_c370</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.571739</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.0640368</td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_6f37894b</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.28572 </td><td style=\"text-align: right;\">                256</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_5ba0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.477807</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.066889 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_2f37bc0c</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.201886</td><td style=\"text-align: right;\">                256</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_7b80</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.485883</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.106163 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_c9db4f2c</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.213366</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_8fd0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_c460</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.638054</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.410677 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_3310e3ce</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.225501</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_94b0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_e4a0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.643794</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.639455 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_81a3720d</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.369147</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_9fc0</td><td>                      </td><td>&lt;ray.tune.searc_b160  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_b670</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.562645</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.212693 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_7e038750</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.167782</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_a380</td><td>                      </td><td>&lt;ray.tune.searc_a140  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_9cf0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.399502</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.216096 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_4baae2f0</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.169166</td><td style=\"text-align: right;\">                512</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>&lt;ray.tune.searc_d000  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_ee30 </td><td>                   </td><td>&lt;ray.tune.searc_ee60</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_e440</td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_a350</td><td>&lt;ray.tune.searc_fd00</td><td>SGD             </td><td style=\"text-align: right;\">   0.42024 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.073848 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_27031a2b</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.388183</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_7be0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.531771</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.797228 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_b4340168</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.393657</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_a020</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.516902</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.661163 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_a6aab840</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.353295</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_7df0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.508584</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.737754 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_62bee38b</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.356576</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_afe0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.505452</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.432039 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_318f2f93</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.338321</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_7a90</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.548209</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.470787 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_caaf7429</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.33294 </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_ff40</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_ff10</td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_f3d0</td><td>&lt;ray.tune.searc_ef80</td><td>SGD             </td><td style=\"text-align: right;\">   0.613484</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.267846 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_949a7c13</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.311559</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_8bb0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_80a0</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a080</td><td>&lt;ray.tune.searc_8130</td><td>SGD             </td><td style=\"text-align: right;\">   0.60829 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.109844 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_6f5e3a79</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.304019</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_1ed0 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_3b80</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_0460</td><td>&lt;ray.tune.searc_3460</td><td>SGD             </td><td style=\"text-align: right;\">   0.729584</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.123441 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_09c84bd2</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.303887</td><td style=\"text-align: right;\">                256</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_6b60 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_4160</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_a170</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.675471</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.125039 </td><td style=\"text-align: right;\">        0.0606</td><td style=\"text-align: right;\">          0.0588235</td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_cb2ae9ed</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.121235</td><td style=\"text-align: right;\">                256</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_5240</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_b850</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.558345</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.0947211</td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_9bb21a5b</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.231193</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_c6a0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.460413</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.543399 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_04ea4269</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.253358</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_7580</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.460156</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.637085 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_2ada8db7</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.256276</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_4520</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.377989</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.462643 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_9f6b4134</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.184237</td><td style=\"text-align: right;\">                128</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_46d0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.666812</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.188386 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_978e621a</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.186406</td><td style=\"text-align: right;\">                128</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_d090</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.540897</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.105882 </td><td style=\"text-align: right;\">        0.1053</td><td style=\"text-align: right;\">          0.0909091</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_34520d92</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.368907</td><td style=\"text-align: right;\">                512</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>&lt;ray.tune.searc_8520  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_a530 </td><td>                   </td><td>&lt;ray.tune.searc_9ab0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_bf40</td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_3e20</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.539148</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.231593 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_67a3c1e0</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.368061</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_ad10  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_b790 </td><td>                   </td><td>&lt;ray.tune.searc_b430</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_a0b0</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_8fa0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.577096</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.191049 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_0c346237</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.149842</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_ab90</td><td>                      </td><td>&lt;ray.tune.searc_b3d0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_91e0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.575993</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.199739 </td><td style=\"text-align: right;\">        0.1224</td><td style=\"text-align: right;\">          0.0909091</td><td style=\"text-align: right;\">          0.1875</td></tr>\n",
       "<tr><td>train_model_d055260c</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.154297</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_1fc0</td><td>                      </td><td>&lt;ray.tune.searc_1d50  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_1270</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.622428</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.311442 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_f76809dc</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.157697</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_9d20</td><td>                      </td><td>&lt;ray.tune.searc_8040  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_bc70</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.617186</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.275651 </td><td style=\"text-align: right;\">        0.087 </td><td style=\"text-align: right;\">          0.142857 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_8c5f48a7</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.129964</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_a920</td><td>                      </td><td>&lt;ray.tune.searc_bf40  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a3e0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.489754</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.62784  </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_6ef23fe5</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.132801</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_f370</td><td>                      </td><td>&lt;ray.tune.searc_e3e0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_db70</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.490537</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.25138  </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_16a45f69</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.210263</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_9b70</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_b3a0</td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_b670</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.593058</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.903867 </td><td style=\"text-align: right;\">        0.125 </td><td style=\"text-align: right;\">          0.125    </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_d0ca5da8</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.221411</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_8f40</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_8d30</td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_8340</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.654807</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.346747 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_5b2c47b7</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.210786</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_96f0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_ba30</td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_1b10</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.655101</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.309759 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_ae12ef45</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.180878</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_bd30</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_86a0</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_d930</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.594696</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.201857 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_00ff4890</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.109055</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_77c0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_4910</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_89a0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.584474</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.209047 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_1f594308</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.19417 </td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_7850</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_4190</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6d10</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.57919 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.1092   </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_8497ae6c</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.149185</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_ee30  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_fe80 </td><td>                   </td><td>&lt;ray.tune.searc_ffd0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_f940</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_f250</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.571832</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.638121 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_ac153e12</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.146032</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_fd90  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_ea70 </td><td>                   </td><td>&lt;ray.tune.searc_dae0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_ff70</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_57e0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.706359</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.569423 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_b4296f14</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.234803</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_30d0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.555789</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.31588  </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_eae9fd88</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.238917</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_ec50</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.550242</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.47785  </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_2134e8cd</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.241422</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_6e00</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_89a0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.52113 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.870829 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_d0ca8f41</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.161053</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_abc0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_9a80</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.520576</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.934938 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_27e02631</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.163139</td><td style=\"text-align: right;\">                128</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_a9b0</td><td>                      </td><td>&lt;ray.tune.searc_bd60  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_aa70</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.631128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.108042 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_8be3d822</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.176086</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_1ab0</td><td>                      </td><td>&lt;ray.tune.searc_d540  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_14e0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.466823</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.17163  </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_d959b280</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.175999</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_9b70</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_b460</td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_b3a0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.455503</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.372609 </td><td style=\"text-align: right;\">        0.1026</td><td style=\"text-align: right;\">          0.0869565</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_05e138c7</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.214227</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_70a0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_7190</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_6ef0</td><td>&lt;ray.tune.searc_7220</td><td>SGD             </td><td style=\"text-align: right;\">   0.441412</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.255382 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_541d00b0</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.208237</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_6e30 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_6aa0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_ded0</td><td>&lt;ray.tune.searc_7f70</td><td>SGD             </td><td style=\"text-align: right;\">   0.566672</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.241784 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_edf07eee</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.381675</td><td style=\"text-align: right;\">                256</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_84f0 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_b0d0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_dff0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.600145</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.105541 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_1bef112f</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.19432 </td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_4f70 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_54b0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_5690</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.600763</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.260956 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_b93ca338</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.20001 </td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_f670 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_ef50</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a350</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.606947</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.259979 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_76ff95dc</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.195981</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_1ed0 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_11b0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_07c0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.618782</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.133026 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_7375787c</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.225642</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6f80</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.627203</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.190269 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_35ba8b42</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.281089</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_40a0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.501513</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.415701 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_279d1d4c</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.290626</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_8250</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.502835</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.888733 </td><td style=\"text-align: right;\">        0.1111</td><td style=\"text-align: right;\">          0.1      </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_88d54643</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.291526</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_94b0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_b760 </td><td>                   </td><td>&lt;ray.tune.searc_9840</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_9120</td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_a620</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.64676 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.271741 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_7e4eff20</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.248734</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_97b0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_8a30 </td><td>                   </td><td>&lt;ray.tune.searc_aef0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_8c70</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_83d0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.532973</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.186176 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_16866538</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.260048</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_8700</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_9150</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_92a0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.590533</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.219581 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_0e4b05a9</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.265328</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_be20</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_abc0</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_b580</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.584331</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.626422 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_b68844fb</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.27069 </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_bc10  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_8220 </td><td>                   </td><td>&lt;ray.tune.searc_86d0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_85b0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_f310</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.306852</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.419686 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_f14afdd1</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.268796</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_afb0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_8af0 </td><td>                   </td><td>&lt;ray.tune.searc_b250</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_8e80</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_b010</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.316915</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.260462 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_c735fb39</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.204545</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_b940  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_a290 </td><td>                   </td><td>&lt;ray.tune.searc_8d30</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_91e0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_ac20</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.369215</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.49151  </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_3bf86462</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.326721</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_e230  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_8e50 </td><td>                   </td><td>&lt;ray.tune.searc_b370</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_bc70</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_cf40</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.359649</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.40988  </td><td style=\"text-align: right;\">        0.1224</td><td style=\"text-align: right;\">          0.0909091</td><td style=\"text-align: right;\">          0.1875</td></tr>\n",
       "<tr><td>train_model_346b2a29</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.220834</td><td style=\"text-align: right;\">                128</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_61a0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.397717</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.0855691</td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_3eb6f4b6</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.219172</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_b370</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.564006</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.0893223</td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_719e35fc</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.382569</td><td style=\"text-align: right;\">                256</td><td>SoftMarginLoss      </td><td>&lt;ray.tune.searc_76a0</td><td>                      </td><td>&lt;ray.tune.searc_5db0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_5e40</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.562128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.055898 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_ec5e0264</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.320946</td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_a920</td><td>                      </td><td>&lt;ray.tune.searc_ac50  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_cee0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.546319</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.126747 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_af638f4b</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.329043</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_9300 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_bfd0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_baf0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.339237</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.598782 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_41e3d537</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.342733</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_8340 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_a860</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_8d60</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.365068</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.822414 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_b95001bf</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.34188 </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_bb20  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_8e20 </td><td>                   </td><td>&lt;ray.tune.searc_ba90</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_b5e0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_8fa0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.300333</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.228493 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_51d48536</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.30561 </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_b610  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_b070 </td><td>                   </td><td>&lt;ray.tune.searc_ab60</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_b8b0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_9d20</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.314869</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.342561 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_d847e5d9</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.399853</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_f190  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_e260 </td><td>                   </td><td>&lt;ray.tune.searc_e0e0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_ca60</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_e230</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.414381</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.367362 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_4b4b5ab3</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.358897</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_1c00  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_27a0 </td><td>                   </td><td>&lt;ray.tune.searc_02b0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_0ca0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_b550</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.342773</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.258633 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_35666426</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.16901 </td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_c1c0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_ec50 </td><td>                   </td><td>&lt;ray.tune.searc_70d0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_5510</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_b7c0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.329256</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.270607 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_21e819bc</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.169636</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_1cf0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.576152</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.233176 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_46762351</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.191732</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_1210</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.35228 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.345675 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_c4abc010</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.18532 </td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_b550</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_aad0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.608794</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.117094 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_a1ee20c5</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.232224</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_1960</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_19f0</td><td>&lt;ray.tune.searc_3af0</td><td>SGD             </td><td style=\"text-align: right;\">   0.640836</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.650943 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_33851fae</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.318377</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_2110</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_08e0</td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_1240</td><td>&lt;ray.tune.searc_0c70</td><td>SGD             </td><td style=\"text-align: right;\">   0.63546 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.118176 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_760204c3</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.143497</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_0e80  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_06a0 </td><td>                   </td><td>&lt;ray.tune.searc_13f0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_1330</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_82b0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.599818</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.198192 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_6a18119b</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.134905</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_1f00  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_01f0 </td><td>                   </td><td>&lt;ray.tune.searc_06d0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_33d0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_a980</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.318752</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.196906 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_6de0f0d0</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.245839</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_9360  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_9ed0 </td><td>                   </td><td>&lt;ray.tune.searc_b160</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_8910</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_07c0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.353967</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.115837 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_e565771b</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.352156</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_b430  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_8cd0 </td><td>                   </td><td>&lt;ray.tune.searc_aef0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_8e50</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_5960</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.331991</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.281084 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_347c545e</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.208802</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_b040  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_30d0 </td><td>                   </td><td>&lt;ray.tune.searc_ae30</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_ac50</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_2350</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.332318</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.104378 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_284d0d1f</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.179183</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_f970</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_f7f0</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_d750</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.395688</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.741721 </td><td style=\"text-align: right;\">        0.1081</td><td style=\"text-align: right;\">          0.0952381</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_db92ae71</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.20223 </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_f550</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_ffd0</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_d5d0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.529119</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.368319 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_13218db8</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.120341</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_5300</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.674672</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.707019 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_f950b243</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.189506</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_a740</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.385995</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.15006  </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_3fce942b</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.299518</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_2f80</td><td>                      </td><td>&lt;ray.tune.searc_4400  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_6320</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.381966</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.201351 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_9c8f3769</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.153215</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_2290  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_2260 </td><td>                   </td><td>&lt;ray.tune.searc_3190</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_1f30</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_1720</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.771733</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.220412 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_5385e90e</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.154435</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_1390  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_0190 </td><td>                   </td><td>&lt;ray.tune.searc_3370</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_2cb0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_29b0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.762319</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.196901 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_571bcbae</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.126358</td><td style=\"text-align: right;\">               1024</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_72e0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_6c80 </td><td>                   </td><td>&lt;ray.tune.searc_6bc0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_6c20</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_6d10</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.735825</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.207669 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_d038fb68</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>gelu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.124929</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_5060  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_56f0 </td><td>                   </td><td>&lt;ray.tune.searc_5c30</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_77c0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_5b40</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.702173</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.163805 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_86d5c24a</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.227288</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_f670  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_2080 </td><td>                   </td><td>&lt;ray.tune.searc_2560</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_2050</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_4940</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.71242 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.136654 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_9facd208</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.138326</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_26b0 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_31f0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_2860</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.582282</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.369644 </td><td style=\"text-align: right;\">        0.0571</td><td style=\"text-align: right;\">          0.0526316</td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_f19e1068</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.13933 </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_3d30 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_2c20</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_0760</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.584689</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.87957  </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_28635916</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.16131 </td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_52a0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.435699</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.274178 </td><td style=\"text-align: right;\">        0.1224</td><td style=\"text-align: right;\">          0.0909091</td><td style=\"text-align: right;\">          0.1875</td></tr>\n",
       "<tr><td>train_model_2ee90ce2</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.215953</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_2710</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.475023</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.227934 </td><td style=\"text-align: right;\">        0.1224</td><td style=\"text-align: right;\">          0.0909091</td><td style=\"text-align: right;\">          0.1875</td></tr>\n",
       "<tr><td>train_model_c54cba25</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.162826</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_4940</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.434158</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.382827 </td><td style=\"text-align: right;\">        0.1224</td><td style=\"text-align: right;\">          0.0909091</td><td style=\"text-align: right;\">          0.1875</td></tr>\n",
       "<tr><td>train_model_b4a8d762</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.216578</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_64d0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.439437</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.135192 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_f6ff8bd5</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.161317</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_2d40</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.438075</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.144899 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_2b8c6add</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.174923</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_af20</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.447177</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.357562 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_9a249907</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.174594</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_9000</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.483549</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.130879 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_29ee977f</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.163947</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_b820</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.475417</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.131173 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_7686ba5c</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.147082</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1bd0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.420931</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.207982 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_14676bea</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.146589</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_b0a0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.408726</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.313854 </td><td style=\"text-align: right;\">        0.1224</td><td style=\"text-align: right;\">          0.0909091</td><td style=\"text-align: right;\">          0.1875</td></tr>\n",
       "<tr><td>train_model_2e5cba87</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.33499 </td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_9150</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.412036</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.270073 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_1411c7b1</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.157755</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_9ab0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.429795</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.07008  </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_8cf08dce</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.198214</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_ad10</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.469724</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.06721  </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_fb836022</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.182903</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_23b0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.449809</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.270766 </td><td style=\"text-align: right;\">        0     </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">          0     </td></tr>\n",
       "<tr><td>train_model_dd26949b</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.149146</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_b250</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.459081</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.707123 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_359ba367</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.259153</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_9180</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.409737</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.972664 </td><td style=\"text-align: right;\">        0.1224</td><td style=\"text-align: right;\">          0.0909091</td><td style=\"text-align: right;\">          0.1875</td></tr>\n",
       "<tr><td>train_model_6e0f5955</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>leaky_relu     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.361781</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a7a0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.430766</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.500795 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_825b56f4</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.258659</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_7c10</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.409881</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.739895 </td><td style=\"text-align: right;\">        0.1224</td><td style=\"text-align: right;\">          0.0909091</td><td style=\"text-align: right;\">          0.1875</td></tr>\n",
       "<tr><td>train_model_4720b242</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.238261</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_5ed0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.406962</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.713543 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_d357da62</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.241909</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6e00</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.407527</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.783373 </td><td style=\"text-align: right;\">        0.1224</td><td style=\"text-align: right;\">          0.0909091</td><td style=\"text-align: right;\">          0.1875</td></tr>\n",
       "<tr><td>train_model_323eea73</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.259741</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_7df0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.403204</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.806063 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_36f3c6f7</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.261592</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_4910</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.406327</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.703583 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_273215c7</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.25715 </td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_9b40</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.386994</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.675311 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_84e8e6be</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.252485</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_c700</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.426458</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.678195 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_e609a056</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.251266</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_9d20</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.424703</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.450563 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_46aadc2c</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.274184</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_2f50</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.425035</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.680313 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_c2e4367e</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.277216</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_eaa0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.369509</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.674177 </td><td style=\"text-align: right;\">        0.0408</td><td style=\"text-align: right;\">          0.030303 </td><td style=\"text-align: right;\">          0.0625</td></tr>\n",
       "<tr><td>train_model_1d5cbf5f</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>relu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.243485</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_7c40</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.393113</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.614056 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_18b5125e</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.242228</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6020</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.416528</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.462238 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_7c025839</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.226371</td><td style=\"text-align: right;\">                256</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_55d0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.41178 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.112557 </td><td style=\"text-align: right;\">        0.1176</td><td style=\"text-align: right;\">          0.111111 </td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "<tr><td>train_model_58e1a6a5</td><td>TERMINATED</td><td>127.0.0.1:10753</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.229987</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_32e0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_2d40</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1750</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.511663</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.701727 </td><td style=\"text-align: right;\">        0.0816</td><td style=\"text-align: right;\">          0.0606061</td><td style=\"text-align: right;\">          0.125 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 11:57:45,022\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-11 11:57:45,120\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1788, in ray._raylet.execute_task\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1790, in ray._raylet.execute_task\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 790, in deserialize_objects\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     context = self.get_serialization_context()\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 678, in get_serialization_context\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     context_map[job_id] = serialization.SerializationContext(self)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/ray/_private/serialization.py\", line 153, in __init__\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     serialization_addons.apply(self)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/ray/util/serialization_addons.py\", line 29, in apply\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     from ray._private.pydantic_compat import register_pydantic_serializers\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/ray/_private/pydantic_compat.py\", line 53, in <module>\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     from pydantic.v1 import (\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/pydantic/v1/__init__.py\", line 2, in <module>\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     from pydantic.v1 import dataclasses\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/pydantic/v1/dataclasses.py\", line 54, in <module>\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     from pydantic.v1.fields import Field, FieldInfo, Required, Undefined\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/pydantic/v1/fields.py\", line 228, in <module>\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny', Any]] = None,\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/typing.py\", line 309, in inner\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     return cached(*args, **kwds)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/typing.py\", line 403, in __getitem__\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     return self._getitem(self, parameters)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/typing.py\", line 521, in Union\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     return _UnionGenericAlias(self, parameters)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/typing.py\", line 1019, in __init__\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     super().__init__(origin, inst=inst, name=name)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/typing.py\", line 949, in __init__\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     self._name = name\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/typing.py\", line 989, in __setattr__\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     super().__setattr__(attr, val)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 841, in sigterm_handler\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     raise_sys_exit_with_custom_error_message(\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 846, in ray._raylet.raise_sys_exit_with_custom_error_message\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m SystemExit: 1\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m \n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m \n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1760, in ray._raylet.execute_task\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/traceback.py\", line 183, in format_exc\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/traceback.py\", line 135, in format_exception\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/traceback.py\", line 502, in __init__\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/traceback.py\", line 383, in extract\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     f.line\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/traceback.py\", line 306, in line\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/linecache.py\", line 30, in getline\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/linecache.py\", line 46, in getlines\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/.pyenv/versions/3.10.9/lib/python3.10/linecache.py\", line 93, in updatecache\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     stat = os.stat(fullname)\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m KeyboardInterrupt\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m \n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m \n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2137, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1090, in ray._raylet.store_task_errors\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/ray/_private/utils.py\", line 178, in push_error_to_driver\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4701, in ray._raylet.CoreWorker.push_error\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 589, in ray._raylet.check_status\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m ray.exceptions.RaySystemError: System error: Broken pipe\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m \n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m \n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2245, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/ray/_private/utils.py\", line 178, in push_error_to_driver\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4701, in ray._raylet.CoreWorker.push_error\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 589, in ray._raylet.check_status\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m ray.exceptions.RaySystemError: System error: Broken pipe\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m Exception ignored in: 'ray._raylet.task_execution_handler'\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2245, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/ray/_private/utils.py\", line 178, in push_error_to_driver\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4701, in ray._raylet.CoreWorker.push_error\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m   File \"python/ray/_raylet.pyx\", line 589, in ray._raylet.check_status\n",
      "\u001b[36m(bundle_reservation_check_func pid=10820)\u001b[0m ray.exceptions.RaySystemError: System error: Broken pipe\n",
      "2025-07-11 11:57:45,663\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-11 11:57:45,786\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-11 11:57:46,219\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n"
     ]
    }
   ],
   "source": [
    "# Define search space for hyperparameter optimization\n",
    "search_space = {\n",
    "    \"output_dim\": len(all_two_grams),  # Output dimension is also the number of unique 2-grams\n",
    "    \"num_layers\": tune.randint(1, 4),  # Vary the number of layers in the model\n",
    "    #\"num_layers\": tune.randint(1, 2),\n",
    "    \"hidden_layer_size\": tune.choice([128, 256, 512, 1024, 2048]),  # Different sizes for hidden layers\n",
    "    #\"hidden_layer_size\": tune.choice([1024, 2048]),  # Different sizes for hidden layers\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.4),  # Dropout rate between 0.1 and 0.4\n",
    "    \"activation_fn\": tune.choice([\"relu\", \"leaky_relu\", \"gelu\", \"elu\", \"selu\", \"tanh\"]),  # Activation functions to choose from\n",
    "    \"optimizer\": tune.choice([\n",
    "        {\"name\": \"Adam\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"AdamW\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"SGD\", \"lr\": tune.loguniform(1e-4, 1e-2), \"momentum\": tune.uniform(0.0, 0.99)},\n",
    "        {\"name\": \"RMSprop\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "    ]),\n",
    "    \"loss_fn\": tune.choice([\"BCEWithLogitsLoss\", \"MultiLabelSoftMarginLoss\", \"SoftMarginLoss\"]),\n",
    "    \"threshold\": tune.uniform(0.3, 0.8),\n",
    "    \"lr_scheduler\": tune.choice([\n",
    "        {\"name\": \"StepLR\", \"step_size\": tune.choice([5, 10, 20]), \"gamma\": tune.uniform(0.1, 0.9)},\n",
    "        {\"name\": \"ExponentialLR\", \"gamma\": tune.uniform(0.85, 0.99)},\n",
    "        {\"name\": \"ReduceLROnPlateau\", \"mode\": \"min\", \"factor\": tune.uniform(0.1, 0.5), \"patience\": tune.choice([5, 10, 15])},\n",
    "        {\"name\": \"CosineAnnealingLR\", \"T_max\": tune.loguniform(10, 50) , \"eta_min\": tune.choice([1e-5, 1e-6, 0])},\n",
    "        {\"name\": \"CyclicLR\", \"base_lr\": tune.loguniform(1e-5, 1e-3), \"max_lr\": tune.loguniform(1e-3, 1e-1), \"step_size_up\": tune.choice([2000, 4000]), \"mode_cyclic\": tune.choice([\"triangular\", \"triangular2\", \"exp_range\"]) },\n",
    "        {\"name\": \"None\"}  # No scheduler\n",
    "    ]),\n",
    "    \"batch_size\": tune.choice([8, 16, 32, 64]),  # Batch sizes to test\n",
    "}\n",
    "\n",
    "# Initialize Ray for hyperparameter optimization\n",
    "ray.init(ignore_reinit_error=True, logging_level=\"ERROR\")\n",
    "\n",
    "# Optuna Search Algorithm for optimizing the hyperparameters\n",
    "optuna_search = OptunaSearch(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"max\")\n",
    "\n",
    "# Use ASHAScheduler to manage trials and early stopping\n",
    "scheduler = ASHAScheduler(metric=\"total_val_loss\", mode=\"min\")\n",
    "\n",
    "\n",
    "\n",
    "# Define and configure the Tuner for Ray Tune\n",
    "tuner = tune.Tuner(\n",
    "    partial(train_model, data_dir=data_dir, output_dim=len(all_two_grams),alice_enc_hash=alice_enc_hash, identifier=identifier, patience=DEA_CONFIG[\"Patience\"], min_delta=DEA_CONFIG[\"MinDelta\"]),  # The function to optimize (training function)\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,  # Search strategy using Optuna\n",
    "        scheduler=scheduler,  # Use ASHA to manage the trials\n",
    "        num_samples=DEA_CONFIG[\"NumSamples\"],  # Number of trials to run\n",
    "        max_concurrent_trials=GLOBAL_CONFIG[\"Workers\"],\n",
    "    ),\n",
    "    param_space=search_space  # Pass in the defined hyperparameter search space\n",
    "\n",
    ")\n",
    "\n",
    "# Run the tuner\n",
    "results = tuner.fit()\n",
    "\n",
    "# Shut down Ray after finishing the optimization\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = results\n",
    "best_result = result_grid.get_best_result(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_hyperparameter_optimization = time.time() - start_hyperparameter_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to all_trial_results.csv\n",
      "\n",
      "🔍 Best_Result\n",
      "----------------------------------------\n",
      "Config: {'output_dim': 1036, 'num_layers': 1, 'hidden_layer_size': 1024, 'dropout_rate': 0.1618324025134827, 'activation_fn': 'selu', 'optimizer': {'name': 'AdamW', 'lr': 2.0443403772145585e-05}, 'loss_fn': 'MultiLabelSoftMarginLoss', 'threshold': 0.5890883699016457, 'lr_scheduler': {'name': 'StepLR', 'step_size': 20, 'gamma': 0.3868061049562671}, 'batch_size': 32}\n",
      "Average Dice: 0.1250\n",
      "Average Precision: 0.1250\n",
      "Average Recall: 0.1250\n",
      "Average F1: 0.1250\n",
      "\n",
      "🔍 Worst_Result\n",
      "----------------------------------------\n",
      "Config: {'output_dim': 1036, 'num_layers': 3, 'hidden_layer_size': 512, 'dropout_rate': 0.17147077465852611, 'activation_fn': 'tanh', 'optimizer': {'name': 'SGD', 'lr': 0.0001724609076511417, 'momentum': 0.9864860000490883}, 'loss_fn': 'BCEWithLogitsLoss', 'threshold': 0.439163987816743, 'lr_scheduler': {'name': 'ExponentialLR', 'gamma': 0.9602150423737108}, 'batch_size': 16}\n",
      "Average Dice: 0.0000\n",
      "Average Precision: 0.0000\n",
      "Average Recall: 0.0000\n",
      "Average F1: 0.0000\n",
      "\n",
      "📊 Average Metrics Across All Trials\n",
      "----------------------------------------\n",
      "Average_dice: 0.0828\n",
      "Average_precision: 0.0741\n",
      "Average_recall: 0.0983\n",
      "Average_f1: 0.0828\n",
      "📊 Saved plot: metric_distributions.png\n",
      "📌 Saved heatmap: correlation_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    worst_result = result_grid.get_best_result(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"min\")\n",
    "\n",
    "    # Combine configs and metrics into a DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            **clean_result_dict(resolve_config(result.config)),\n",
    "            **{k: result.metrics.get(k) for k in [\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]},\n",
    "        }\n",
    "        for result in result_grid\n",
    "    ])\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(f\"{save_to}/hyperparameteroptimization/all_trial_results.csv\", index=False)\n",
    "    print(\"✅ Results saved to all_trial_results.csv\")\n",
    "\n",
    "    print_and_save_result(\"Best_Result\", best_result, f\"{save_to}/hyperparameteroptimization\")\n",
    "    print_and_save_result(\"Worst_Result\", worst_result, f\"{save_to}/hyperparameteroptimization\")\n",
    "\n",
    "    # Compute and print average metrics\n",
    "    print(\"\\n📊 Average Metrics Across All Trials\")\n",
    "    avg_metrics = df[[\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]].mean()\n",
    "    print(\"-\" * 40)\n",
    "    for key, value in avg_metrics.items():\n",
    "        print(f\"{key.capitalize()}: {value:.4f}\")\n",
    "\n",
    "    # --- 📈 Plotting performance metrics ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df[[\"average_dice\", \"average_recall\", \"average_f1\", \"average_precision\"]])\n",
    "    plt.title(\"Distribution of Performance Metrics Across Trials\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{save_to}/hyperparameteroptimization/metric_distributions.png\")\n",
    "    plt.close()\n",
    "    print(\"📊 Saved plot: metric_distributions.png\")\n",
    "\n",
    "    # --- 📌 Correlation between config params and performance ---\n",
    "    # Only include numeric config columns\n",
    "    exclude_cols = {\"input_dim\", \"output_dim\"}\n",
    "    numeric_config_cols = [\n",
    "        col for col in df.columns\n",
    "        if pd.api.types.is_numeric_dtype(df[col]) and col not in exclude_cols\n",
    "    ]\n",
    "    correlation_df = df[numeric_config_cols].corr()\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Between Parameters and Metrics\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_to}/hyperparameteroptimization/correlation_heatmap.png\")\n",
    "    plt.close()\n",
    "    print(\"📌 Saved heatmap: correlation_heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "\n",
    "The neural network model is selected dynamically based on the encoding technique used for Alice’s data.\n",
    "\n",
    "### Supported Models:\n",
    "\n",
    "- **BloomFilter** → `BloomFilterToTwoGramClassifier`  \n",
    "  - Input: Binary vector (Bloom filter)  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TabMinHash** → `TabMinHashToTwoGramClassifier`  \n",
    "  - Input: Tabulated MinHash signature  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TwoStepHash** → `TwoStepHashToTwoGramClassifier`  \n",
    "  - Input: Length of the unique integers present\n",
    "  - Output: 2-gram predicition\n",
    "    \n",
    "Each model outputs predictions over the set of all possible 2-grams (`all_two_grams`), and the input dimension is dynamically configured based on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_model_training = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = resolve_config(best_result.config)\n",
    "data_train, data_val, data_test = load_data(data_dir, alice_enc_hash, identifier, load_test=True)\n",
    "input_dim=data_train[0][0].shape[0]\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    data_train,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),  # Default to 32 if not specified\n",
    "    shuffle=True  # Important for training\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    data_val,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),\n",
    "    shuffle=False  # Allows variation in validation batches\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    data_test,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),\n",
    "    shuffle=False  # Allows variation in validation batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): SELU()\n",
      "    (2): Dropout(p=0.1618324025134827, inplace=False)\n",
      "    (3): Linear(in_features=1024, out_features=1036, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=len(all_two_grams),\n",
    "            hidden_layer=best_config.get(\"hidden_layer_size\", 128),  # Default to 128 if not specified\n",
    "            num_layers=best_config.get(\"num_layers\", 2),  # Default to 2 if not specified\n",
    "            dropout_rate=best_config.get(\"dropout_rate\", 0.2),  # Default to 0.2 if not specified\n",
    "            activation_fn=best_config.get(\"activation_fn\", \"relu\")  # Default to 'relu' if not specified\n",
    "        )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Environment Setup\n",
    "This code initializes the core components needed for training a neural network model.\n",
    "\n",
    "1. TensorBoard Setup\n",
    "    - Creates unique run name by combining:\n",
    "    - Loss function type\n",
    "    - Optimizer choice\n",
    "    - Alice's algorithm\n",
    "    - Initializes TensorBoard writer in runs directory\n",
    "2. Device Configuration\n",
    "    - Automatically selects GPU if available, falls back to CPU\n",
    "    - Moves model to selected device\n",
    "3. Loss Functions\n",
    "    - `BCEWithLogitsLoss`: Binary Cross Entropy with Logits\n",
    "    - `MultiLabelSoftMarginLoss`: Multi-Label Soft Margin Loss\n",
    "4. Optimizers:\n",
    "    - `Adam`: Adaptive Moment Estimation\n",
    "    - `AdamW`: Adam with Weight Decay\n",
    "    - `SGD`: Stochastic Gradient Descent (with momentum)\n",
    "    - `RMSprop`: Root Mean Square Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    # Setup tensorboard logging\n",
    "    run_name = \"\".join([\n",
    "        best_config.get(\"loss_fn\", \"MultiLabelSoftMarginLoss\"),\n",
    "        best_config.get(\"optimizer\").get(\"name\", \"Adam\"),\n",
    "        ENC_CONFIG[\"AliceAlgo\"],\n",
    "        best_config.get(\"activation_fn\", \"relu\"),\n",
    "    ])\n",
    "    tb_writer = SummaryWriter(f\"{save_to}/{run_name}\")\n",
    "\n",
    "# Setup compute device (GPU/CPU)\n",
    "compute_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(compute_device)\n",
    "\n",
    "# Initialize loss function\n",
    "match best_config.get(\"loss_fn\", \"MultiLabelSoftMarginLoss\"):\n",
    "    case \"BCEWithLogitsLoss\":\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    case \"MultiLabelSoftMarginLoss\":\n",
    "        criterion = nn.MultiLabelSoftMarginLoss(reduction='mean')\n",
    "    case \"SoftMarginLoss\":\n",
    "        criterion = nn.SoftMarginLoss()\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported loss function: {best_config.get('loss_fn', 'MultiLabelSoftMarginLoss')}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "match best_config.get(\"optimizer\").get(\"name\", \"Adam\"):\n",
    "    case \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                            lr=best_config.get(\"optimizer\").get(\"lr\"),\n",
    "                            momentum=best_config.get(\"optimizer\").get(\"momentum\"))\n",
    "    case \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported optimizer: {best_config.get('optimizer').get('name', 'Adam')}\")\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "match best_config.get(\"lr_scheduler\").get(\"name\", \"None\"):\n",
    "    case \"StepLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=best_config.get(\"lr_scheduler\").get(\"step_size\"),\n",
    "            gamma=best_config.get(\"lr_scheduler\").get(\"gamma\")\n",
    "        )\n",
    "    case \"ExponentialLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=best_config.get(\"lr_scheduler\").get(\"gamma\")\n",
    "        )\n",
    "    case \"ReduceLROnPlateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=best_config.get(\"lr_scheduler\").get(\"mode\"),\n",
    "            factor=best_config.get(\"lr_scheduler\").get(\"factor\"),\n",
    "            patience=best_config.get(\"lr_scheduler\").get(\"patience\")\n",
    "        )\n",
    "    case \"CosineAnnealingLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=best_config.get(\"lr_scheduler\").get(\"T_max\")\n",
    "        )\n",
    "    case \"CyclicLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=best_config.get(\"lr_scheduler\").get(\"base_lr\"),\n",
    "            max_lr=best_config.get(\"lr_scheduler\").get(\"max_lr\"),\n",
    "            step_size_up=best_config.get(\"lr_scheduler\").get(\"step_size_up\"),\n",
    "            mode=best_config.get(\"lr_scheduler\").get(\"mode_cyclic\"),\n",
    "            cycle_momentum=False  # usually False for Adam/AdamW\n",
    "        )\n",
    "    case None | \"None\":\n",
    "        scheduler = None\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported LR scheduler: {best_config.get('lr_scheduler').get('name', 'None')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Early Stopping\n",
    "\n",
    "The function `train_model` orchestrates the training process for the neural network, including both training and validation phases for each epoch. It also utilizes **early stopping** to halt training when the validation loss fails to improve over multiple epochs, avoiding overfitting.\n",
    "\n",
    "### Key Phases:\n",
    "1. **Training Phase**: \n",
    "   - The model is trained on the `dataloader_train`, computing the training loss using the specified loss function (`criterion`) and optimizer. Gradients are calculated, and the model parameters are updated.\n",
    "  \n",
    "2. **Validation Phase**:\n",
    "   - The model is evaluated on the `dataloader_val` without updating weights. The validation loss is computed to track model performance on unseen data.\n",
    "\n",
    "3. **Logging**: \n",
    "   - Training and validation losses are logged to both the console and **TensorBoard** for tracking model performance during training.\n",
    "\n",
    "4. **Early Stopping**: \n",
    "   - If the validation loss does not improve after a certain number of epochs (defined by `DEA_CONFIG[\"Patience\"]`), the training process is halted to prevent overfitting.\n",
    "\n",
    "### Helper Functions:\n",
    "- `run_epoch`: Handles a single epoch, either for training or validation, depending on the flag `is_training`.\n",
    "- `log_metrics`: Logs the training and validation losses to the console and TensorBoard for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_epoch_metrics(epoch, total_epochs, train_loss, val_loss):\n",
    "    epoch_str = f\"[{epoch + 1}/{total_epochs}]\"\n",
    "    print(f\"{epoch_str} 🔧 Train Loss: {train_loss:.4f} | 🔍 Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if DEA_CONFIG.get(\"SaveResults\", False) and 'tb_writer' in globals():\n",
    "        tb_writer.add_scalar(\"Loss/train\", train_loss, epoch + 1)\n",
    "        tb_writer.add_scalar(\"Loss/validation\", val_loss, epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, device, scheduler=None):\n",
    "    num_epochs = best_config.get(\"epochs\", DEA_CONFIG[\"Epochs\"])\n",
    "    verbose = GLOBAL_CONFIG[\"Verbose\"]\n",
    "    patience = DEA_CONFIG[\"Patience\"]\n",
    "    min_delta = DEA_CONFIG[\"MinDelta\"]\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=min_delta, verbose=verbose)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- Training ----\n",
    "        model.train()\n",
    "        train_loss = run_epoch(\n",
    "            model, dataloader_train, criterion, optimizer,\n",
    "            device, is_training=True, verbose=verbose, scheduler=scheduler\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = run_epoch(\n",
    "                model, dataloader_val, criterion, optimizer,\n",
    "                device, is_training=False, verbose=verbose, scheduler=scheduler\n",
    "            )\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # ---- Scheduler step ----\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # ---- Logging ----\n",
    "        _log_epoch_metrics(epoch, num_epochs, train_loss, val_loss)\n",
    "\n",
    "        # ---- Early stopping ----\n",
    "        if early_stopper(val_loss):\n",
    "            if verbose:\n",
    "                print(f\"⏹️ Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20] 🔧 Train Loss: 0.7066 | 🔍 Val Loss: 0.6929\n",
      "[2/20] 🔧 Train Loss: 0.6879 | 🔍 Val Loss: 0.6836\n",
      "[3/20] 🔧 Train Loss: 0.6678 | 🔍 Val Loss: 0.6743\n",
      "[4/20] 🔧 Train Loss: 0.6488 | 🔍 Val Loss: 0.6649\n",
      "[5/20] 🔧 Train Loss: 0.6328 | 🔍 Val Loss: 0.6554\n",
      "[6/20] 🔧 Train Loss: 0.6171 | 🔍 Val Loss: 0.6457\n",
      "[7/20] 🔧 Train Loss: 0.5933 | 🔍 Val Loss: 0.6357\n",
      "[8/20] 🔧 Train Loss: 0.5796 | 🔍 Val Loss: 0.6255\n",
      "[9/20] 🔧 Train Loss: 0.5589 | 🔍 Val Loss: 0.6150\n",
      "[10/20] 🔧 Train Loss: 0.5434 | 🔍 Val Loss: 0.6041\n",
      "[11/20] 🔧 Train Loss: 0.5258 | 🔍 Val Loss: 0.5966\n",
      "[12/20] 🔧 Train Loss: 0.5089 | 🔍 Val Loss: 0.5888\n",
      "[13/20] 🔧 Train Loss: 0.4962 | 🔍 Val Loss: 0.5809\n",
      "[14/20] 🔧 Train Loss: 0.4838 | 🔍 Val Loss: 0.5728\n",
      "[15/20] 🔧 Train Loss: 0.4636 | 🔍 Val Loss: 0.5644\n",
      "[16/20] 🔧 Train Loss: 0.4516 | 🔍 Val Loss: 0.5558\n",
      "[17/20] 🔧 Train Loss: 0.4418 | 🔍 Val Loss: 0.5471\n",
      "[18/20] 🔧 Train Loss: 0.4307 | 🔍 Val Loss: 0.5381\n",
      "[19/20] 🔧 Train Loss: 0.4072 | 🔍 Val Loss: 0.5290\n",
      "[20/20] 🔧 Train Loss: 0.3980 | 🔍 Val Loss: 0.5197\n"
     ]
    }
   ],
   "source": [
    "model, train_losses, val_losses = train_model(\n",
    "    model, dataloader_train, dataloader_val,\n",
    "    criterion, optimizer, compute_device,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_model_training = time.time() - start_model_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Visualization over Epochs\n",
    "\n",
    "This code snippet generates a plot to visualize the **training loss** and **validation loss** across epochs. It's useful for tracking model performance during training and evaluating if overfitting is occurring (i.e., when validation loss starts increasing while training loss continues to decrease).\n",
    "\n",
    "### Key Elements:\n",
    "1. **Plotting the Losses**: \n",
    "   - The `train_losses` and `val_losses` are plotted over the epochs. \n",
    "   - The **blue line** represents the training loss, and the **red line** represents the validation loss.\n",
    "\n",
    "2. **Legend**: \n",
    "   - A legend is added to distinguish between training and validation losses.\n",
    "\n",
    "3. **Title and Labels**: \n",
    "   - The plot is titled \"Training and Validation Loss over Epochs\" for context.\n",
    "   - **X-axis** represents the epoch number, and **Y-axis** represents the loss value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation loss', color='red')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "if DEA_CONFIG.get(\"SaveResults\", False):\n",
    "    plt.savefig(f\"{save_to}/loss_curve.png\")\n",
    "    plt.close()\n",
    "    print(\"📉 Saved loss curve to loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Application to Encoded Data\n",
    "\n",
    "This code performs inference on the test data and compares the predicted 2-grams with the actual 2-grams, providing a performance evaluation based on the **Dice similarity coefficient**.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Prepare for Evaluation**:\n",
    "   - The model is switched to **evaluation mode** (`model.eval()`), ensuring no gradient computation.\n",
    "   \n",
    "2. **Thresholding**:\n",
    "   - A threshold (`DEA_CONFIG[\"FilterThreshold\"]`) is applied to filter out low-probability predictions, retaining only the most confident predictions.\n",
    "\n",
    "3. **Inference and 2-Gram Scoring**:\n",
    "   - The model is applied to the batch, and the **logits** are converted into probabilities using the **sigmoid function**.\n",
    "   - The probabilities are then mapped to **2-gram scores**, and scores below the threshold are discarded.\n",
    "\n",
    "4. **Reconstructing Words**:\n",
    "   - For each sample in the batch, **2-grams** are reconstructed into words based on the filtered scores.\n",
    "\n",
    "5. **Performance Metrics**:\n",
    "   - The actual 2-grams (from the test dataset) are compared with the predicted 2-grams, and the **Dice similarity coefficient** is calculated for each sample.\n",
    "\n",
    "### Result:\n",
    "- The code generates a list `combined_results_performance`, which contains a detailed comparison for each UID, including:\n",
    "  - **Actual 2-grams** (from the test data)\n",
    "  - **Predicted 2-grams** (from the model)\n",
    "  - **Dice similarity** score indicating how similar the actual and predicted 2-grams are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(\n",
    "    GLOBAL_CONFIG[\"LoadPath\"] if GLOBAL_CONFIG[\"LoadResults\"] else save_to,\n",
    "    \"trained_model\"\n",
    ")\n",
    "model_file   = f\"{base_path}/model.pt\"\n",
    "config_file  = f\"{base_path}/config.json\"\n",
    "result_file  = f\"{base_path}/result.json\"\n",
    "metrics_file = f\"{base_path}/metrics.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_config(model, config, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(path, \"model.pt\"))\n",
    "    with open(os.path.join(path, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    print(f\"✅ Saved model and config to {path}\")\n",
    "\n",
    "\n",
    "def load_model_and_config(model_cls, path, input_dim, output_dim):\n",
    "    with open(os.path.join(path, \"config.json\")) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    model = model_cls(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        hidden_layer=config.get(\"hidden_layer_size\", 128),\n",
    "        num_layers=config.get(\"num_layers\", 2),\n",
    "        dropout_rate=config.get(\"dropout_rate\", 0.2),\n",
    "        activation_fn=config.get(\"activation_fn\", \"relu\")\n",
    "    )\n",
    "    model.load_state_dict(torch.load(os.path.join(path, \"model.pt\")))\n",
    "    model.eval()\n",
    "    return model, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model and config to experiment_results/experiment_TabMinHash_fakename_1k_2025-07-11_11-57-28/trained_model\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    save_model_and_config(model, best_config, base_path)\n",
    "\n",
    "if GLOBAL_CONFIG[\"LoadResults\"]:\n",
    "    #TODO: how to figure out input_dim without loading dataset\n",
    "    model, best_config = load_model_and_config(BaseModel, base_path, input_dim=1024, output_dim=len(all_two_grams))\n",
    "    model.to(compute_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_application_to_encoded_data = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Test Metrics:\n",
      "  Dice:      0.0807\n",
      "  Precision: 0.0905\n",
      "  Recall:    0.0744\n",
      "  F1 Score:  0.0808\n"
     ]
    }
   ],
   "source": [
    "# Initialize metric accumulators\n",
    "total_dice = total_precision = total_recall = total_f1 = 0.0\n",
    "num_samples = 0\n",
    "results = []\n",
    "\n",
    "threshold = best_config.get(\"threshold\", 0.5)\n",
    "model.eval()\n",
    "\n",
    "# Progress bar only if verbose\n",
    "dataloader_iter = tqdm(dataloader_test, desc=\"Test loop\") if GLOBAL_CONFIG[\"Verbose\"] else dataloader_test\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels, uids in dataloader_iter:\n",
    "        data, labels = data.to(compute_device), labels.to(compute_device)\n",
    "\n",
    "        logits = model(data)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Actual and predicted 2-grams\n",
    "        actual_two_grams = decode_labels_to_two_grams(two_gram_dict, labels)\n",
    "        predicted_scores = map_probabilities_to_two_grams(two_gram_dict, probs)\n",
    "        predicted_filtered = filter_high_scoring_two_grams(predicted_scores, threshold)\n",
    "\n",
    "        # Batch metrics\n",
    "        bs = data.size(0)\n",
    "        dice, precision, recall, f1 = calculate_performance_metrics(actual_two_grams, predicted_filtered)\n",
    "\n",
    "        total_dice += dice\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        num_samples += bs\n",
    "\n",
    "        # Store per-sample predictions\n",
    "        for uid, actual, predicted in zip(uids, actual_two_grams, predicted_filtered):\n",
    "            metrics = metrics_per_entry(actual, predicted)\n",
    "            results.append({\n",
    "                \"uid\": uid,\n",
    "                \"actual_two_grams\": actual,\n",
    "                \"predicted_two_grams\": predicted,\n",
    "                \"precision\": metrics[\"precision\"],\n",
    "                \"recall\": metrics[\"recall\"],\n",
    "                \"f1\": metrics[\"f1\"],\n",
    "                \"dice\": metrics[\"dice\"],\n",
    "                \"jaccard\": metrics[\"jaccard\"]\n",
    "            })\n",
    "\n",
    "# Avoid division by zero\n",
    "if num_samples > 0:\n",
    "    avg_dice = total_dice / num_samples\n",
    "    avg_precision = total_precision / num_samples\n",
    "    avg_recall = total_recall / num_samples\n",
    "    avg_f1 = total_f1 / num_samples\n",
    "else:\n",
    "    avg_dice = avg_precision = avg_recall = avg_f1 = 0.0\n",
    "\n",
    "# Logging\n",
    "print(f\"\\n📊 Final Test Metrics:\")\n",
    "print(f\"  Dice:      {avg_dice:.4f}\")\n",
    "print(f\"  Precision: {avg_precision:.4f}\")\n",
    "print(f\"  Recall:    {avg_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {avg_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_application_to_encoded_data = time.time() - start_application_to_encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE - Metrics and Result\n",
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        f.write(f\"Average Precision: {avg_precision:.4f}\\n\")\n",
    "        f.write(f\"Average Recall: {avg_recall:.4f}\\n\")\n",
    "        f.write(f\"Average F1 Score: {avg_f1:.4f}\\n\")\n",
    "        f.write(f\"Average Dice Similarity: {avg_dice:.4f}\\n\")\n",
    "\n",
    "    with open(result_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD - Result\n",
    "if GLOBAL_CONFIG[\"LoadResults\"]:\n",
    "    with open(result_file, 'r', encoding='utf-8') as f:\n",
    "        result = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance for Re-Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Sample Reconstructions (first 5)\n",
      "UID: 80930\n",
      "  Actual 2-grams:    ['aa', 'ae', 'an', 'av', 'dr', 'ed', 'eg', 'ga', 'ma', 'ns', 'ra', 'sa', 've', 'a5', '16', '19', '51', '58', '61', '95']\n",
      "  Predicted 2-grams: ['ha', 'ie', 'bl', 'am', 'eh', 'ma', 'tr', 'in', 'c0', '81', 'mb']\n",
      "------------------------------------------------------------\n",
      "UID: 35384\n",
      "  Actual 2-grams:    ['am', 'es', 'hn', 'ja', 'jo', 'me', 'ns', 'oh', 'on', 'sj', 'so', 'n7', '19', '42', '71', '91', '94']\n",
      "  Predicted 2-grams: ['ie', 'c0', 'bl', 'ha', 'ar', 'am', '06', '81', 'ma', 'oi', 'in', 'ri', 'ke', 'oa', 'is']\n",
      "------------------------------------------------------------\n",
      "UID: 58810\n",
      "  Actual 2-grams:    ['ar', 'da', 'el', 'en', 'le', 'lu', 'ne', 'rl', 'tz', 'ut', 'z3', '19', '38', '53', '81', '95']\n",
      "  Predicted 2-grams: ['ha', 'am', 'bl', 'ie', 'in', '98', 'l1', '10', 'zd', 'lw', '81', 'eh', 'uc', 'ad', 'tf', 'li']\n",
      "------------------------------------------------------------\n",
      "UID: 37185\n",
      "  Actual 2-grams:    ['an', 'ar', 'ea', 'is', 'li', 'ne', 'ry', 'sa', 'y1', '02', '10', '19', '25', '51', '97', '99']\n",
      "  Predicted 2-grams: ['ha', '81', 'bl', 'in', 'ie', 'c0', 'ma', 'am', 'eh', 'lw', '10', 'rd']\n",
      "------------------------------------------------------------\n",
      "UID: 50006\n",
      "  Actual 2-grams:    ['an', 'el', 'io', 'li', 'll', 'ma', 'ne', 'no', 'or', 'ot', 'rm', 't6', '19', '43', '51', '65', '94']\n",
      "  Predicted 2-grams: ['ie', 'ha', 'bl', 'am', 'in', '01', 'g3', 'c0', 'eh', 'v6', 'l1', '06', 'ky', 'ma', '98']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.json_normalize(results) # ≈2× faster than DataFrame(list)\n",
    "\n",
    "metric_cols = [\"precision\", \"recall\", \"f1\", \"dice\", \"jaccard\"]        # keys created earlier\n",
    "melted = results_df.melt(value_vars=metric_cols,\n",
    "                         var_name=\"metric\",\n",
    "                         value_name=\"score\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=melted,\n",
    "             x=\"score\",\n",
    "             hue=\"metric\",\n",
    "             bins=20,\n",
    "             element=\"step\",\n",
    "             fill=False,\n",
    "             kde=True,\n",
    "             palette=\"Set2\")\n",
    "plt.title(\"Distribution of Precision / Recall / F1 across Samples\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "if DEA_CONFIG.get(\"SaveResults\", False):\n",
    "    plt.savefig(f\"{save_to}/metric_distributions.png\")\n",
    "    print(\"📊  Saved plot: metric_distributions.png\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n🔍 Sample Reconstructions (first 5)\")\n",
    "for _, row in results_df.iloc[:5].iterrows():\n",
    "    print(f\"UID: {row.uid}\")\n",
    "    print(f\"  Actual 2-grams:    {row.actual_two_grams}\")\n",
    "    print(f\"  Predicted 2-grams: {row.predicted_two_grams}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Refinement and Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_refinement_and_reconstruction = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Reidentification Analysis:\n",
      "Total Reidentified Individuals: 0\n",
      "Total Not Reidentified Individuals: 643\n",
      "Reidentification Rate: 0.00%\n",
      "\n",
      "🔄 Reconstructing results using fuzzy matching (entry-wise, parallelized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I538952/Desktop/master/4-semester-thesis/dataset-extension-attack/.venv/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Reidentification Analysis:\n",
      "Total Reidentified Individuals: 0\n",
      "Total Not Reidentified Individuals: 643\n",
      "Reidentification Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_not_reidentified_df(data_dir: str, identifier: str) -> pd.DataFrame:\n",
    "    df = load_not_reidentified_data(data_dir, alice_enc_hash, identifier)\n",
    "    return lowercase_df(df)\n",
    "\n",
    "def create_identifier(df: pd.DataFrame, comps):\n",
    "    df = df.copy()\n",
    "    df[\"identifier\"] = create_identifier_column_dynamic(df, comps)\n",
    "    return df[[\"uid\", \"identifier\"]]\n",
    "\n",
    "def run_reidentification_once(reconstructed, df_not_reidentified, merge_cols, technique, identifier_components=None):\n",
    "    df_reconstructed = lowercase_df(pd.DataFrame(reconstructed, columns=merge_cols))\n",
    "\n",
    "    if(identifier_components):\n",
    "        df_not_reidentified = create_identifier(df_not_reidentified, identifier_components)\n",
    "\n",
    "    return reidentification_analysis(\n",
    "        df_reconstructed,\n",
    "        df_not_reidentified,\n",
    "        merge_cols,\n",
    "        len(df_not_reidentified),\n",
    "        technique,\n",
    "        save_path=f\"{save_to}/re_identification_results\"\n",
    "    )\n",
    "\n",
    "header = read_header(GLOBAL_CONFIG[\"Data\"])\n",
    "\n",
    "\n",
    "include_birthday = not (GLOBAL_CONFIG[\"Data\"] == \"./data/datasets/titanic_full.tsv\")\n",
    "\n",
    "TECHNIQUES = {\n",
    "    \"ai\": {\n",
    "        \"fn\": reconstruct_identities_with_llm,\n",
    "        \"merge_cols\": header[:3] + [header[-1]],\n",
    "        \"identifier_comps\": None,\n",
    "    },\n",
    "    \"greedy\": {\n",
    "        \"fn\": greedy_reconstruction,\n",
    "        \"merge_cols\": [\"uid\", \"identifier\"],\n",
    "        \"identifier_comps\": header[:-1],\n",
    "    },\n",
    "    \"fuzzy\": {\n",
    "        \"fn\": fuzzy_reconstruction_approach,\n",
    "        \"merge_cols\": (header[:3] if include_birthday else header[:2]) + [header[-1]],\n",
    "        \"identifier_comps\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "selected = DEA_CONFIG[\"MatchingTechnique\"]\n",
    "df_not_reid_cached = get_not_reidentified_df(data_dir, identifier)\n",
    "save_dir = f\"{save_to}/re_identification_results\"\n",
    "\n",
    "if selected == \"fuzzy_and_greedy\":\n",
    "    reidentified = {}\n",
    "    for name in (\"greedy\", \"fuzzy\"):\n",
    "        info = TECHNIQUES[name]\n",
    "        if name == \"fuzzy\":\n",
    "            recon = info[\"fn\"](results, GLOBAL_CONFIG[\"Workers\"], include_birthday )\n",
    "        else:\n",
    "            recon = info[\"fn\"](results)\n",
    "        reidentified[name] = run_reidentification_once(\n",
    "            recon,\n",
    "            df_not_reid_cached,\n",
    "            info[\"merge_cols\"],\n",
    "            name,\n",
    "            info[\"identifier_comps\"],\n",
    "        )\n",
    "else:\n",
    "    if selected not in TECHNIQUES:\n",
    "        raise ValueError(f\"Unsupported matching technique: {selected}\")\n",
    "    info = TECHNIQUES[selected]\n",
    "    if selected == \"fuzzy\":\n",
    "        recon = info[\"fn\"](results, GLOBAL_CONFIG[\"Workers\"], include_birthday)\n",
    "    if selected == \"ai\":\n",
    "        recon = info[\"fn\"](results, info[\"merge_cols\"][:-1])\n",
    "    else:\n",
    "        recon = info[\"fn\"](results)\n",
    "    reidentified = run_reidentification_once(\n",
    "        recon,\n",
    "        df_not_reid_cached,\n",
    "        info[\"merge_cols\"],\n",
    "        selected,\n",
    "        info[\"identifier_comps\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Combined Reidentification (greedy ∪ fuzzy):\n",
      "Total not re-identified individuals: 643\n",
      "Total Unique Reidentified Individuals: 0\n",
      "Combined Reidentification Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "if selected == \"fuzzy_and_greedy\":\n",
    "    # Extract UIDs from both methods\n",
    "    uids_greedy = set(reidentified[\"greedy\"][\"uid\"])\n",
    "    uids_fuzzy = set(reidentified[\"fuzzy\"][\"uid\"])\n",
    "\n",
    "    # Combine them\n",
    "    combined_uids = uids_greedy.union(uids_fuzzy)\n",
    "    total_reidentified_combined = len(combined_uids)\n",
    "\n",
    "    # Get not re-identified count\n",
    "    df_not_reid_cached = get_not_reidentified_df(data_dir, identifier)\n",
    "    len_not_reidentified = len(df_not_reid_cached)\n",
    "\n",
    "    # Compute rate\n",
    "    reidentification_rate_combined = (total_reidentified_combined / len_not_reidentified) * 100\n",
    "\n",
    "    # Print\n",
    "    print(\"\\n🔁 Combined Reidentification (greedy ∪ fuzzy):\")\n",
    "    print(f\"Total not re-identified individuals: {len_not_reidentified}\")\n",
    "    print(f\"Total Unique Reidentified Individuals: {total_reidentified_combined}\")\n",
    "    print(f\"Combined Reidentification Rate: {reidentification_rate_combined:.2f}%\")\n",
    "\n",
    "    # Save UIDs to CSV\n",
    "    save_dir = os.path.join(save_to, \"re_identification_results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame({\"uid\": list(combined_uids)}).to_csv(\n",
    "        os.path.join(save_dir, \"result_fuzzy_and_greedy.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Save summary to TXT\n",
    "    summary_path = os.path.join(save_dir, \"summary_fuzzy_and_greedy.txt\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        f.write(\"Reidentification Method: fuzzy_and_greedy\\n\")\n",
    "        f.write(f\"Total not re-identified individuals: {len_not_reidentified}\\n\")\n",
    "        f.write(f\"Total Unique Reidentified Individuals: {total_reidentified_combined}\\n\")\n",
    "        f.write(f\"Combined Reidentification Rate: {reidentification_rate_combined:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_refinement_and_reconstruction = time.time() - start_refinement_and_reconstruction\n",
    "    elapsed_total = time.time() - start_total\n",
    "    save_dea_runtime_log(\n",
    "        elapsed_gma=elapsed_gma,\n",
    "        elapsed_hyperparameter_optimization=elapsed_hyperparameter_optimization,\n",
    "        elapsed_model_training=elapsed_model_training,\n",
    "        elapsed_application_to_encoded_data=elapsed_application_to_encoded_data,\n",
    "        elapsed_refinement_and_reconstruction=elapsed_refinement_and_reconstruction,\n",
    "        elapsed_total=elapsed_total,\n",
    "        output_dir=save_to\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
