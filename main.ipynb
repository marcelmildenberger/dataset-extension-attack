{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL): Investigating Dataset Extension Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Imports\n",
    "\n",
    "#### PyTorch\n",
    "- `torch`, `torch.nn`, `torch.optim`, `DataLoader`: For building, training, and evaluating neural networks for DEA.\n",
    "- `SummaryWriter`: Logs training metrics for visualization in TensorBoard.\n",
    "\n",
    "#### Ray\n",
    "- `tune`, `air`, `train`, `OptunaSearch`, `ASHAScheduler`: Used for distributed hyperparameter tuning and model optimization.\n",
    "\n",
    "#### Data Handling & Visualization\n",
    "- `pandas`, `numpy`, `matplotlib.pyplot`, `seaborn`: For data manipulation, analysis, and plotting.\n",
    "- `hickle`: Efficient binary serialization format for NumPy arrays and Python objects.\n",
    "- `tqdm.notebook`: Progress bars for loops, especially in Jupyter notebooks.\n",
    "\n",
    "#### Custom Modules\n",
    "- `utils`: A comprehensive set of utility functions for DEA-specific tasks like reconstruction and result logging.\n",
    "- `datasets`: Dataset wrappers for different encoding schemes (Bloom Filter, Tab MinHash, Two-Step Hash).\n",
    "- `pytorch_models`, `early_stopping`: Custom PyTorch model definitions and early stopping mechanism.\n",
    "- `graphMatching.gma`: Executes Graph Matching Attack (GMA) to prepare DEA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ray libraries for hyperparameter tuning and parallelism\n",
    "import ray\n",
    "from ray import air, train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "# Custom utilities and logic\n",
    "from early_stopping.early_stopping import EarlyStopping\n",
    "from graphMatching.gma import run_gma\n",
    "from datasets.bloom_filter_dataset import BloomFilterDataset\n",
    "from datasets.tab_min_hash_dataset import TabMinHashDataset\n",
    "from datasets.two_step_hash_dataset import TwoStepHashDataset\n",
    "from pytorch_models.base_model import BaseModel\n",
    "from pytorch_models_hyperparameter_optimization.base_model_hyperparameter_optimization import BaseModelHyperparameterOptimization\n",
    "from utils import (\n",
    "    calculate_performance_metrics,\n",
    "    clean_result_dict,\n",
    "    create_identifier_column_dynamic,\n",
    "    decode_labels_to_two_grams,\n",
    "    filter_high_scoring_two_grams,\n",
    "    fuzzy_reconstruction_approach,\n",
    "    get_hashes,\n",
    "    greedy_reconstruction,\n",
    "    load_dataframe,\n",
    "    lowercase_df,\n",
    "    map_probabilities_to_two_grams,\n",
    "    metrics_per_entry,\n",
    "    print_and_save_result,\n",
    "    read_header,\n",
    "    reconstruct_identities_with_llm,\n",
    "    reidentification_analysis,\n",
    "    resolve_config,\n",
    "    run_epoch,\n",
    "    save_dea_runtime_log,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Overview\n",
    "\n",
    "#### GLOBAL_CONFIG\n",
    "General control parameters for DEA runs.\n",
    "- `Data`: Path to dataset (e.g., fakename_5k.tsv).\n",
    "- `Overlap`: Proportion of shared entities between Alice and Eve.\n",
    "- `DropFrom`: Which party (Alice, Eve or both) gets the non-overlapping entities removed.\n",
    "- `MatchingMetric`, `Matching`: Used in GMA (e.g., cosine similarity, MinWeight matching).\n",
    "- `Workers`: Number of parallel threads (-1 = all available).\n",
    "- `BenchMode`, `DevMode`: Toggle benchmarking or development behaviors.\n",
    "\n",
    "#### DEA_CONFIG\n",
    "Training configuration for the neural network used in the Dataset Extension Attack.\n",
    "- `TrainSize`, `Epochs`, `Patience`: Classic train/test split and early stopping.\n",
    "- `NumSamples`: Number of tuning samples for Ray.\n",
    "- `MetricToOptimize`: Metric guiding model selection (e.g., `average_dice`).\n",
    "- `MatchingTechnique`: Post-processing method (e.g.,`fuzzy`, `greedy`, `ai`, `fuzzy_and_greedy`).\n",
    "\n",
    "#### ENC_CONFIG\n",
    "Controls how both Alice’s and Eve’s data are encoded.\n",
    "- `AliceAlgo`, `EveAlgo`: Chosen encoding methods (BloomFilter, TabMinHash, TwoStepHash).\n",
    "- Parameters are grouped by technique (BF, TMH, 2SH), but all are present to allow switching.\n",
    "\n",
    "#### EMB_CONFIG\n",
    "Defines embedding model (e.g., Node2Vec) parameters for both parties.\n",
    "- `Dim`, `Context`, `Negative`: Node2Vec embedding dimensions and context window.\n",
    "- `WalkLen`, `NWalks`, `P`, `Q`: Random walk hyperparameters.\n",
    "- `Quantile`, `Discretize`, `Normalize`: Post-embedding processing.\n",
    "\n",
    "#### ALIGN_CONFIG\n",
    "Parameters for alignment-based reconstruction.\n",
    "- `RegWS`, `RegInit`: Regularization weights.\n",
    "- `NIterWS`, `NIterInit`, `NEpochWS`: Iteration limits.\n",
    "- `Wasserstein`: Use Wasserstein-based alignment loss.\n",
    "- `EarlyStopping`, `LRDecay`: Learning stability controls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General Parameters ===\n",
    "GLOBAL_CONFIG = {\n",
    "    \"Data\": \"./data/datasets/titanic_full.tsv\",\n",
    "    \"Overlap\": 0.9,\n",
    "    \"DropFrom\": \"Both\",\n",
    "    \"Verbose\": False,\n",
    "    \"MatchingMetric\": \"cosine\",\n",
    "    \"Matching\": \"MinWeight\",\n",
    "    \"Workers\": os.cpu_count() - 1,\n",
    "    \"SaveAliceEncs\": False,\n",
    "    \"SaveEveEncs\": False,\n",
    "    \"DevMode\": False,\n",
    "    \"BenchMode\": True,\n",
    "    \"LoadResults\": False,\n",
    "    \"LoadPath\": \"\",\n",
    "    \"SaveResults\": True,\n",
    "}\n",
    "\n",
    "# === DEA Training Parameters ===\n",
    "DEA_CONFIG = {\n",
    "    \"TrainSize\": 0.8,\n",
    "    \"Patience\": 5,\n",
    "    \"MinDelta\": 1e-4,\n",
    "    \"NumSamples\": 125,\n",
    "    \"Epochs\": 20,\n",
    "    \"MetricToOptimize\": \"average_dice\",  # Options: \"average_dice\", \"average_precision\", ...\n",
    "    # Fuzyy works only if first three columns resemble: givenname, surname, birthdate (dd/mm/yyyy) format (naming of columns is irellevant)\n",
    "    \"MatchingTechnique\": \"fuzzy_and_greedy\",  # Options: \"ai\", \"greedy\", \"fuzzy\", ...\n",
    "}\n",
    "\n",
    "# === Encoding Parameters for Alice & Eve ===\n",
    "ENC_CONFIG = {\n",
    "    # Encoding technique\n",
    "    \"AliceAlgo\": \"BloomFilter\",\n",
    "    \"AliceSecret\": \"SuperSecretSalt1337\",\n",
    "    \"AliceN\": 2,\n",
    "    \"AliceMetric\": \"dice\",\n",
    "    \"EveAlgo\": \"BloomFilter\",\n",
    "    \"EveSecret\": \"ATotallyDifferentString42\",\n",
    "    \"EveN\": 2,\n",
    "    \"EveMetric\": \"dice\",\n",
    "\n",
    "    # Bloom Filter specific\n",
    "    \"AliceBFLength\": 1024,\n",
    "    \"AliceBits\": 10,\n",
    "    \"AliceDiffuse\": False,\n",
    "    \"AliceT\": 10,\n",
    "    \"AliceEldLength\": 1024,\n",
    "    \"EveBFLength\": 1024,\n",
    "    \"EveBits\": 10,\n",
    "    \"EveDiffuse\": False,\n",
    "    \"EveT\": 10,\n",
    "    \"EveEldLength\": 1024,\n",
    "\n",
    "    # Tabulation MinHash specific\n",
    "    \"AliceNHash\": 1024,\n",
    "    \"AliceNHashBits\": 64,\n",
    "    \"AliceNSubKeys\": 8,\n",
    "    \"Alice1BitHash\": True,\n",
    "    \"EveNHash\": 1024,\n",
    "    \"EveNHashBits\": 64,\n",
    "    \"EveNSubKeys\": 8,\n",
    "    \"Eve1BitHash\": True,\n",
    "\n",
    "    # Two-Step Hashing specific\n",
    "    \"AliceNHashFunc\": 10,\n",
    "    \"AliceNHashCol\": 1000,\n",
    "    \"AliceRandMode\": \"PNG\",\n",
    "    \"EveNHashFunc\": 10,\n",
    "    \"EveNHashCol\": 1000,\n",
    "    \"EveRandMode\": \"PNG\",\n",
    "}\n",
    "\n",
    "# === Embedding Configuration (e.g., Node2Vec) ===\n",
    "EMB_CONFIG = {\n",
    "    \"Algo\": \"Node2Vec\",\n",
    "    \"AliceQuantile\": 0.9,\n",
    "    \"AliceDiscretize\": False,\n",
    "    \"AliceDim\": 128,\n",
    "    \"AliceContext\": 10,\n",
    "    \"AliceNegative\": 1,\n",
    "    \"AliceNormalize\": True,\n",
    "    \"EveQuantile\": 0.9,\n",
    "    \"EveDiscretize\": False,\n",
    "    \"EveDim\": 128,\n",
    "    \"EveContext\": 10,\n",
    "    \"EveNegative\": 1,\n",
    "    \"EveNormalize\": True,\n",
    "    \"AliceWalkLen\": 100,\n",
    "    \"AliceNWalks\": 20,\n",
    "    \"AliceP\": 250,\n",
    "    \"AliceQ\": 300,\n",
    "    \"AliceEpochs\": 5,\n",
    "    \"AliceSeed\": 42,\n",
    "    \"EveWalkLen\": 100,\n",
    "    \"EveNWalks\": 20,\n",
    "    \"EveP\": 250,\n",
    "    \"EveQ\": 300,\n",
    "    \"EveEpochs\": 5,\n",
    "    \"EveSeed\": 42,\n",
    "}\n",
    "\n",
    "# === Graph Alignment Config ===\n",
    "ALIGN_CONFIG = {\n",
    "    \"RegWS\": max(0.1, GLOBAL_CONFIG[\"Overlap\"] / 2),\n",
    "    \"RegInit\": 1,\n",
    "    \"Batchsize\": 1,\n",
    "    \"LR\": 200.0,\n",
    "    \"NIterWS\": 100,\n",
    "    \"NIterInit\": 5,\n",
    "    \"NEpochWS\": 100,\n",
    "    \"LRDecay\": 1,\n",
    "    \"Sqrt\": True,\n",
    "    \"EarlyStopping\": 10,\n",
    "    \"Selection\": \"None\",\n",
    "    \"MaxLoad\": None,\n",
    "    \"Wasserstein\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define character sets\n",
    "alphabet = string.ascii_lowercase  # 'a' to 'z'\n",
    "digits = string.digits             # '0' to '9'\n",
    "\n",
    "# Generate 2-grams\n",
    "letter_letter_grams = [a + b for a in alphabet for b in alphabet]   # 'aa' to 'zz'\n",
    "digit_digit_grams = [d1 + d2 for d1 in digits for d2 in digits]     # '00' to '99'\n",
    "letter_digit_grams = [l + d for l in alphabet for d in digits]      # 'a0' to 'z9'\n",
    "\n",
    "# Combine all 2-gram types\n",
    "all_two_grams = letter_letter_grams + letter_digit_grams + digit_digit_grams\n",
    "\n",
    "# Map index to 2-gram string\n",
    "two_gram_dict = {i: two_gram for i, two_gram in enumerate(all_two_grams)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load or Compute Graph Matching Attack (GMA) Results\n",
    "\n",
    "This step ensures GMA results are available by either loading existing output files or triggering a new attack run.\n",
    "\n",
    "1. **Start Benchmark Timing (Optional):**  \n",
    "   If benchmarking is enabled (`BenchMode=True`), timers are started to measure runtime for the GMA computation.\n",
    "\n",
    "2. **Generate Configuration Hashes:**  \n",
    "   The function `get_hashes()` generates unique identifiers (hashes) based on the encoding (`ENC_CONFIG`) and embedding (`EMB_CONFIG`) settings for both Alice and Eve.  \n",
    "   These hashes are concatenated into a unique string identifier to distinguish different experiment setups.\n",
    "\n",
    "3. **Construct File Paths:**  \n",
    "   The identifier is used to define expected file paths for:\n",
    "   - `reidentified_individuals.h5` — records successfully linked between Eve and Alice  \n",
    "   - `not_reidentified_individuals.h5` — records not matched  \n",
    "   - `alice_data_complete_with_encoding.h5` — full encoded data for Alice  \n",
    "\n",
    "4. **Check for Existing Results:**  \n",
    "   If all three expected files exist, the GMA step is skipped. This avoids redundant computation.\n",
    "\n",
    "5. **Run GMA if Needed:**  \n",
    "   If any file is missing, the Graph Matching Attack is executed using `run_gma()`, which writes the results to disk using the same naming convention based on the identifier.\n",
    "\n",
    "6. **End Benchmark Timing (Optional):**  \n",
    "   If benchmarking is active, the elapsed time for the GMA step is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_total = time.time()\n",
    "    start_gma = time.time()\n",
    "\n",
    "# Get absolute path to data directory\n",
    "data_dir = os.path.abspath(\"./data\")\n",
    "\n",
    "# Generate encoding and embedding hashes for reproducible identifiers\n",
    "eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash = get_hashes(\n",
    "    GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG\n",
    ")\n",
    "identifier = f\"{eve_enc_hash}_{alice_enc_hash}_{eve_emb_hash}_{alice_emb_hash}\"\n",
    "\n",
    "# Build file paths for reidentified, not reidentified, and full encoded data\n",
    "path_reidentified = f\"{data_dir}/available_to_eve/reidentified_individuals_{identifier}.h5\"\n",
    "path_not_reidentified = f\"{data_dir}/available_to_eve/not_reidentified_individuals_{identifier}.h5\"\n",
    "path_all = f\"{data_dir}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\"\n",
    "\n",
    "# Run GMA only if the expected output files do not yet exist\n",
    "if not (\n",
    "    os.path.isfile(path_reidentified)\n",
    "    and os.path.isfile(path_not_reidentified)\n",
    "    and os.path.isfile(path_all)\n",
    "):\n",
    "    run_gma(\n",
    "        GLOBAL_CONFIG, ENC_CONFIG, EMB_CONFIG, ALIGN_CONFIG, DEA_CONFIG,\n",
    "        eve_enc_hash, alice_enc_hash, eve_emb_hash, alice_emb_hash\n",
    "    )\n",
    "\n",
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_gma = time.time() - start_gma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Training, Validation, and Test Data\n",
    "\n",
    "This step prepares the datasets needed for training and evaluation of the Dataset Extension Attack (DEA).  \n",
    "Depending on the selected encoding algorithm and available data, different dataset loaders are used.\n",
    "\n",
    "1. **Start Benchmark Timing (Optional):**  \n",
    "   If `BenchMode` is enabled, a timer is started to track the time spent in data preparation.\n",
    "\n",
    "2. **Load Reidentified and Not Reidentified Data:**  \n",
    "   - `reidentified_individuals_*.h5`: Contains entities successfully linked by the Graph Matching Attack.\n",
    "   - `not_reidentified_individuals_*.h5` and `alice_data_complete_with_encoding_*.h5`:  \n",
    "     Used to construct a labeled test set of entities not reidentified by the GMA.\n",
    "\n",
    "3. **Select Dataset Type Based on Encoding:**\n",
    "   - If `AliceAlgo` is set to `\"BloomFilter\"`: `BloomFilterDataset` is used.\n",
    "   - If `TabMinHash`: `TabMinHashDataset` is used.\n",
    "   - If `TwoStepHash`: `TwoStepHashDataset` is used with integer feature vectors.\n",
    "\n",
    "4. **Split Labeled Data into Train and Validation Sets:**  \n",
    "   The reidentified dataset is split according to `DEA_CONFIG[\"TrainSize\"]`.\n",
    "\n",
    "5. **Return Datasets:**  \n",
    "   The function returns:\n",
    "   - `data_train`: for model training  \n",
    "   - `data_val`: for validation during training  \n",
    "   - `data_test` (optional): for evaluating the reconstruction on not reidentified entities\n",
    "\n",
    "6. **Drop Redundant Columns (if needed):**  \n",
    "   The function `load_not_reidentified_data()` ensures compatibility of test data by removing unnecessary columns.\n",
    "\n",
    "7. **End Benchmark Timing (Optional):**  \n",
    "   If benchmarking is active, the elapsed time for data preparation is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_path(data_directory, identifier, alice_enc_hash, name=\"dataset\"):\n",
    "    os.makedirs(f\"{data_directory}/cache\", exist_ok=True)\n",
    "    return os.path.join(data_directory, \"cache\", f\"{name}_{identifier}_{alice_enc_hash}.pkl\")\n",
    "\n",
    "def load_data(data_directory, alice_enc_hash, identifier, load_test=False):\n",
    "    cache_path = get_cache_path(data_directory, identifier, alice_enc_hash)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            data_train, data_val, data_test = pickle.load(f)\n",
    "        return data_train, data_val, data_test\n",
    "\n",
    "    # Load from raw files\n",
    "    df_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/reidentified_individuals_{identifier}.h5\")\n",
    "\n",
    "    df_test = None\n",
    "    if load_test:\n",
    "        df_not_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/not_reidentified_individuals_{identifier}.h5\")\n",
    "        df_all = load_dataframe(f\"{data_directory}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\")\n",
    "        df_test = df_all[df_all[\"uid\"].isin(df_not_reidentified[\"uid\"])].reset_index(drop=True)\n",
    "\n",
    "    def get_encoding_dataset_class():\n",
    "        algo = ENC_CONFIG[\"AliceAlgo\"]\n",
    "        if algo == \"BloomFilter\":\n",
    "            return BloomFilterDataset\n",
    "        elif algo == \"TabMinHash\":\n",
    "            return TabMinHashDataset\n",
    "        elif algo == \"TwoStepHash\":\n",
    "            return TwoStepHashDataset\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoding algorithm: {algo}\")\n",
    "\n",
    "    DatasetClass = get_encoding_dataset_class()\n",
    "\n",
    "    if ENC_CONFIG[\"AliceAlgo\"] == \"TwoStepHash\":\n",
    "        unique_ints = sorted(set().union(*df_reidentified[\"twostephash\"]))\n",
    "        dataset_args = {\"all_integers\": unique_ints}\n",
    "    else:\n",
    "        dataset_args = {}\n",
    "\n",
    "    common_args = {\n",
    "        \"is_labeled\": True,\n",
    "        \"all_two_grams\": all_two_grams,\n",
    "        \"dev_mode\": GLOBAL_CONFIG[\"DevMode\"]\n",
    "    }\n",
    "\n",
    "    data_labeled = DatasetClass(df_reidentified, **common_args, **dataset_args)\n",
    "    data_test = DatasetClass(df_test, **common_args, **dataset_args) if load_test else None\n",
    "\n",
    "    train_size = int(DEA_CONFIG[\"TrainSize\"] * len(data_labeled))\n",
    "    val_size = len(data_labeled) - train_size\n",
    "    data_train, data_val = random_split(data_labeled, [train_size, val_size])\n",
    "\n",
    "    # Save to cache\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump((data_train, data_val, data_test), f)\n",
    "    return data_train, data_val, data_test\n",
    "\n",
    "\n",
    "def load_not_reidentified_data(data_directory, alice_enc_hash, identifier):\n",
    "        cache_path = get_cache_path(data_directory, identifier, alice_enc_hash, name=\"not_reidentified\")\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                df_filtered = pickle.load(f)\n",
    "            return df_filtered\n",
    "\n",
    "        df_not_reidentified = load_dataframe(f\"{data_directory}/available_to_eve/not_reidentified_individuals_{identifier}.h5\")\n",
    "        df_all = load_dataframe(f\"{data_directory}/dev/alice_data_complete_with_encoding_{alice_enc_hash}.h5\")\n",
    "\n",
    "        df_filtered = df_all[df_all[\"uid\"].isin(df_not_reidentified[\"uid\"])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        # Drop column by name instead of position if possible\n",
    "        drop_col = df_filtered.columns[-2]\n",
    "        df_filtered = df_filtered.drop(columns=[drop_col])\n",
    "\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(df_filtered, f)\n",
    "\n",
    "        return df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = GLOBAL_CONFIG[\"Data\"].split(\"/\")[-1].replace(\".tsv\", \"\")\n",
    "experiment_tag = \"experiment_\" + ENC_CONFIG[\"AliceAlgo\"] + \"_\" + selected_dataset + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_to = f\"experiment_results/{experiment_tag}\"\n",
    "os.makedirs(save_to, exist_ok=True)\n",
    "\n",
    "# Combine all configs into one dictionary\n",
    "all_configs = {\n",
    "    \"GLOBAL_CONFIG\": GLOBAL_CONFIG,\n",
    "    \"DEA_CONFIG\": DEA_CONFIG,\n",
    "    \"ENC_CONFIG\": ENC_CONFIG,\n",
    "    \"EMB_CONFIG\": EMB_CONFIG,\n",
    "    \"ALIGN_CONFIG\": ALIGN_CONFIG\n",
    "}\n",
    "\n",
    "# Save as a readable .txt file\n",
    "with open(os.path.join(save_to, \"config.txt\"), \"w\") as f:\n",
    "    for config_name, config_dict in all_configs.items():\n",
    "        f.write(f\"# === {config_name} ===\\n\")\n",
    "        f.write(json.dumps(config_dict, indent=4))\n",
    "        f.write(\"\\n\\n\")\n",
    "os.makedirs(f\"{save_to}/hyperparameteroptimization\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val, data_test = load_data(data_dir, alice_enc_hash, identifier, load_test=True)\n",
    "df_not_reidentified = load_not_reidentified_data(data_dir, alice_enc_hash, identifier)\n",
    "# Exit the function if any of the data frames are empty\n",
    "if len(data_train) == 0 or len(data_val) == 0 or len(data_test) == 0 or df_not_reidentified.empty:\n",
    "    log_path = os.path.join(save_to, \"termination_log.txt\")\n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(\"Training process canceled due to empty dataset.\\n\")\n",
    "        f.write(f\"Length of data_train: {len(data_train)}\\n\")\n",
    "        f.write(f\"Length of data_val: {len(data_val)}\\n\")\n",
    "        f.write(f\"Length of data_test: {len(data_test)}\\n\")\n",
    "        f.write(f\"Length of df_not_reidentified: {len(df_not_reidentified)}\\n\")\n",
    "    print(\"One or more datasets are empty. Termination log written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_hyperparameter_optimization = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, data_dir, output_dim, alice_enc_hash, identifier, patience, min_delta):\n",
    "    # Create DataLoaders for training, validation, and testing\n",
    "\n",
    "    data_train, data_val, _ = load_data(data_dir, alice_enc_hash, identifier, load_test=False)\n",
    "\n",
    "    input_dim = data_train[0][0].shape[0]  # Get the input dimension from the first sample\n",
    "\n",
    "    dataloader_train = DataLoader(\n",
    "        data_train,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,  # Important for training\n",
    "    )\n",
    "\n",
    "    dataloader_val = DataLoader(\n",
    "        data_val,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    total_precision = total_recall = total_f1 = total_dice = total_val_loss = 0.0\n",
    "    num_samples = 0\n",
    "    epochs = 0\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "\n",
    "    # Define and initialize model with hyperparameters from config\n",
    "    model = BaseModelHyperparameterOptimization(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        activation_fn=config[\"activation_fn\"]\n",
    "    )\n",
    "\n",
    "    # Set device for model (GPU or CPU)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Select loss function based on config\n",
    "    loss_functions = {\n",
    "        \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss(),\n",
    "        \"MultiLabelSoftMarginLoss\": nn.MultiLabelSoftMarginLoss(),\n",
    "        \"SoftMarginLoss\": nn.SoftMarginLoss(),\n",
    "    }\n",
    "    criterion = loss_functions[config[\"loss_fn\"]]\n",
    "\n",
    "    learning_rate = config[\"optimizer\"][\"lr\"].sample()\n",
    "    # Select optimizer based on config\n",
    "    optimizers = {\n",
    "        \"Adam\": lambda: optim.Adam(model.parameters(), lr=learning_rate),\n",
    "        \"AdamW\": lambda: optim.AdamW(model.parameters(), lr=learning_rate),\n",
    "        \"SGD\": lambda: optim.SGD(model.parameters(), lr=learning_rate, momentum=config[\"optimizer\"][\"momentum\"].sample()),\n",
    "        \"RMSprop\": lambda: optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    }\n",
    "    optimizer = optimizers[config[\"optimizer\"][\"name\"]]()\n",
    "\n",
    "    schedulers = {\n",
    "        \"StepLR\": lambda: torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config[\"lr_scheduler\"][\"step_size\"].sample(),\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ExponentialLR\": lambda: torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=config[\"lr_scheduler\"][\"gamma\"].sample()\n",
    "        ),\n",
    "        \"ReduceLROnPlateau\": lambda: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=config[\"lr_scheduler\"][\"mode\"],\n",
    "            factor=config[\"lr_scheduler\"][\"factor\"].sample(),\n",
    "            patience=config[\"lr_scheduler\"][\"patience\"].sample()\n",
    "        ),\n",
    "        \"CosineAnnealingLR\": lambda: torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config[\"lr_scheduler\"][\"T_max\"].sample(),\n",
    "            eta_min=config[\"lr_scheduler\"][\"eta_min\"].sample()\n",
    "        ),\n",
    "        \"CyclicLR\": lambda: torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=config[\"lr_scheduler\"][\"base_lr\"].sample(),\n",
    "            max_lr=config[\"lr_scheduler\"][\"max_lr\"].sample(),\n",
    "            step_size_up=config[\"lr_scheduler\"][\"step_size_up\"].sample(),\n",
    "            mode=config[\"lr_scheduler\"][\"mode_cyclic\"].sample(),\n",
    "            cycle_momentum=False\n",
    "        ),\n",
    "        \"None\": lambda: None,\n",
    "    }\n",
    "    scheduler = schedulers[config[\"lr_scheduler\"][\"name\"]]()\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(DEA_CONFIG[\"Epochs\"]):\n",
    "        epochs += 1\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = run_epoch(model, dataloader_train, criterion, optimizer, device, is_training=True, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(model, dataloader_val, criterion, optimizer, device, is_training=False, verbose=GLOBAL_CONFIG[\"Verbose\"], scheduler=scheduler)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        total_val_loss += val_loss\n",
    "\n",
    "         # Early stopping check\n",
    "        if early_stopper(val_loss):\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    # Test phase with reconstruction and evaluation\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in dataloader_val:\n",
    "\n",
    "            actual_two_grams = decode_labels_to_two_grams(two_gram_dict, labels)\n",
    "\n",
    "            # Move data to device and make predictions\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities into 2-gram scores\n",
    "            batch_two_gram_scores = map_probabilities_to_two_grams(two_gram_dict, probabilities)\n",
    "\n",
    "            # Filter out low-scoring 2-grams\n",
    "            batch_filtered_two_gram_scores = filter_high_scoring_two_grams(batch_two_gram_scores, config[\"threshold\"])\n",
    "\n",
    "            # Calculate performance metrics for evaluation\n",
    "            dice, precision, recall, f1 = calculate_performance_metrics(\n",
    "                actual_two_grams, batch_filtered_two_gram_scores)\n",
    "\n",
    "            total_dice += dice\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "            num_samples += data.size(0) # Batch Size\n",
    "\n",
    "    train.report({\n",
    "            \"average_dice\": total_dice / num_samples,\n",
    "            \"average_precision\": total_precision / num_samples,\n",
    "            \"average_recall\": total_recall / num_samples,\n",
    "            \"average_f1\": total_f1 / num_samples,\n",
    "            \"total_val_loss\": total_val_loss,\n",
    "            \"len_train\": len(dataloader_train.dataset),\n",
    "            \"len_val\": len(dataloader_val.dataset),\n",
    "            \"epochs\": epochs\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-07-11 17:57:39</td></tr>\n",
       "<tr><td>Running for: </td><td>00:05:44.04        </td></tr>\n",
       "<tr><td>Memory:      </td><td>16.3/32.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=65<br>Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: -0.8647065482926454<br>Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th>activation_fn  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_layer_size</th><th>loss_fn             </th><th>lr_scheduler/T_max  </th><th>lr_scheduler/base_lr  </th><th>lr_scheduler/eta_min  </th><th>lr_scheduler/factor  </th><th>lr_scheduler/gamma  </th><th>lr_scheduler/max_lr  </th><th>lr_scheduler/mode  </th><th>lr_scheduler/mode_cy\n",
       "clic                     </th><th>lr_scheduler/name  </th><th>lr_scheduler/patienc\n",
       "e                     </th><th>lr_scheduler/step_si\n",
       "ze                     </th><th>lr_scheduler/step_si\n",
       "ze_up                     </th><th style=\"text-align: right;\">  num_layers</th><th>optimizer/lr        </th><th>optimizer/momentum  </th><th>optimizer/name  </th><th style=\"text-align: right;\">  threshold</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  average_dice</th><th style=\"text-align: right;\">  average_precision</th><th style=\"text-align: right;\">  average_recall</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_8bfef5f4</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>selu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.32441 </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_b4c0 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_9990</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_d0c0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.620131</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4.22807 </td><td style=\"text-align: right;\">     0.201314 </td><td style=\"text-align: right;\">          0.947242 </td><td style=\"text-align: right;\">     0.11712    </td></tr>\n",
       "<tr><td>train_model_e3ae8e13</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.37198 </td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_69b0 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_69e0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1f00</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.337356</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.20669 </td><td style=\"text-align: right;\">     0.260488 </td><td style=\"text-align: right;\">          0.198387 </td><td style=\"text-align: right;\">     0.408544   </td></tr>\n",
       "<tr><td>train_model_d13558fa</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.182894</td><td style=\"text-align: right;\">                128</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_09a0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_2020</td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_3190</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.744803</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.17665 </td><td style=\"text-align: right;\">     0        </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">     0          </td></tr>\n",
       "<tr><td>train_model_e41a7559</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.23606 </td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_8bb0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_8850</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.342936</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       13.8963  </td><td style=\"text-align: right;\">     0.267737 </td><td style=\"text-align: right;\">          0.204055 </td><td style=\"text-align: right;\">     0.419427   </td></tr>\n",
       "<tr><td>train_model_17b54ef4</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>leaky_relu     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.108352</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>&lt;ray.tune.searc_2e00</td><td>                      </td><td>&lt;ray.tune.searc_0e80  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_2f80</td><td>&lt;ray.tune.searc_27a0</td><td>SGD             </td><td style=\"text-align: right;\">   0.775851</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.704033</td><td style=\"text-align: right;\">     0        </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">     0          </td></tr>\n",
       "<tr><td>train_model_362a0584</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.196948</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_25c0</td><td>                      </td><td>&lt;ray.tune.searc_1f30  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1930</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.500017</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       10.6918  </td><td style=\"text-align: right;\">     0.0951014</td><td style=\"text-align: right;\">          0.733813 </td><td style=\"text-align: right;\">     0.0513346  </td></tr>\n",
       "<tr><td>train_model_6037cd3a</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>leaky_relu     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.143494</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_3a30  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_1ea0 </td><td>                   </td><td>&lt;ray.tune.searc_0ee0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_0af0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_c490</td><td>&lt;ray.tune.searc_03a0</td><td>SGD             </td><td style=\"text-align: right;\">   0.339813</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.07416 </td><td style=\"text-align: right;\">     0.0162468</td><td style=\"text-align: right;\">          0.0124264</td><td style=\"text-align: right;\">     0.0252922  </td></tr>\n",
       "<tr><td>train_model_ed3709cd</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>leaky_relu     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.109618</td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_8dc0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_d0c0</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_ca30</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.768395</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.52879 </td><td style=\"text-align: right;\">     0        </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">     0          </td></tr>\n",
       "<tr><td>train_model_6fc80fab</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>leaky_relu     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.106414</td><td style=\"text-align: right;\">                512</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_8bb0</td><td>                      </td><td>&lt;ray.tune.searc_9450  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_9ff0</td><td>&lt;ray.tune.searc_b670</td><td>SGD             </td><td style=\"text-align: right;\">   0.436743</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.55356 </td><td style=\"text-align: right;\">     0.0224878</td><td style=\"text-align: right;\">          0.0176586</td><td style=\"text-align: right;\">     0.0339662  </td></tr>\n",
       "<tr><td>train_model_f1b813e5</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>leaky_relu     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.104544</td><td style=\"text-align: right;\">                512</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_7100  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_5600 </td><td>                   </td><td>&lt;ray.tune.searc_61d0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_6b60</td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_0550</td><td>&lt;ray.tune.searc_5810</td><td>SGD             </td><td style=\"text-align: right;\">   0.314307</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.63577 </td><td style=\"text-align: right;\">     0.017277 </td><td style=\"text-align: right;\">          0.0135165</td><td style=\"text-align: right;\">     0.025423   </td></tr>\n",
       "<tr><td>train_model_40ef2026</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>leaky_relu     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.300714</td><td style=\"text-align: right;\">                128</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_c790</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.771914</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.06707 </td><td style=\"text-align: right;\">     0.0499525</td><td style=\"text-align: right;\">          0.47482  </td><td style=\"text-align: right;\">     0.0264499  </td></tr>\n",
       "<tr><td>train_model_826299e2</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.111313</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_5990</td><td>&lt;ray.tune.searc_a950</td><td>SGD             </td><td style=\"text-align: right;\">   0.705844</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.791412</td><td style=\"text-align: right;\">     0        </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">     0          </td></tr>\n",
       "<tr><td>train_model_a5b60b17</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>selu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.14876 </td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_ec80</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_d870</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a1a0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.392413</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.10854 </td><td style=\"text-align: right;\">     0.0701086</td><td style=\"text-align: right;\">          0.0529758</td><td style=\"text-align: right;\">     0.111965   </td></tr>\n",
       "<tr><td>train_model_b62684e9</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.382732</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_f790 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_f610</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_f2e0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.421825</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.73452 </td><td style=\"text-align: right;\">     0.286019 </td><td style=\"text-align: right;\">          0.216699 </td><td style=\"text-align: right;\">     0.452969   </td></tr>\n",
       "<tr><td>train_model_51fe22bf</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.307945</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_e1d0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_d690</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.420474</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.52429 </td><td style=\"text-align: right;\">     0.278488 </td><td style=\"text-align: right;\">          0.212121 </td><td style=\"text-align: right;\">     0.436795   </td></tr>\n",
       "<tr><td>train_model_b2c1cb5d</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.393   </td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_e350</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_f160</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.39977 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.62159 </td><td style=\"text-align: right;\">     0.291993 </td><td style=\"text-align: right;\">          0.221714 </td><td style=\"text-align: right;\">     0.46127    </td></tr>\n",
       "<tr><td>train_model_cf2df438</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.395537</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_cfd0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_4be0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.469929</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.05667 </td><td style=\"text-align: right;\">     0.301194 </td><td style=\"text-align: right;\">          0.228254 </td><td style=\"text-align: right;\">     0.477749   </td></tr>\n",
       "<tr><td>train_model_83c86745</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.243299</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_7730</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_5bd0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.467514</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       10.5478  </td><td style=\"text-align: right;\">     0.292658 </td><td style=\"text-align: right;\">          0.222586 </td><td style=\"text-align: right;\">     0.461164   </td></tr>\n",
       "<tr><td>train_model_5dda06d5</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>gelu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.397928</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_7190</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_41f0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.526398</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.81489 </td><td style=\"text-align: right;\">     0.287741 </td><td style=\"text-align: right;\">          0.218225 </td><td style=\"text-align: right;\">     0.455568   </td></tr>\n",
       "<tr><td>train_model_b307a739</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>tanh           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.388992</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_2950 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_31c0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_2e60</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.52069 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.0547  </td><td style=\"text-align: right;\">     0.0951014</td><td style=\"text-align: right;\">          0.733813 </td><td style=\"text-align: right;\">     0.0513346  </td></tr>\n",
       "<tr><td>train_model_032d6068</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>tanh           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.338224</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_ccd0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_e9b0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.536955</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.28178 </td><td style=\"text-align: right;\">     0.0250748</td><td style=\"text-align: right;\">          0.0191847</td><td style=\"text-align: right;\">     0.0389002  </td></tr>\n",
       "<tr><td>train_model_088781f9</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.342087</td><td style=\"text-align: right;\">               1024</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_cd90</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_d4e0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.595674</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.97622 </td><td style=\"text-align: right;\">     0.0951014</td><td style=\"text-align: right;\">          0.733813 </td><td style=\"text-align: right;\">     0.0513346  </td></tr>\n",
       "<tr><td>train_model_b72b8e70</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.26237 </td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_6710</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_54e0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.485158</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.56562 </td><td style=\"text-align: right;\">     0.0628245</td><td style=\"text-align: right;\">          0.0483976</td><td style=\"text-align: right;\">     0.097091   </td></tr>\n",
       "<tr><td>train_model_8fc1c276</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.275175</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_db70</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_ea40</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.468088</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.25252 </td><td style=\"text-align: right;\">     0.294512 </td><td style=\"text-align: right;\">          0.223458 </td><td style=\"text-align: right;\">     0.466593   </td></tr>\n",
       "<tr><td>train_model_624b2ad0</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>gelu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.353012</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_0070</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_22c0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.604507</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.77447 </td><td style=\"text-align: right;\">     0.0012964</td><td style=\"text-align: right;\">          0.0107914</td><td style=\"text-align: right;\">     0.000690844</td></tr>\n",
       "<tr><td>train_model_987fec0c</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>gelu           </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.36187 </td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_c340</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_2a70</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.603094</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.43893 </td><td style=\"text-align: right;\">     0        </td><td style=\"text-align: right;\">          0        </td><td style=\"text-align: right;\">     0          </td></tr>\n",
       "<tr><td>train_model_b31c158b</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.357256</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_8a30</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_a6e0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.463549</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.64125 </td><td style=\"text-align: right;\">     0.270443 </td><td style=\"text-align: right;\">          0.205581 </td><td style=\"text-align: right;\">     0.425664   </td></tr>\n",
       "<tr><td>train_model_81d41d07</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.271969</td><td style=\"text-align: right;\">               1024</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_afb0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_79d0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.459739</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.23185 </td><td style=\"text-align: right;\">     0.062095 </td><td style=\"text-align: right;\">          0.0475256</td><td style=\"text-align: right;\">     0.0972457  </td></tr>\n",
       "<tr><td>train_model_8fbb691b</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>elu            </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.265927</td><td style=\"text-align: right;\">                256</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_a8c0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_b1c0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.462356</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.864885</td><td style=\"text-align: right;\">     0.221176 </td><td style=\"text-align: right;\">          0.167212 </td><td style=\"text-align: right;\">     0.352759   </td></tr>\n",
       "<tr><td>train_model_72a18704</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.277162</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_ad10</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_b730</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.464058</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        7.91933 </td><td style=\"text-align: right;\">     0.303154 </td><td style=\"text-align: right;\">          0.230216 </td><td style=\"text-align: right;\">     0.478803   </td></tr>\n",
       "<tr><td>train_model_552a539b</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.265614</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_3b80</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_e200</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.576586</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       10.5943  </td><td style=\"text-align: right;\">     0.243895 </td><td style=\"text-align: right;\">          0.186464 </td><td style=\"text-align: right;\">     0.375923   </td></tr>\n",
       "<tr><td>train_model_43832b15</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.234503</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_81c0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_e170 </td><td>                   </td><td>&lt;ray.tune.searc_ff40</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_e4a0</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_68f0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.565258</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       17.1684  </td><td style=\"text-align: right;\">     0.278063 </td><td style=\"text-align: right;\">          0.211685 </td><td style=\"text-align: right;\">     0.437719   </td></tr>\n",
       "<tr><td>train_model_a8113d44</td><td>TERMINATED</td><td>127.0.0.1:51602</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.21599 </td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_82b0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_82e0 </td><td>                   </td><td>&lt;ray.tune.searc_b370</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_ac50</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_8130</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.570614</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       16.451   </td><td style=\"text-align: right;\">     0.257891 </td><td style=\"text-align: right;\">          0.194463 </td><td style=\"text-align: right;\">     0.410981   </td></tr>\n",
       "<tr><td>train_model_a4fc31d4</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.22232 </td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_c4f0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.640113</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       14.5131  </td><td style=\"text-align: right;\">     0.263299 </td><td style=\"text-align: right;\">          0.199913 </td><td style=\"text-align: right;\">     0.415091   </td></tr>\n",
       "<tr><td>train_model_131946a7</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.217646</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a170</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.550595</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       51.8773  </td><td style=\"text-align: right;\">     0.630091 </td><td style=\"text-align: right;\">          0.987699 </td><td style=\"text-align: right;\">     0.478261   </td></tr>\n",
       "<tr><td>train_model_1f532af8</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.229381</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>&lt;ray.tune.searc_9c60  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_b3a0 </td><td>                   </td><td>&lt;ray.tune.searc_5840</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_a6e0</td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_8ac0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.560636</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        7.78587 </td><td style=\"text-align: right;\">     0.259345 </td><td style=\"text-align: right;\">          0.194463 </td><td style=\"text-align: right;\">     0.418218   </td></tr>\n",
       "<tr><td>train_model_09e9e861</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.221117</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_bdc0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.37936 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        8.53034 </td><td style=\"text-align: right;\">     0.291254 </td><td style=\"text-align: right;\">          0.219969 </td><td style=\"text-align: right;\">     0.463117   </td></tr>\n",
       "<tr><td>train_model_0d328de9</td><td>TERMINATED</td><td>127.0.0.1:51602</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.298867</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_9690</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.369104</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       20.7228  </td><td style=\"text-align: right;\">     0.285693 </td><td style=\"text-align: right;\">          0.216699 </td><td style=\"text-align: right;\">     0.452132   </td></tr>\n",
       "<tr><td>train_model_f869f7c5</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.292457</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_2b90</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.652146</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       19.03    </td><td style=\"text-align: right;\">     0.291386 </td><td style=\"text-align: right;\">          0.221278 </td><td style=\"text-align: right;\">     0.460578   </td></tr>\n",
       "<tr><td>train_model_641aed39</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.291668</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_1ba0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_0700</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.38146 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        8.94057 </td><td style=\"text-align: right;\">     0.0379187</td><td style=\"text-align: right;\">          0.028777 </td><td style=\"text-align: right;\">     0.0602292  </td></tr>\n",
       "<tr><td>train_model_2cbd6f97</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.288766</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_5de0</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_d660</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.376079</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       11.2567  </td><td style=\"text-align: right;\">     0.295633 </td><td style=\"text-align: right;\">          0.224984 </td><td style=\"text-align: right;\">     0.465547   </td></tr>\n",
       "<tr><td>train_model_4fb96609</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.289331</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_2710</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_0040</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.498265</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4.88162 </td><td style=\"text-align: right;\">     0.0330734</td><td style=\"text-align: right;\">          0.0248528</td><td style=\"text-align: right;\">     0.0531695  </td></tr>\n",
       "<tr><td>train_model_cad2cf0a</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>selu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.302696</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_f280</td><td>                     </td><td>                   </td><td>                    </td><td>ExponentialLR      </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_dff0</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.494206</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        9.99247 </td><td style=\"text-align: right;\">     0.0227784</td><td style=\"text-align: right;\">          0.0176586</td><td style=\"text-align: right;\">     0.0340545  </td></tr>\n",
       "<tr><td>train_model_d85280fc</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.317479</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_dcc0 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_e770</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_6890</td><td>                    </td><td>AdamW           </td><td style=\"text-align: right;\">   0.483054</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.49914 </td><td style=\"text-align: right;\">     0.152361 </td><td style=\"text-align: right;\">          0.858513 </td><td style=\"text-align: right;\">     0.0865142  </td></tr>\n",
       "<tr><td>train_model_23500ef9</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.319144</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_d2a0</td><td>                      </td><td>&lt;ray.tune.searc_d660  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_5c30</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.65912 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       11.1357  </td><td style=\"text-align: right;\">     0.580468 </td><td style=\"text-align: right;\">          0.992175 </td><td style=\"text-align: right;\">     0.428936   </td></tr>\n",
       "<tr><td>train_model_94051b7d</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.320179</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_0910</td><td>                      </td><td>&lt;ray.tune.searc_0f10  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_91e0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.364048</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       16.1819  </td><td style=\"text-align: right;\">     0.746968 </td><td style=\"text-align: right;\">          0.935434 </td><td style=\"text-align: right;\">     0.634553   </td></tr>\n",
       "<tr><td>train_model_1857888d</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.197307</td><td style=\"text-align: right;\">                128</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_0970</td><td>                      </td><td>&lt;ray.tune.searc_3760  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_3d30</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.359628</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.30881 </td><td style=\"text-align: right;\">     0.0665612</td><td style=\"text-align: right;\">          0.0501417</td><td style=\"text-align: right;\">     0.106756   </td></tr>\n",
       "<tr><td>train_model_b231eb53</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.249299</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>&lt;ray.tune.searc_fd00</td><td>                      </td><td>&lt;ray.tune.searc_e290  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_e1d0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.358662</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       32.0402  </td><td style=\"text-align: right;\">     0.28682  </td><td style=\"text-align: right;\">          0.218225 </td><td style=\"text-align: right;\">     0.450679   </td></tr>\n",
       "<tr><td>train_model_174021f3</td><td>TERMINATED</td><td>127.0.0.1:51602</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.248146</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>&lt;ray.tune.searc_19f0</td><td>                      </td><td>&lt;ray.tune.searc_3be0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_f970</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.432737</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       35.9367  </td><td style=\"text-align: right;\">     0.284628 </td><td style=\"text-align: right;\">          0.216263 </td><td style=\"text-align: right;\">     0.448229   </td></tr>\n",
       "<tr><td>train_model_8eba9c7d</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.248098</td><td style=\"text-align: right;\">                128</td><td>SoftMarginLoss      </td><td>&lt;ray.tune.searc_6290</td><td>                      </td><td>&lt;ray.tune.searc_a560  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_1300</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.350232</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.25057 </td><td style=\"text-align: right;\">     0.181947 </td><td style=\"text-align: right;\">          0.137999 </td><td style=\"text-align: right;\">     0.287544   </td></tr>\n",
       "<tr><td>train_model_de71d77a</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>relu           </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.27639 </td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_24a0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_29b0</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_21d0</td><td>&lt;ray.tune.searc_c640</td><td>SGD             </td><td style=\"text-align: right;\">   0.433669</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        7.3984  </td><td style=\"text-align: right;\">     0.0290388</td><td style=\"text-align: right;\">          0.0222368</td><td style=\"text-align: right;\">     0.0445095  </td></tr>\n",
       "<tr><td>train_model_6eb3eb85</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>elu            </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">      0.246085</td><td style=\"text-align: right;\">               2048</td><td>SoftMarginLoss      </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_b3d0</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_baf0</td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_9de0</td><td>&lt;ray.tune.searc_85e0</td><td>SGD             </td><td style=\"text-align: right;\">   0.303537</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        7.00164 </td><td style=\"text-align: right;\">     0.0217532</td><td style=\"text-align: right;\">          0.0170046</td><td style=\"text-align: right;\">     0.0321766  </td></tr>\n",
       "<tr><td>train_model_81481047</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.331489</td><td style=\"text-align: right;\">                256</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_9ff0</td><td>                      </td><td>&lt;ray.tune.searc_c190  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           1</td><td>&lt;ray.tune.searc_94b0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.701999</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.42278 </td><td style=\"text-align: right;\">     0.0935029</td><td style=\"text-align: right;\">          0.726619 </td><td style=\"text-align: right;\">     0.0504354  </td></tr>\n",
       "<tr><td>train_model_d027211b</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.324869</td><td style=\"text-align: right;\">                256</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_6080</td><td>                      </td><td>&lt;ray.tune.searc_57e0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6ad0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.311493</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.58056 </td><td style=\"text-align: right;\">     0.173166 </td><td style=\"text-align: right;\">          0.451079 </td><td style=\"text-align: right;\">     0.11091    </td></tr>\n",
       "<tr><td>train_model_8ac476c0</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.332089</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_1420</td><td>                      </td><td>&lt;ray.tune.searc_6b00  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_09d0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.313994</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       26.61    </td><td style=\"text-align: right;\">     0.174548 </td><td style=\"text-align: right;\">          0.434053 </td><td style=\"text-align: right;\">     0.111948   </td></tr>\n",
       "<tr><td>train_model_2a9cf505</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.321516</td><td style=\"text-align: right;\">                256</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_97b0</td><td>                      </td><td>&lt;ray.tune.searc_88e0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_b970</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.706398</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.7271  </td><td style=\"text-align: right;\">     0.0951014</td><td style=\"text-align: right;\">          0.733813 </td><td style=\"text-align: right;\">     0.0513346  </td></tr>\n",
       "<tr><td>train_model_2b8e14c2</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>selu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.324274</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_0a00</td><td>                      </td><td>&lt;ray.tune.searc_3490  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_0e50</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.31951 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       27.2072  </td><td style=\"text-align: right;\">     0.242952 </td><td style=\"text-align: right;\">          0.632845 </td><td style=\"text-align: right;\">     0.154886   </td></tr>\n",
       "<tr><td>train_model_8a13d06a</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.314851</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_32e0</td><td>                      </td><td>&lt;ray.tune.searc_6800  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_cbe0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.409905</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       26.3868  </td><td style=\"text-align: right;\">     0.339352 </td><td style=\"text-align: right;\">          0.848566 </td><td style=\"text-align: right;\">     0.224551   </td></tr>\n",
       "<tr><td>train_model_b6265808</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>tanh           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.370868</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_da50</td><td>                      </td><td>&lt;ray.tune.searc_aa40  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           3</td><td>&lt;ray.tune.searc_d270</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.410171</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       27.5132  </td><td style=\"text-align: right;\">     0.0951014</td><td style=\"text-align: right;\">          0.733813 </td><td style=\"text-align: right;\">     0.0513346  </td></tr>\n",
       "<tr><td>train_model_e163814b</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.3794  </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_19c0</td><td>                      </td><td>&lt;ray.tune.searc_1990  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_44c0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.408783</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       18.9567  </td><td style=\"text-align: right;\">     0.544777 </td><td style=\"text-align: right;\">          0.917896 </td><td style=\"text-align: right;\">     0.405332   </td></tr>\n",
       "<tr><td>train_model_8c8f9f19</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.159217</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_5de0</td><td>                      </td><td>&lt;ray.tune.searc_5750  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a0b0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.333376</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       19.1126  </td><td style=\"text-align: right;\">     0.727001 </td><td style=\"text-align: right;\">          0.898839 </td><td style=\"text-align: right;\">     0.626876   </td></tr>\n",
       "<tr><td>train_model_7be6baa5</td><td>TERMINATED</td><td>127.0.0.1:51602</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.372992</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_a260</td><td>                      </td><td>&lt;ray.tune.searc_a020  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_ad70</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.404692</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       35.8301  </td><td style=\"text-align: right;\">     0.765976 </td><td style=\"text-align: right;\">          0.958554 </td><td style=\"text-align: right;\">     0.649413   </td></tr>\n",
       "<tr><td>train_model_1b41f01b</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>tanh           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.17197 </td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_7c10</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.404966</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       17.457   </td><td style=\"text-align: right;\">     0.647696 </td><td style=\"text-align: right;\">          0.929487 </td><td style=\"text-align: right;\">     0.51335    </td></tr>\n",
       "<tr><td>train_model_6860fb91</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>tanh           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.206011</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_2e00</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.40825 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       17.0698  </td><td style=\"text-align: right;\">     0.0959144</td><td style=\"text-align: right;\">          0.733813 </td><td style=\"text-align: right;\">     0.0517809  </td></tr>\n",
       "<tr><td>train_model_67ff7e64</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.374997</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_78b0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.416101</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       31.6683  </td><td style=\"text-align: right;\">     0.802588 </td><td style=\"text-align: right;\">          0.978427 </td><td style=\"text-align: right;\">     0.694132   </td></tr>\n",
       "<tr><td>train_model_55fcd482</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>leaky_relu     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.181246</td><td style=\"text-align: right;\">                512</td><td>BCEWithLogitsLoss   </td><td>&lt;ray.tune.searc_7a30</td><td>                      </td><td>&lt;ray.tune.searc_fca0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_4df0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.406691</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.24948 </td><td style=\"text-align: right;\">     0.312573 </td><td style=\"text-align: right;\">          0.876121 </td><td style=\"text-align: right;\">     0.201204   </td></tr>\n",
       "<tr><td>train_model_ddb176ba</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.376989</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_17b0</td><td>                      </td><td>&lt;ray.tune.searc_b850  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6710</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.423258</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       21.0597  </td><td style=\"text-align: right;\">     0.1188   </td><td style=\"text-align: right;\">          0.760192 </td><td style=\"text-align: right;\">     0.066061   </td></tr>\n",
       "<tr><td>train_model_df2e084a</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.164272</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_4c70</td><td>                      </td><td>&lt;ray.tune.searc_7fa0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_d570</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.409288</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.6153  </td><td style=\"text-align: right;\">     0.571912 </td><td style=\"text-align: right;\">          0.937997 </td><td style=\"text-align: right;\">     0.42939    </td></tr>\n",
       "<tr><td>train_model_c70e7171</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>leaky_relu     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.160266</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_bb80</td><td>                      </td><td>&lt;ray.tune.searc_6470  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_d2a0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.400943</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       19.0971  </td><td style=\"text-align: right;\">     0.313442 </td><td style=\"text-align: right;\">          0.897923 </td><td style=\"text-align: right;\">     0.201123   </td></tr>\n",
       "<tr><td>train_model_9d60c155</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>leaky_relu     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.148651</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_7a60</td><td>                      </td><td>&lt;ray.tune.searc_6fe0  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_4580</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.335126</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.23134 </td><td style=\"text-align: right;\">     0.22498  </td><td style=\"text-align: right;\">          0.699914 </td><td style=\"text-align: right;\">     0.142409   </td></tr>\n",
       "<tr><td>train_model_f44b24cf</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>tanh           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.140896</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_d540</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.397852</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       17.5464  </td><td style=\"text-align: right;\">     0.473304 </td><td style=\"text-align: right;\">          0.924783 </td><td style=\"text-align: right;\">     0.33556    </td></tr>\n",
       "<tr><td>train_model_cd5cd063</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.166418</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>&lt;ray.tune.searc_7af0</td><td>                      </td><td>&lt;ray.tune.searc_7f40  </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>CosineAnnealingLR  </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6fb0</td><td>                    </td><td>Adam            </td><td style=\"text-align: right;\">   0.33172 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       21.767   </td><td style=\"text-align: right;\">     0.702818 </td><td style=\"text-align: right;\">          0.919065 </td><td style=\"text-align: right;\">     0.582473   </td></tr>\n",
       "<tr><td>train_model_fe3fcb02</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.161256</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_b040</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.332583</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       17.6507  </td><td style=\"text-align: right;\">     0.734266 </td><td style=\"text-align: right;\">          0.81891  </td><td style=\"text-align: right;\">     0.679285   </td></tr>\n",
       "<tr><td>train_model_bec76d6f</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>relu           </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.170198</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_5b40</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.445601</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       17.4717  </td><td style=\"text-align: right;\">     0.334634 </td><td style=\"text-align: right;\">          0.911277 </td><td style=\"text-align: right;\">     0.215727   </td></tr>\n",
       "<tr><td>train_model_b9155ec9</td><td>TERMINATED</td><td>127.0.0.1:51602</td><td>relu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.119046</td><td style=\"text-align: right;\">               2048</td><td>BCEWithLogitsLoss   </td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_76d0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.333075</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       29.4643  </td><td style=\"text-align: right;\">     0.733229 </td><td style=\"text-align: right;\">          0.776851 </td><td style=\"text-align: right;\">     0.708916   </td></tr>\n",
       "<tr><td>train_model_15dd1704</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.126099</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_7010</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.341871</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.4464  </td><td style=\"text-align: right;\">     0.75635  </td><td style=\"text-align: right;\">          0.946908 </td><td style=\"text-align: right;\">     0.641963   </td></tr>\n",
       "<tr><td>train_model_9a98d2fd</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.181465</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_f7c0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.334372</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.9097  </td><td style=\"text-align: right;\">     0.831217 </td><td style=\"text-align: right;\">          0.944829 </td><td style=\"text-align: right;\">     0.754032   </td></tr>\n",
       "<tr><td>train_model_bef5106a</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.177807</td><td style=\"text-align: right;\">                128</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_9720</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.333651</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.97514 </td><td style=\"text-align: right;\">     0.0633777</td><td style=\"text-align: right;\">          0.0477436</td><td style=\"text-align: right;\">     0.102802   </td></tr>\n",
       "<tr><td>train_model_a21cb192</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.178986</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_7580</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.328882</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.0039  </td><td style=\"text-align: right;\">     0.172661 </td><td style=\"text-align: right;\">          0.452038 </td><td style=\"text-align: right;\">     0.108157   </td></tr>\n",
       "<tr><td>train_model_7b4ea14a</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.128749</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_4610</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.333085</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       35.3025  </td><td style=\"text-align: right;\">     0.801742 </td><td style=\"text-align: right;\">          0.894434 </td><td style=\"text-align: right;\">     0.737592   </td></tr>\n",
       "<tr><td>train_model_4448a5cf</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.134922</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_2a40</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.343763</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.9241  </td><td style=\"text-align: right;\">     0.80889  </td><td style=\"text-align: right;\">          0.879831 </td><td style=\"text-align: right;\">     0.760244   </td></tr>\n",
       "<tr><td>train_model_be832d29</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.169807</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a950</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.362134</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       35.0669  </td><td style=\"text-align: right;\">     0.809871 </td><td style=\"text-align: right;\">          0.939408 </td><td style=\"text-align: right;\">     0.723801   </td></tr>\n",
       "<tr><td>train_model_db136e93</td><td>TERMINATED</td><td>127.0.0.1:52286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.13173 </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6b30</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.328408</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.3798  </td><td style=\"text-align: right;\">     0.77174  </td><td style=\"text-align: right;\">          0.950459 </td><td style=\"text-align: right;\">     0.661986   </td></tr>\n",
       "<tr><td>train_model_5dd7b1f4</td><td>TERMINATED</td><td>127.0.0.1:51602</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.132959</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_5b40</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.32717 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       35.1462  </td><td style=\"text-align: right;\">     0.819125 </td><td style=\"text-align: right;\">          0.953694 </td><td style=\"text-align: right;\">     0.728818   </td></tr>\n",
       "<tr><td>train_model_07f09b60</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.130253</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_5480</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.326405</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       35.4539  </td><td style=\"text-align: right;\">     0.818899 </td><td style=\"text-align: right;\">          0.919674 </td><td style=\"text-align: right;\">     0.749042   </td></tr>\n",
       "<tr><td>train_model_f416a16d</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.123769</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_aa10</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.323561</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.8795  </td><td style=\"text-align: right;\">     0.574232 </td><td style=\"text-align: right;\">          0.696481 </td><td style=\"text-align: right;\">     0.516265   </td></tr>\n",
       "<tr><td>train_model_596d5975</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.122225</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_92a0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.350703</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.7131  </td><td style=\"text-align: right;\">     0.175678 </td><td style=\"text-align: right;\">          0.438849 </td><td style=\"text-align: right;\">     0.112137   </td></tr>\n",
       "<tr><td>train_model_00fc10c8</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.121516</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_c850</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.353166</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.572   </td><td style=\"text-align: right;\">     0.728915 </td><td style=\"text-align: right;\">          0.965292 </td><td style=\"text-align: right;\">     0.599401   </td></tr>\n",
       "<tr><td>train_model_92046231</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.129412</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_ae00</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.348289</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.7701  </td><td style=\"text-align: right;\">     0.752753 </td><td style=\"text-align: right;\">          0.933348 </td><td style=\"text-align: right;\">     0.644078   </td></tr>\n",
       "<tr><td>train_model_ca35f391</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.120575</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_f490</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.347902</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.8076  </td><td style=\"text-align: right;\">     0.170822 </td><td style=\"text-align: right;\">          0.458034 </td><td style=\"text-align: right;\">     0.106405   </td></tr>\n",
       "<tr><td>train_model_ebf4b411</td><td>TERMINATED</td><td>127.0.0.1:52286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.133592</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_10f0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.353026</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       30.4717  </td><td style=\"text-align: right;\">     0.542683 </td><td style=\"text-align: right;\">          0.902675 </td><td style=\"text-align: right;\">     0.404574   </td></tr>\n",
       "<tr><td>train_model_369fd2cb</td><td>TERMINATED</td><td>127.0.0.1:51602</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.129854</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1210</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.35071 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.4444  </td><td style=\"text-align: right;\">     0.760478 </td><td style=\"text-align: right;\">          0.940542 </td><td style=\"text-align: right;\">     0.650451   </td></tr>\n",
       "<tr><td>train_model_12790896</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.128459</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_0be0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.34668 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.3291  </td><td style=\"text-align: right;\">     0.773426 </td><td style=\"text-align: right;\">          0.955654 </td><td style=\"text-align: right;\">     0.661643   </td></tr>\n",
       "<tr><td>train_model_7c0911d0</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.1333  </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_b460</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.301246</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.819   </td><td style=\"text-align: right;\">     0.817636 </td><td style=\"text-align: right;\">          0.891039 </td><td style=\"text-align: right;\">     0.76605    </td></tr>\n",
       "<tr><td>train_model_43a21182</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.136208</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_7fd0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.350523</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.7384  </td><td style=\"text-align: right;\">     0.143885 </td><td style=\"text-align: right;\">          0.544964 </td><td style=\"text-align: right;\">     0.0834487  </td></tr>\n",
       "<tr><td>train_model_06fe4d98</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.132913</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_5b10</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.304531</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.0075  </td><td style=\"text-align: right;\">     0.753572 </td><td style=\"text-align: right;\">          0.925678 </td><td style=\"text-align: right;\">     0.649421   </td></tr>\n",
       "<tr><td>train_model_831a77a4</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.13922 </td><td style=\"text-align: right;\">                128</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_c280</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.38639 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.10894 </td><td style=\"text-align: right;\">     0.0951014</td><td style=\"text-align: right;\">          0.733813 </td><td style=\"text-align: right;\">     0.0513346  </td></tr>\n",
       "<tr><td>train_model_4f21b2d6</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.100607</td><td style=\"text-align: right;\">                128</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_0370</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.300434</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.07883 </td><td style=\"text-align: right;\">     0.14261  </td><td style=\"text-align: right;\">          0.165093 </td><td style=\"text-align: right;\">     0.134162   </td></tr>\n",
       "<tr><td>train_model_94cfdc9f</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.102524</td><td style=\"text-align: right;\">                128</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_7a60 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_e1d0</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_de40</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.304798</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.392   </td><td style=\"text-align: right;\">     0.121774 </td><td style=\"text-align: right;\">          0.107921 </td><td style=\"text-align: right;\">     0.15031    </td></tr>\n",
       "<tr><td>train_model_fd59b50d</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.132255</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_f8e0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.304436</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.7161  </td><td style=\"text-align: right;\">     0.832026 </td><td style=\"text-align: right;\">          0.917587 </td><td style=\"text-align: right;\">     0.77234    </td></tr>\n",
       "<tr><td>train_model_c1cadec1</td><td>TERMINATED</td><td>127.0.0.1:52286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.133012</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_e6e0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.368723</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       31.205   </td><td style=\"text-align: right;\">     0.820812 </td><td style=\"text-align: right;\">          0.936139 </td><td style=\"text-align: right;\">     0.742155   </td></tr>\n",
       "<tr><td>train_model_dedf7c3a</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.15361 </td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>&lt;ray.tune.searc_aa40 </td><td>                    </td><td>                     </td><td>min                </td><td>                    </td><td>ReduceLROnPlateau  </td><td>&lt;ray.tune.searc_0250</td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a170</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.37143 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.2975  </td><td style=\"text-align: right;\">     0.427396 </td><td style=\"text-align: right;\">          0.882641 </td><td style=\"text-align: right;\">     0.297126   </td></tr>\n",
       "<tr><td>train_model_90aac25a</td><td>TERMINATED</td><td>127.0.0.1:51602</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.115115</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a6b0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.369279</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.1934  </td><td style=\"text-align: right;\">     0.20905  </td><td style=\"text-align: right;\">          0.642532 </td><td style=\"text-align: right;\">     0.131009   </td></tr>\n",
       "<tr><td>train_model_8bf5274f</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.112208</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_ece0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.372247</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.0234  </td><td style=\"text-align: right;\">     0.77614  </td><td style=\"text-align: right;\">          0.964477 </td><td style=\"text-align: right;\">     0.661905   </td></tr>\n",
       "<tr><td>train_model_e3bf53b0</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.109049</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_d660</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.370764</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.762   </td><td style=\"text-align: right;\">     0.23841  </td><td style=\"text-align: right;\">          0.836434 </td><td style=\"text-align: right;\">     0.143572   </td></tr>\n",
       "<tr><td>train_model_fe574e9f</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.110812</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_c2e0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.369315</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.7775  </td><td style=\"text-align: right;\">     0.519914 </td><td style=\"text-align: right;\">          0.925547 </td><td style=\"text-align: right;\">     0.377698   </td></tr>\n",
       "<tr><td>train_model_54a9b2b9</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.112624</td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_c940</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.373542</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4.24395 </td><td style=\"text-align: right;\">     0.0945022</td><td style=\"text-align: right;\">          0.726619 </td><td style=\"text-align: right;\">     0.0513346  </td></tr>\n",
       "<tr><td>train_model_b55791c4</td><td>TERMINATED</td><td>127.0.0.1:52286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.11139 </td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_8490</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.370729</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4.41028 </td><td style=\"text-align: right;\">     0.284101 </td><td style=\"text-align: right;\">          0.770708 </td><td style=\"text-align: right;\">     0.184344   </td></tr>\n",
       "<tr><td>train_model_aa239651</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.15149 </td><td style=\"text-align: right;\">                256</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_1d50</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.367235</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4.67944 </td><td style=\"text-align: right;\">     0.0951014</td><td style=\"text-align: right;\">          0.733813 </td><td style=\"text-align: right;\">     0.0513346  </td></tr>\n",
       "<tr><td>train_model_6b1228cf</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.113172</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_f6d0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.318065</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       34.0176  </td><td style=\"text-align: right;\">     0.174882 </td><td style=\"text-align: right;\">          0.435252 </td><td style=\"text-align: right;\">     0.112137   </td></tr>\n",
       "<tr><td>train_model_5bdebd95</td><td>TERMINATED</td><td>127.0.0.1:52286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.153008</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_4130</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.316838</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       30.8001  </td><td style=\"text-align: right;\">     0.744454 </td><td style=\"text-align: right;\">          0.932215 </td><td style=\"text-align: right;\">     0.634977   </td></tr>\n",
       "<tr><td>train_model_bcc8947d</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.142069</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_9a50  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_9fc0 </td><td>                   </td><td>&lt;ray.tune.searc_a170</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_a110</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_9e40</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.317713</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.8567  </td><td style=\"text-align: right;\">     0.841969 </td><td style=\"text-align: right;\">          0.944666 </td><td style=\"text-align: right;\">     0.770953   </td></tr>\n",
       "<tr><td>train_model_183d1cbc</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.147273</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_b8e0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.319561</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       33.661   </td><td style=\"text-align: right;\">     0.783805 </td><td style=\"text-align: right;\">          0.933414 </td><td style=\"text-align: right;\">     0.686981   </td></tr>\n",
       "<tr><td>train_model_c2788f7c</td><td>TERMINATED</td><td>127.0.0.1:51602</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.188878</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_8220  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_8880 </td><td>                   </td><td>&lt;ray.tune.searc_9bd0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_ac20</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_8d00</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.317699</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       32.4981  </td><td style=\"text-align: right;\">     0.815248 </td><td style=\"text-align: right;\">          0.916818 </td><td style=\"text-align: right;\">     0.744777   </td></tr>\n",
       "<tr><td>train_model_f2c5d95e</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.106722</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>&lt;ray.tune.searc_b700</td><td>                     </td><td>                   </td><td>                    </td><td>StepLR             </td><td>                    </td><td>&lt;ray.tune.searc_8d60</td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_3e50</td><td>&lt;ray.tune.searc_8b20</td><td>SGD             </td><td style=\"text-align: right;\">   0.319496</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        9.35973 </td><td style=\"text-align: right;\">     0.0236151</td><td style=\"text-align: right;\">          0.0180946</td><td style=\"text-align: right;\">     0.036967   </td></tr>\n",
       "<tr><td>train_model_89516962</td><td>TERMINATED</td><td>127.0.0.1:51736</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.146118</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_9630  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_8ca0 </td><td>                   </td><td>&lt;ray.tune.searc_9210</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_8e20</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_9300</td><td>&lt;ray.tune.searc_a770</td><td>SGD             </td><td style=\"text-align: right;\">   0.317261</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       24.4853  </td><td style=\"text-align: right;\">     0.0227266</td><td style=\"text-align: right;\">          0.0170046</td><td style=\"text-align: right;\">     0.0362285  </td></tr>\n",
       "<tr><td>train_model_fd28d61a</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.143174</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_de70  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_cc10 </td><td>                   </td><td>&lt;ray.tune.searc_d510</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_d7e0</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a6e0</td><td>&lt;ray.tune.searc_d0c0</td><td>SGD             </td><td style=\"text-align: right;\">   0.317972</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        6.32103 </td><td style=\"text-align: right;\">     0.0360453</td><td style=\"text-align: right;\">          0.0272509</td><td style=\"text-align: right;\">     0.0572532  </td></tr>\n",
       "<tr><td>train_model_6598edd8</td><td>TERMINATED</td><td>127.0.0.1:51377</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.144317</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_a980</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.31943 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       31.4787  </td><td style=\"text-align: right;\">     0.501768 </td><td style=\"text-align: right;\">          0.878622 </td><td style=\"text-align: right;\">     0.371575   </td></tr>\n",
       "<tr><td>train_model_7f92dc21</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.143791</td><td style=\"text-align: right;\">                512</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_ed40  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_d060 </td><td>                   </td><td>&lt;ray.tune.searc_e3e0</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_e4a0</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_cb50</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.388776</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        7.90815 </td><td style=\"text-align: right;\">     0.698473 </td><td style=\"text-align: right;\">          0.780475 </td><td style=\"text-align: right;\">     0.648429   </td></tr>\n",
       "<tr><td>train_model_9601e1af</td><td>TERMINATED</td><td>127.0.0.1:52286</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.145795</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_cd60</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.312681</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       27.0367  </td><td style=\"text-align: right;\">     0.832194 </td><td style=\"text-align: right;\">          0.933766 </td><td style=\"text-align: right;\">     0.761851   </td></tr>\n",
       "<tr><td>train_model_0ea55c5e</td><td>TERMINATED</td><td>127.0.0.1:51656</td><td>tanh           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.188815</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_4b20</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.383669</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       28.9545  </td><td style=\"text-align: right;\">     0.221407 </td><td style=\"text-align: right;\">          0.738849 </td><td style=\"text-align: right;\">     0.135296   </td></tr>\n",
       "<tr><td>train_model_a1d36397</td><td>TERMINATED</td><td>127.0.0.1:51949</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.146237</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>                      </td><td>                      </td><td>                     </td><td>                    </td><td>                     </td><td>                   </td><td>                    </td><td>None               </td><td>                    </td><td>                    </td><td>                    </td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_f4c0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.388035</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       27.6315  </td><td style=\"text-align: right;\">     0.64958  </td><td style=\"text-align: right;\">          0.924254 </td><td style=\"text-align: right;\">     0.515833   </td></tr>\n",
       "<tr><td>train_model_694ecbdd</td><td>TERMINATED</td><td>127.0.0.1:51286</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.140911</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_8580  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_92d0 </td><td>                   </td><td>&lt;ray.tune.searc_bc40</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_a620</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_af50</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.391101</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       17.3008  </td><td style=\"text-align: right;\">     0.339961 </td><td style=\"text-align: right;\">          0.53751  </td><td style=\"text-align: right;\">     0.274169   </td></tr>\n",
       "<tr><td>train_model_4144b9b3</td><td>TERMINATED</td><td>127.0.0.1:51336</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.138883</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_5e70  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_7a30 </td><td>                   </td><td>&lt;ray.tune.searc_6e30</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_6440</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_7ca0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.309583</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       23.3455  </td><td style=\"text-align: right;\">     0.283221 </td><td style=\"text-align: right;\">          0.401961 </td><td style=\"text-align: right;\">     0.229712   </td></tr>\n",
       "<tr><td>train_model_bdad736c</td><td>TERMINATED</td><td>127.0.0.1:51602</td><td>gelu           </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.146836</td><td style=\"text-align: right;\">               2048</td><td>MultiLabelSoftM_0260</td><td>                    </td><td>&lt;ray.tune.searc_53c0  </td><td>                      </td><td>                     </td><td>                    </td><td>&lt;ray.tune.searc_4550 </td><td>                   </td><td>&lt;ray.tune.searc_4850</td><td>CyclicLR           </td><td>                    </td><td>                    </td><td>&lt;ray.tune.searc_4cd0</td><td style=\"text-align: right;\">           2</td><td>&lt;ray.tune.searc_6bc0</td><td>                    </td><td>RMSprop         </td><td style=\"text-align: right;\">   0.312795</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       22.7401  </td><td style=\"text-align: right;\">     0.550312 </td><td style=\"text-align: right;\">          0.593028 </td><td style=\"text-align: right;\">     0.532349   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:56:10,364\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n"
     ]
    }
   ],
   "source": [
    "# Define search space for hyperparameter optimization\n",
    "search_space = {\n",
    "    \"output_dim\": len(all_two_grams),  # Output dimension is also the number of unique 2-grams\n",
    "    \"num_layers\": tune.randint(1, 4),  # Vary the number of layers in the model\n",
    "    #\"num_layers\": tune.randint(1, 2),\n",
    "    \"hidden_layer_size\": tune.choice([128, 256, 512, 1024, 2048]),  # Different sizes for hidden layers\n",
    "    #\"hidden_layer_size\": tune.choice([1024, 2048]),  # Different sizes for hidden layers\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.4),  # Dropout rate between 0.1 and 0.4\n",
    "    \"activation_fn\": tune.choice([\"relu\", \"leaky_relu\", \"gelu\", \"elu\", \"selu\", \"tanh\"]),  # Activation functions to choose from\n",
    "    \"optimizer\": tune.choice([\n",
    "        {\"name\": \"Adam\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"AdamW\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "        {\"name\": \"SGD\", \"lr\": tune.loguniform(1e-4, 1e-2), \"momentum\": tune.uniform(0.0, 0.99)},\n",
    "        {\"name\": \"RMSprop\", \"lr\": tune.loguniform(1e-5, 1e-3)},\n",
    "    ]),\n",
    "    \"loss_fn\": tune.choice([\"BCEWithLogitsLoss\", \"MultiLabelSoftMarginLoss\", \"SoftMarginLoss\"]),\n",
    "    \"threshold\": tune.uniform(0.3, 0.8),\n",
    "    \"lr_scheduler\": tune.choice([\n",
    "        {\"name\": \"StepLR\", \"step_size\": tune.choice([5, 10, 20]), \"gamma\": tune.uniform(0.1, 0.9)},\n",
    "        {\"name\": \"ExponentialLR\", \"gamma\": tune.uniform(0.85, 0.99)},\n",
    "        {\"name\": \"ReduceLROnPlateau\", \"mode\": \"min\", \"factor\": tune.uniform(0.1, 0.5), \"patience\": tune.choice([5, 10, 15])},\n",
    "        {\"name\": \"CosineAnnealingLR\", \"T_max\": tune.loguniform(10, 50) , \"eta_min\": tune.choice([1e-5, 1e-6, 0])},\n",
    "        {\"name\": \"CyclicLR\", \"base_lr\": tune.loguniform(1e-5, 1e-3), \"max_lr\": tune.loguniform(1e-3, 1e-1), \"step_size_up\": tune.choice([2000, 4000]), \"mode_cyclic\": tune.choice([\"triangular\", \"triangular2\", \"exp_range\"]) },\n",
    "        {\"name\": \"None\"}  # No scheduler\n",
    "    ]),\n",
    "    \"batch_size\": tune.choice([8, 16, 32, 64]),  # Batch sizes to test\n",
    "}\n",
    "\n",
    "# Initialize Ray for hyperparameter optimization\n",
    "ray.init(ignore_reinit_error=True, logging_level=\"ERROR\")\n",
    "\n",
    "# Optuna Search Algorithm for optimizing the hyperparameters\n",
    "optuna_search = OptunaSearch(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"max\")\n",
    "\n",
    "# Use ASHAScheduler to manage trials and early stopping\n",
    "scheduler = ASHAScheduler(metric=\"total_val_loss\", mode=\"min\")\n",
    "\n",
    "\n",
    "\n",
    "# Define and configure the Tuner for Ray Tune\n",
    "tuner = tune.Tuner(\n",
    "    partial(train_model, data_dir=data_dir, output_dim=len(all_two_grams),alice_enc_hash=alice_enc_hash, identifier=identifier, patience=DEA_CONFIG[\"Patience\"], min_delta=DEA_CONFIG[\"MinDelta\"]),  # The function to optimize (training function)\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,  # Search strategy using Optuna\n",
    "        scheduler=scheduler,  # Use ASHA to manage the trials\n",
    "        num_samples=DEA_CONFIG[\"NumSamples\"],  # Number of trials to run\n",
    "        max_concurrent_trials=GLOBAL_CONFIG[\"Workers\"],\n",
    "    ),\n",
    "    param_space=search_space  # Pass in the defined hyperparameter search space\n",
    "\n",
    ")\n",
    "\n",
    "# Run the tuner\n",
    "results = tuner.fit()\n",
    "\n",
    "# Shut down Ray after finishing the optimization\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = results\n",
    "best_result = result_grid.get_best_result(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_hyperparameter_optimization = time.time() - start_hyperparameter_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to all_trial_results.csv\n",
      "\n",
      "🔍 Best_Result\n",
      "----------------------------------------\n",
      "Config: {'output_dim': 1036, 'num_layers': 2, 'hidden_layer_size': 2048, 'dropout_rate': 0.14206887287899106, 'activation_fn': 'tanh', 'optimizer': {'name': 'RMSprop', 'lr': 0.0004131952643445311}, 'loss_fn': 'MultiLabelSoftMarginLoss', 'threshold': 0.3177131923858107, 'lr_scheduler': {'name': 'CyclicLR', 'base_lr': 1.7742324119128493e-05, 'max_lr': 0.08733737520673353, 'step_size_up': 4000, 'mode_cyclic': 'triangular'}, 'batch_size': 16}\n",
      "Average Dice: 0.8420\n",
      "Average Precision: 0.9447\n",
      "Average Recall: 0.7710\n",
      "Average F1: 0.8420\n",
      "\n",
      "🔍 Worst_Result\n",
      "----------------------------------------\n",
      "Config: {'output_dim': 1036, 'num_layers': 3, 'hidden_layer_size': 128, 'dropout_rate': 0.18289400235026654, 'activation_fn': 'relu', 'optimizer': {'name': 'Adam', 'lr': 2.3219239802551432e-05}, 'loss_fn': 'BCEWithLogitsLoss', 'threshold': 0.744803349482235, 'lr_scheduler': {'name': 'StepLR', 'step_size': 20, 'gamma': 0.5732076257079861}, 'batch_size': 8}\n",
      "Average Dice: 0.0000\n",
      "Average Precision: 0.0000\n",
      "Average Recall: 0.0000\n",
      "Average F1: 0.0000\n",
      "\n",
      "📊 Average Metrics Across All Trials\n",
      "----------------------------------------\n",
      "Average_dice: 0.3524\n",
      "Average_precision: 0.5377\n",
      "Average_recall: 0.3343\n",
      "Average_f1: 0.3524\n",
      "📊 Saved plot: metric_distributions.png\n",
      "📌 Saved heatmap: correlation_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    worst_result = result_grid.get_best_result(metric=DEA_CONFIG[\"MetricToOptimize\"], mode=\"min\")\n",
    "\n",
    "    # Combine configs and metrics into a DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            **clean_result_dict(resolve_config(result.config)),\n",
    "            **{k: result.metrics.get(k) for k in [\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]},\n",
    "        }\n",
    "        for result in result_grid\n",
    "    ])\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(f\"{save_to}/hyperparameteroptimization/all_trial_results.csv\", index=False)\n",
    "    print(\"✅ Results saved to all_trial_results.csv\")\n",
    "\n",
    "    print_and_save_result(\"Best_Result\", best_result, f\"{save_to}/hyperparameteroptimization\")\n",
    "    print_and_save_result(\"Worst_Result\", worst_result, f\"{save_to}/hyperparameteroptimization\")\n",
    "\n",
    "    # Compute and print average metrics\n",
    "    print(\"\\n📊 Average Metrics Across All Trials\")\n",
    "    avg_metrics = df[[\"average_dice\", \"average_precision\", \"average_recall\", \"average_f1\"]].mean()\n",
    "    print(\"-\" * 40)\n",
    "    for key, value in avg_metrics.items():\n",
    "        print(f\"{key.capitalize()}: {value:.4f}\")\n",
    "\n",
    "    # --- 📈 Plotting performance metrics ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df[[\"average_dice\", \"average_recall\", \"average_f1\", \"average_precision\"]])\n",
    "    plt.title(\"Distribution of Performance Metrics Across Trials\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{save_to}/hyperparameteroptimization/metric_distributions.png\")\n",
    "    plt.close()\n",
    "    print(\"📊 Saved plot: metric_distributions.png\")\n",
    "\n",
    "    # --- 📌 Correlation between config params and performance ---\n",
    "    # Only include numeric config columns\n",
    "    exclude_cols = {\"input_dim\", \"output_dim\"}\n",
    "    numeric_config_cols = [\n",
    "        col for col in df.columns\n",
    "        if pd.api.types.is_numeric_dtype(df[col]) and col not in exclude_cols\n",
    "    ]\n",
    "    correlation_df = df[numeric_config_cols].corr()\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Between Parameters and Metrics\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_to}/hyperparameteroptimization/correlation_heatmap.png\")\n",
    "    plt.close()\n",
    "    print(\"📌 Saved heatmap: correlation_heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "\n",
    "The neural network model is selected dynamically based on the encoding technique used for Alice’s data.\n",
    "\n",
    "### Supported Models:\n",
    "\n",
    "- **BloomFilter** → `BloomFilterToTwoGramClassifier`  \n",
    "  - Input: Binary vector (Bloom filter)  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TabMinHash** → `TabMinHashToTwoGramClassifier`  \n",
    "  - Input: Tabulated MinHash signature  \n",
    "  - Output: 2-gram prediction\n",
    "\n",
    "- **TwoStepHash** → `TwoStepHashToTwoGramClassifier`  \n",
    "  - Input: Length of the unique integers present\n",
    "  - Output: 2-gram predicition\n",
    "    \n",
    "Each model outputs predictions over the set of all possible 2-grams (`all_two_grams`), and the input dimension is dynamically configured based on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_model_training = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = resolve_config(best_result.config)\n",
    "data_train, data_val, data_test = load_data(data_dir, alice_enc_hash, identifier, load_test=True)\n",
    "input_dim=data_train[0][0].shape[0]\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    data_train,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),  # Default to 32 if not specified\n",
    "    shuffle=True  # Important for training\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    data_val,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),\n",
    "    shuffle=False  # Allows variation in validation batches\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    data_test,\n",
    "    batch_size=int(best_config.get(\"batch_size\", 32)),\n",
    "    shuffle=False  # Allows variation in validation batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.14206887287899106, inplace=False)\n",
      "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (4): Tanh()\n",
      "    (5): Dropout(p=0.14206887287899106, inplace=False)\n",
      "    (6): Linear(in_features=2048, out_features=1036, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=len(all_two_grams),\n",
    "            hidden_layer=best_config.get(\"hidden_layer_size\", 128),  # Default to 128 if not specified\n",
    "            num_layers=best_config.get(\"num_layers\", 2),  # Default to 2 if not specified\n",
    "            dropout_rate=best_config.get(\"dropout_rate\", 0.2),  # Default to 0.2 if not specified\n",
    "            activation_fn=best_config.get(\"activation_fn\", \"relu\")  # Default to 'relu' if not specified\n",
    "        )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Environment Setup\n",
    "This code initializes the core components needed for training a neural network model.\n",
    "\n",
    "1. TensorBoard Setup\n",
    "    - Creates unique run name by combining:\n",
    "    - Loss function type\n",
    "    - Optimizer choice\n",
    "    - Alice's algorithm\n",
    "    - Initializes TensorBoard writer in runs directory\n",
    "2. Device Configuration\n",
    "    - Automatically selects GPU if available, falls back to CPU\n",
    "    - Moves model to selected device\n",
    "3. Loss Functions\n",
    "    - `BCEWithLogitsLoss`: Binary Cross Entropy with Logits\n",
    "    - `MultiLabelSoftMarginLoss`: Multi-Label Soft Margin Loss\n",
    "4. Optimizers:\n",
    "    - `Adam`: Adaptive Moment Estimation\n",
    "    - `AdamW`: Adam with Weight Decay\n",
    "    - `SGD`: Stochastic Gradient Descent (with momentum)\n",
    "    - `RMSprop`: Root Mean Square Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    # Setup tensorboard logging\n",
    "    run_name = \"\".join([\n",
    "        best_config.get(\"loss_fn\", \"MultiLabelSoftMarginLoss\"),\n",
    "        best_config.get(\"optimizer\").get(\"name\", \"Adam\"),\n",
    "        ENC_CONFIG[\"AliceAlgo\"],\n",
    "        best_config.get(\"activation_fn\", \"relu\"),\n",
    "    ])\n",
    "    tb_writer = SummaryWriter(f\"{save_to}/{run_name}\")\n",
    "\n",
    "# Setup compute device (GPU/CPU)\n",
    "compute_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(compute_device)\n",
    "\n",
    "# Initialize loss function\n",
    "match best_config.get(\"loss_fn\", \"MultiLabelSoftMarginLoss\"):\n",
    "    case \"BCEWithLogitsLoss\":\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    case \"MultiLabelSoftMarginLoss\":\n",
    "        criterion = nn.MultiLabelSoftMarginLoss(reduction='mean')\n",
    "    case \"SoftMarginLoss\":\n",
    "        criterion = nn.SoftMarginLoss()\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported loss function: {best_config.get('loss_fn', 'MultiLabelSoftMarginLoss')}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "match best_config.get(\"optimizer\").get(\"name\", \"Adam\"):\n",
    "    case \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                            lr=best_config.get(\"optimizer\").get(\"lr\"),\n",
    "                            momentum=best_config.get(\"optimizer\").get(\"momentum\"))\n",
    "    case \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=best_config.get(\"optimizer\").get(\"lr\"))\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported optimizer: {best_config.get('optimizer').get('name', 'Adam')}\")\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "match best_config.get(\"lr_scheduler\").get(\"name\", \"None\"):\n",
    "    case \"StepLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=best_config.get(\"lr_scheduler\").get(\"step_size\"),\n",
    "            gamma=best_config.get(\"lr_scheduler\").get(\"gamma\")\n",
    "        )\n",
    "    case \"ExponentialLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=best_config.get(\"lr_scheduler\").get(\"gamma\")\n",
    "        )\n",
    "    case \"ReduceLROnPlateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=best_config.get(\"lr_scheduler\").get(\"mode\"),\n",
    "            factor=best_config.get(\"lr_scheduler\").get(\"factor\"),\n",
    "            patience=best_config.get(\"lr_scheduler\").get(\"patience\")\n",
    "        )\n",
    "    case \"CosineAnnealingLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=best_config.get(\"lr_scheduler\").get(\"T_max\")\n",
    "        )\n",
    "    case \"CyclicLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=best_config.get(\"lr_scheduler\").get(\"base_lr\"),\n",
    "            max_lr=best_config.get(\"lr_scheduler\").get(\"max_lr\"),\n",
    "            step_size_up=best_config.get(\"lr_scheduler\").get(\"step_size_up\"),\n",
    "            mode=best_config.get(\"lr_scheduler\").get(\"mode_cyclic\"),\n",
    "            cycle_momentum=False  # usually False for Adam/AdamW\n",
    "        )\n",
    "    case None | \"None\":\n",
    "        scheduler = None\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported LR scheduler: {best_config.get('lr_scheduler').get('name', 'None')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Early Stopping\n",
    "\n",
    "The function `train_model` orchestrates the training process for the neural network, including both training and validation phases for each epoch. It also utilizes **early stopping** to halt training when the validation loss fails to improve over multiple epochs, avoiding overfitting.\n",
    "\n",
    "### Key Phases:\n",
    "1. **Training Phase**: \n",
    "   - The model is trained on the `dataloader_train`, computing the training loss using the specified loss function (`criterion`) and optimizer. Gradients are calculated, and the model parameters are updated.\n",
    "  \n",
    "2. **Validation Phase**:\n",
    "   - The model is evaluated on the `dataloader_val` without updating weights. The validation loss is computed to track model performance on unseen data.\n",
    "\n",
    "3. **Logging**: \n",
    "   - Training and validation losses are logged to both the console and **TensorBoard** for tracking model performance during training.\n",
    "\n",
    "4. **Early Stopping**: \n",
    "   - If the validation loss does not improve after a certain number of epochs (defined by `DEA_CONFIG[\"Patience\"]`), the training process is halted to prevent overfitting.\n",
    "\n",
    "### Helper Functions:\n",
    "- `run_epoch`: Handles a single epoch, either for training or validation, depending on the flag `is_training`.\n",
    "- `log_metrics`: Logs the training and validation losses to the console and TensorBoard for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_epoch_metrics(epoch, total_epochs, train_loss, val_loss):\n",
    "    epoch_str = f\"[{epoch + 1}/{total_epochs}]\"\n",
    "    print(f\"{epoch_str} 🔧 Train Loss: {train_loss:.4f} | 🔍 Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if DEA_CONFIG.get(\"SaveResults\", False) and 'tb_writer' in globals():\n",
    "        tb_writer.add_scalar(\"Loss/train\", train_loss, epoch + 1)\n",
    "        tb_writer.add_scalar(\"Loss/validation\", val_loss, epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, device, scheduler=None):\n",
    "    num_epochs = best_config.get(\"epochs\", DEA_CONFIG[\"Epochs\"])\n",
    "    verbose = GLOBAL_CONFIG[\"Verbose\"]\n",
    "    patience = DEA_CONFIG[\"Patience\"]\n",
    "    min_delta = DEA_CONFIG[\"MinDelta\"]\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=min_delta, verbose=verbose)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- Training ----\n",
    "        model.train()\n",
    "        train_loss = run_epoch(\n",
    "            model, dataloader_train, criterion, optimizer,\n",
    "            device, is_training=True, verbose=verbose, scheduler=scheduler\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = run_epoch(\n",
    "                model, dataloader_val, criterion, optimizer,\n",
    "                device, is_training=False, verbose=verbose, scheduler=scheduler\n",
    "            )\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # ---- Scheduler step ----\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # ---- Logging ----\n",
    "        _log_epoch_metrics(epoch, num_epochs, train_loss, val_loss)\n",
    "\n",
    "        # ---- Early stopping ----\n",
    "        if early_stopper(val_loss):\n",
    "            if verbose:\n",
    "                print(f\"⏹️ Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20] 🔧 Train Loss: 0.2247 | 🔍 Val Loss: 0.0620\n",
      "[2/20] 🔧 Train Loss: 0.0570 | 🔍 Val Loss: 0.0562\n",
      "[3/20] 🔧 Train Loss: 0.0543 | 🔍 Val Loss: 0.0560\n",
      "[4/20] 🔧 Train Loss: 0.0537 | 🔍 Val Loss: 0.0563\n",
      "[5/20] 🔧 Train Loss: 0.0539 | 🔍 Val Loss: 0.0567\n",
      "[6/20] 🔧 Train Loss: 0.0541 | 🔍 Val Loss: 0.0566\n",
      "[7/20] 🔧 Train Loss: 0.0527 | 🔍 Val Loss: 0.0561\n",
      "[8/20] 🔧 Train Loss: 0.0489 | 🔍 Val Loss: 0.0486\n",
      "[9/20] 🔧 Train Loss: 0.0426 | 🔍 Val Loss: 0.0461\n",
      "[10/20] 🔧 Train Loss: 0.0370 | 🔍 Val Loss: 0.0394\n",
      "[11/20] 🔧 Train Loss: 0.0283 | 🔍 Val Loss: 0.0338\n",
      "[12/20] 🔧 Train Loss: 0.0225 | 🔍 Val Loss: 0.0303\n",
      "[13/20] 🔧 Train Loss: 0.0171 | 🔍 Val Loss: 0.0274\n",
      "[14/20] 🔧 Train Loss: 0.0127 | 🔍 Val Loss: 0.0281\n",
      "[15/20] 🔧 Train Loss: 0.0093 | 🔍 Val Loss: 0.0232\n",
      "[16/20] 🔧 Train Loss: 0.0063 | 🔍 Val Loss: 0.0219\n",
      "[17/20] 🔧 Train Loss: 0.0046 | 🔍 Val Loss: 0.0217\n",
      "[18/20] 🔧 Train Loss: 0.0033 | 🔍 Val Loss: 0.0211\n",
      "[19/20] 🔧 Train Loss: 0.0024 | 🔍 Val Loss: 0.0204\n",
      "[20/20] 🔧 Train Loss: 0.0017 | 🔍 Val Loss: 0.0230\n"
     ]
    }
   ],
   "source": [
    "model, train_losses, val_losses = train_model(\n",
    "    model, dataloader_train, dataloader_val,\n",
    "    criterion, optimizer, compute_device,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_model_training = time.time() - start_model_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Visualization over Epochs\n",
    "\n",
    "This code snippet generates a plot to visualize the **training loss** and **validation loss** across epochs. It's useful for tracking model performance during training and evaluating if overfitting is occurring (i.e., when validation loss starts increasing while training loss continues to decrease).\n",
    "\n",
    "### Key Elements:\n",
    "1. **Plotting the Losses**: \n",
    "   - The `train_losses` and `val_losses` are plotted over the epochs. \n",
    "   - The **blue line** represents the training loss, and the **red line** represents the validation loss.\n",
    "\n",
    "2. **Legend**: \n",
    "   - A legend is added to distinguish between training and validation losses.\n",
    "\n",
    "3. **Title and Labels**: \n",
    "   - The plot is titled \"Training and Validation Loss over Epochs\" for context.\n",
    "   - **X-axis** represents the epoch number, and **Y-axis** represents the loss value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation loss', color='red')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "if DEA_CONFIG.get(\"SaveResults\", False):\n",
    "    plt.savefig(f\"{save_to}/loss_curve.png\")\n",
    "    plt.close()\n",
    "    print(\"📉 Saved loss curve to loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Application to Encoded Data\n",
    "\n",
    "This code performs inference on the test data and compares the predicted 2-grams with the actual 2-grams, providing a performance evaluation based on the **Dice similarity coefficient**.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Prepare for Evaluation**:\n",
    "   - The model is switched to **evaluation mode** (`model.eval()`), ensuring no gradient computation.\n",
    "   \n",
    "2. **Thresholding**:\n",
    "   - A threshold (`DEA_CONFIG[\"FilterThreshold\"]`) is applied to filter out low-probability predictions, retaining only the most confident predictions.\n",
    "\n",
    "3. **Inference and 2-Gram Scoring**:\n",
    "   - The model is applied to the batch, and the **logits** are converted into probabilities using the **sigmoid function**.\n",
    "   - The probabilities are then mapped to **2-gram scores**, and scores below the threshold are discarded.\n",
    "\n",
    "4. **Reconstructing Words**:\n",
    "   - For each sample in the batch, **2-grams** are reconstructed into words based on the filtered scores.\n",
    "\n",
    "5. **Performance Metrics**:\n",
    "   - The actual 2-grams (from the test dataset) are compared with the predicted 2-grams, and the **Dice similarity coefficient** is calculated for each sample.\n",
    "\n",
    "### Result:\n",
    "- The code generates a list `combined_results_performance`, which contains a detailed comparison for each UID, including:\n",
    "  - **Actual 2-grams** (from the test data)\n",
    "  - **Predicted 2-grams** (from the model)\n",
    "  - **Dice similarity** score indicating how similar the actual and predicted 2-grams are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(\n",
    "    GLOBAL_CONFIG[\"LoadPath\"] if GLOBAL_CONFIG[\"LoadResults\"] else save_to,\n",
    "    \"trained_model\"\n",
    ")\n",
    "model_file   = f\"{base_path}/model.pt\"\n",
    "config_file  = f\"{base_path}/config.json\"\n",
    "result_file  = f\"{base_path}/result.json\"\n",
    "metrics_file = f\"{base_path}/metrics.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_config(model, config, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(path, \"model.pt\"))\n",
    "    with open(os.path.join(path, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    print(f\"✅ Saved model and config to {path}\")\n",
    "\n",
    "\n",
    "def load_model_and_config(model_cls, path, input_dim, output_dim):\n",
    "    with open(os.path.join(path, \"config.json\")) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    model = model_cls(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        hidden_layer=config.get(\"hidden_layer_size\", 128),\n",
    "        num_layers=config.get(\"num_layers\", 2),\n",
    "        dropout_rate=config.get(\"dropout_rate\", 0.2),\n",
    "        activation_fn=config.get(\"activation_fn\", \"relu\")\n",
    "    )\n",
    "    model.load_state_dict(torch.load(os.path.join(path, \"model.pt\")))\n",
    "    model.eval()\n",
    "    return model, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model and config to experiment_results/experiment_BloomFilter_titanic_full_2025-07-11_17-51-49/trained_model\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    save_model_and_config(model, best_config, base_path)\n",
    "\n",
    "if GLOBAL_CONFIG[\"LoadResults\"]:\n",
    "    #TODO: how to figure out input_dim without loading dataset\n",
    "    model, best_config = load_model_and_config(BaseModel, base_path, input_dim=1024, output_dim=len(all_two_grams))\n",
    "    model.to(compute_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_application_to_encoded_data = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Test Metrics:\n",
      "  Dice:      0.8298\n",
      "  Precision: 0.9224\n",
      "  Recall:    0.7665\n",
      "  F1 Score:  0.8298\n"
     ]
    }
   ],
   "source": [
    "# Initialize metric accumulators\n",
    "total_dice = total_precision = total_recall = total_f1 = 0.0\n",
    "num_samples = 0\n",
    "results = []\n",
    "\n",
    "threshold = best_config.get(\"threshold\", 0.5)\n",
    "model.eval()\n",
    "\n",
    "# Progress bar only if verbose\n",
    "dataloader_iter = tqdm(dataloader_test, desc=\"Test loop\") if GLOBAL_CONFIG[\"Verbose\"] else dataloader_test\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels, uids in dataloader_iter:\n",
    "        data, labels = data.to(compute_device), labels.to(compute_device)\n",
    "\n",
    "        logits = model(data)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Actual and predicted 2-grams\n",
    "        actual_two_grams = decode_labels_to_two_grams(two_gram_dict, labels)\n",
    "        predicted_scores = map_probabilities_to_two_grams(two_gram_dict, probs)\n",
    "        predicted_filtered = filter_high_scoring_two_grams(predicted_scores, threshold)\n",
    "\n",
    "        # Batch metrics\n",
    "        bs = data.size(0)\n",
    "        dice, precision, recall, f1 = calculate_performance_metrics(actual_two_grams, predicted_filtered)\n",
    "\n",
    "        total_dice += dice\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        num_samples += bs\n",
    "\n",
    "        # Store per-sample predictions\n",
    "        for uid, actual, predicted in zip(uids, actual_two_grams, predicted_filtered):\n",
    "            metrics = metrics_per_entry(actual, predicted)\n",
    "            results.append({\n",
    "                \"uid\": uid,\n",
    "                \"actual_two_grams\": actual,\n",
    "                \"predicted_two_grams\": predicted,\n",
    "                \"precision\": metrics[\"precision\"],\n",
    "                \"recall\": metrics[\"recall\"],\n",
    "                \"f1\": metrics[\"f1\"],\n",
    "                \"dice\": metrics[\"dice\"],\n",
    "                \"jaccard\": metrics[\"jaccard\"]\n",
    "            })\n",
    "\n",
    "# Avoid division by zero\n",
    "if num_samples > 0:\n",
    "    avg_dice = total_dice / num_samples\n",
    "    avg_precision = total_precision / num_samples\n",
    "    avg_recall = total_recall / num_samples\n",
    "    avg_f1 = total_f1 / num_samples\n",
    "else:\n",
    "    avg_dice = avg_precision = avg_recall = avg_f1 = 0.0\n",
    "\n",
    "# Logging\n",
    "print(f\"\\n📊 Final Test Metrics:\")\n",
    "print(f\"  Dice:      {avg_dice:.4f}\")\n",
    "print(f\"  Precision: {avg_precision:.4f}\")\n",
    "print(f\"  Recall:    {avg_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {avg_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_application_to_encoded_data = time.time() - start_application_to_encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE - Metrics and Result\n",
    "if GLOBAL_CONFIG[\"SaveResults\"]:\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        f.write(f\"Average Precision: {avg_precision:.4f}\\n\")\n",
    "        f.write(f\"Average Recall: {avg_recall:.4f}\\n\")\n",
    "        f.write(f\"Average F1 Score: {avg_f1:.4f}\\n\")\n",
    "        f.write(f\"Average Dice Similarity: {avg_dice:.4f}\\n\")\n",
    "\n",
    "    with open(result_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD - Result\n",
    "if GLOBAL_CONFIG[\"LoadResults\"]:\n",
    "    with open(result_file, 'r', encoding='utf-8') as f:\n",
    "        result = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance for Re-Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Sample Reconstructions (first 5)\n",
      "UID: 434\n",
      "  Actual 2-grams:    ['ai', 'am', 'ba', 'ey', 'ia', 'il', 'ir', 'li', 'll', 'lv', 'mr', 'rd', 'si', 've', 'wi']\n",
      "  Predicted 2-grams: ['il', 'li', 'mr', 'ia', 'll', 'si', 'am', 'wi', 'ba', 'ai', 've', 'rd', 'ir', 'ey', 'lv']\n",
      "------------------------------------------------------------\n",
      "UID: 801\n",
      "  Actual 2-grams:    ['an', 'ar', 'at', 'ch', 'co', 'er', 'ey', 'ha', 'ie', 'll', 'lo', 'ly', 'mr', 'ni', 'nn', 'ol', 'ot', 'rl', 'rs', 'rv', 'ta', 'te', 'tt', 've', 'ye']\n",
      "  Predicted 2-grams: ['an', 'mr', 'ar', 'rs', 'co', 'ha', 'ol', 'er', 'lo', 'ly', 'nn', 'ot', 'ni', 'ye', 'tt', 'ey', 'll', 'jo', 'ie', 'ka', 'il']\n",
      "------------------------------------------------------------\n",
      "UID: 641\n",
      "  Actual 2-grams:    ['ag', 'em', 'er', 'es', 'ge', 'le', 'll', 'ma', 'ml', 'mm', 'sa', 'se', 'ss']\n",
      "  Predicted 2-grams: ['ss', 'ma', 'll', 'er', 'ag', 'sa', 'es', 'le', 'mm', 'ge', 'se', 'um', 'ad', 'em']\n",
      "------------------------------------------------------------\n",
      "UID: 853\n",
      "  Actual 2-grams:    ['ar', 'co', 'er', 'es', 'in', 'is', 'li', 'ma', 'mi', 'ne', 'no', 'on', 'ov', 'ry', 'ss', 've']\n",
      "  Predicted 2-grams: ['ar', 'ry', 'is', 'ss', 'mi', 'er', 'ma', 'ne', 'in', 'li', 'on', 've', 'co']\n",
      "------------------------------------------------------------\n",
      "UID: 616\n",
      "  Actual 2-grams:    ['an', 'be', 'bo', 'da', 'er', 'gi', 'il', 'lb', 'mr', 'nb', 'ns', 'om', 'rn', 'rt', 'st']\n",
      "  Predicted 2-grams: ['mr', 'rt', 'er', 'rn', 'st', 'be', 'om', 'an', 'lb', 'il', 'gi', 'ns', 'da', 'ia']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.json_normalize(results) # ≈2× faster than DataFrame(list)\n",
    "\n",
    "metric_cols = [\"precision\", \"recall\", \"f1\", \"dice\", \"jaccard\"]        # keys created earlier\n",
    "melted = results_df.melt(value_vars=metric_cols,\n",
    "                         var_name=\"metric\",\n",
    "                         value_name=\"score\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=melted,\n",
    "             x=\"score\",\n",
    "             hue=\"metric\",\n",
    "             bins=20,\n",
    "             element=\"step\",\n",
    "             fill=False,\n",
    "             kde=True,\n",
    "             palette=\"Set2\")\n",
    "plt.title(\"Distribution of Precision / Recall / F1 across Samples\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "if DEA_CONFIG.get(\"SaveResults\", False):\n",
    "    plt.savefig(f\"{save_to}/metric_distributions.png\")\n",
    "    print(\"📊  Saved plot: metric_distributions.png\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n🔍 Sample Reconstructions (first 5)\")\n",
    "for _, row in results_df.iloc[:5].iterrows():\n",
    "    print(f\"UID: {row.uid}\")\n",
    "    print(f\"  Actual 2-grams:    {row.actual_two_grams}\")\n",
    "    print(f\"  Predicted 2-grams: {row.predicted_two_grams}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Refinement and Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    start_refinement_and_reconstruction = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Reidentification Analysis:\n",
      "Total Reidentified Individuals: 0\n",
      "Total Not Reidentified Individuals: 118\n",
      "Reidentification Rate: 0.00%\n",
      "\n",
      "🔄 Reconstructing results using fuzzy matching (entry-wise, parallelized)...\n",
      "\n",
      "🔍 Reidentification Analysis:\n",
      "Total Reidentified Individuals: 0\n",
      "Total Not Reidentified Individuals: 118\n",
      "Reidentification Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_not_reidentified_df(data_dir: str, identifier: str) -> pd.DataFrame:\n",
    "    df = load_not_reidentified_data(data_dir, alice_enc_hash, identifier)\n",
    "    return lowercase_df(df)\n",
    "\n",
    "def create_identifier(df: pd.DataFrame, comps):\n",
    "    df = df.copy()\n",
    "    df[\"identifier\"] = create_identifier_column_dynamic(df, comps)\n",
    "    return df[[\"uid\", \"identifier\"]]\n",
    "\n",
    "def run_reidentification_once(reconstructed, df_not_reidentified, merge_cols, technique, identifier_components=None):\n",
    "    df_reconstructed = lowercase_df(pd.DataFrame(reconstructed, columns=merge_cols))\n",
    "\n",
    "    if(identifier_components):\n",
    "        df_not_reidentified = create_identifier(df_not_reidentified, identifier_components)\n",
    "\n",
    "    return reidentification_analysis(\n",
    "        df_reconstructed,\n",
    "        df_not_reidentified,\n",
    "        merge_cols,\n",
    "        len(df_not_reidentified),\n",
    "        technique,\n",
    "        save_path=f\"{save_to}/re_identification_results\"\n",
    "    )\n",
    "\n",
    "header = read_header(GLOBAL_CONFIG[\"Data\"])\n",
    "\n",
    "\n",
    "include_birthday = not (GLOBAL_CONFIG[\"Data\"] == \"./data/datasets/titanic_full.tsv\")\n",
    "\n",
    "TECHNIQUES = {\n",
    "    \"ai\": {\n",
    "        \"fn\": reconstruct_identities_with_llm,\n",
    "        \"merge_cols\": header[:3] + [header[-1]],\n",
    "        \"identifier_comps\": None,\n",
    "    },\n",
    "    \"greedy\": {\n",
    "        \"fn\": greedy_reconstruction,\n",
    "        \"merge_cols\": [\"uid\", \"identifier\"],\n",
    "        \"identifier_comps\": header[:-1],\n",
    "    },\n",
    "    \"fuzzy\": {\n",
    "        \"fn\": fuzzy_reconstruction_approach,\n",
    "        \"merge_cols\": (header[:3] if include_birthday else header[:2]) + [header[-1]],\n",
    "        \"identifier_comps\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "selected = DEA_CONFIG[\"MatchingTechnique\"]\n",
    "df_not_reid_cached = get_not_reidentified_df(data_dir, identifier)\n",
    "save_dir = f\"{save_to}/re_identification_results\"\n",
    "\n",
    "if selected == \"fuzzy_and_greedy\":\n",
    "    reidentified = {}\n",
    "    for name in (\"greedy\", \"fuzzy\"):\n",
    "        info = TECHNIQUES[name]\n",
    "        if name == \"fuzzy\":\n",
    "            recon = info[\"fn\"](results, GLOBAL_CONFIG[\"Workers\"], include_birthday )\n",
    "        else:\n",
    "            recon = info[\"fn\"](results)\n",
    "        reidentified[name] = run_reidentification_once(\n",
    "            recon,\n",
    "            df_not_reid_cached,\n",
    "            info[\"merge_cols\"],\n",
    "            name,\n",
    "            info[\"identifier_comps\"],\n",
    "        )\n",
    "else:\n",
    "    if selected not in TECHNIQUES:\n",
    "        raise ValueError(f\"Unsupported matching technique: {selected}\")\n",
    "    info = TECHNIQUES[selected]\n",
    "    if selected == \"fuzzy\":\n",
    "        recon = info[\"fn\"](results, GLOBAL_CONFIG[\"Workers\"], include_birthday)\n",
    "    if selected == \"ai\":\n",
    "        recon = info[\"fn\"](results, info[\"merge_cols\"][:-1])\n",
    "    else:\n",
    "        recon = info[\"fn\"](results)\n",
    "    reidentified = run_reidentification_once(\n",
    "        recon,\n",
    "        df_not_reid_cached,\n",
    "        info[\"merge_cols\"],\n",
    "        selected,\n",
    "        info[\"identifier_comps\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Combined Reidentification (greedy ∪ fuzzy):\n",
      "Total not re-identified individuals: 118\n",
      "Total Unique Reidentified Individuals: 0\n",
      "Combined Reidentification Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "if selected == \"fuzzy_and_greedy\":\n",
    "    # Extract UIDs from both methods\n",
    "    uids_greedy = set(reidentified[\"greedy\"][\"uid\"])\n",
    "    uids_fuzzy = set(reidentified[\"fuzzy\"][\"uid\"])\n",
    "\n",
    "    # Combine them\n",
    "    combined_uids = uids_greedy.union(uids_fuzzy)\n",
    "    total_reidentified_combined = len(combined_uids)\n",
    "\n",
    "    # Get not re-identified count\n",
    "    df_not_reid_cached = get_not_reidentified_df(data_dir, identifier)\n",
    "    len_not_reidentified = len(df_not_reid_cached)\n",
    "\n",
    "    # Compute rate\n",
    "    reidentification_rate_combined = (total_reidentified_combined / len_not_reidentified) * 100\n",
    "\n",
    "    # Print\n",
    "    print(\"\\n🔁 Combined Reidentification (greedy ∪ fuzzy):\")\n",
    "    print(f\"Total not re-identified individuals: {len_not_reidentified}\")\n",
    "    print(f\"Total Unique Reidentified Individuals: {total_reidentified_combined}\")\n",
    "    print(f\"Combined Reidentification Rate: {reidentification_rate_combined:.2f}%\")\n",
    "\n",
    "    # Save UIDs to CSV\n",
    "    save_dir = os.path.join(save_to, \"re_identification_results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame({\"uid\": list(combined_uids)}).to_csv(\n",
    "        os.path.join(save_dir, \"result_fuzzy_and_greedy.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Save summary to TXT\n",
    "    summary_path = os.path.join(save_dir, \"summary_fuzzy_and_greedy.txt\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        f.write(\"Reidentification Method: fuzzy_and_greedy\\n\")\n",
    "        f.write(f\"Total not re-identified individuals: {len_not_reidentified}\\n\")\n",
    "        f.write(f\"Total Unique Reidentified Individuals: {total_reidentified_combined}\\n\")\n",
    "        f.write(f\"Combined Reidentification Rate: {reidentification_rate_combined:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOBAL_CONFIG[\"BenchMode\"]:\n",
    "    elapsed_refinement_and_reconstruction = time.time() - start_refinement_and_reconstruction\n",
    "    elapsed_total = time.time() - start_total\n",
    "    save_dea_runtime_log(\n",
    "        elapsed_gma=elapsed_gma,\n",
    "        elapsed_hyperparameter_optimization=elapsed_hyperparameter_optimization,\n",
    "        elapsed_model_training=elapsed_model_training,\n",
    "        elapsed_application_to_encoded_data=elapsed_application_to_encoded_data,\n",
    "        elapsed_refinement_and_reconstruction=elapsed_refinement_and_reconstruction,\n",
    "        elapsed_total=elapsed_total,\n",
    "        output_dir=save_to\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
